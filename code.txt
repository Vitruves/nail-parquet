
// File: src/commands/id.rs
// Base: id

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct IdArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(long, help = "Create new ID column")]
	pub create: bool,
	
	#[arg(long, help = "Prefix for ID values", default_value = "id")]
	pub prefix: String,
	
	#[arg(long, help = "ID column name", default_value = "id")]
	pub id_col_name: String,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: IdArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	
	let result_df = if args.create {
		if args.verbose {
			eprintln!("Creating ID column '{}' with prefix '{}'", args.id_col_name, args.prefix);
		}
		add_id_column(&df, &args.id_col_name, &args.prefix, args.jobs).await?
	} else {
		df
	};
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&result_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&result_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

async fn add_id_column(df: &DataFrame, col_name: &str, prefix: &str, jobs: Option<usize>) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	// Check if column already exists
	let schema = df.schema();
	if schema.field_with_name(None, col_name).is_ok() {
		return Err(crate::error::NailError::InvalidArgument(
			format!("Column '{}' already exists. Use --id-col-name to specify a different name.", col_name)
		));
	}
	
	let id_col = if prefix.is_empty() {
		"ROW_NUMBER() OVER()".to_string()
	} else {
		format!("CONCAT('{}', ROW_NUMBER() OVER())", prefix)
	};
	
	let columns: Vec<String> = df.schema().fields().iter()
		.map(|f| format!("\"{}\"", f.name()))
		.collect();
	
	let sql = format!(
		"SELECT {} as \"{}\", {} FROM {}",
		id_col,
		col_name,
		columns.join(", "),
		table_name
	);
	
	let result = ctx.sql(&sql).await?;
	Ok(result)
}


// File: src/commands/convert.rs
// Base: convert

use clap::Args;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::{read_data, write_data};
use crate::utils::{detect_file_format};

#[derive(Args, Clone)]
pub struct ConvertArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Output file")]
	pub output: PathBuf,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: ConvertArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Converting {} to {}", args.input.display(), args.output.display());
	}
	
	let input_format = detect_file_format(&args.input)?;
	let output_format = detect_file_format(&args.output)?;
	
	if args.verbose {
		eprintln!("Input format: {:?}, Output format: {:?}", input_format, output_format);
	}
	
	let df = read_data(&args.input).await?;
	
	if args.verbose {
		let rows = df.clone().count().await?;
		let cols = df.schema().fields().len();
		eprintln!("Processing {} rows, {} columns", rows, cols);
	}
	
	write_data(&df, &args.output, Some(&output_format)).await?;
	
	// Conversion completed successfully
	
	if args.verbose {
		eprintln!("Conversion completed successfully");
	}
	
	Ok(())
}


// File: src/commands/tail.rs
// Base: tail

use clap::Args;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::read_data;
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct TailArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Number of rows to display", default_value = "5")]
	pub number: usize,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: TailArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let total_rows = df.clone().count().await?;
	
	if total_rows <= args.number {
		display_dataframe(&df, args.output.as_deref(), args.format.as_ref()).await?;
	} else {
		let skip_rows = total_rows - args.number;
		let tail_df = df.limit(skip_rows, Some(args.number))?;
		
		if args.verbose {
			eprintln!("Displaying last {} rows (total: {})", args.number, total_rows);
		}
		
		display_dataframe(&tail_df, args.output.as_deref(), args.format.as_ref()).await?;
	}
	
	Ok(())
}


// File: src/commands/merge.rs
// Base: merge

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct MergeArgs {
	#[arg(short, long, help = "Input file (left table)")]
	pub input: PathBuf,
	
	#[arg(long, help = "Right table file to merge with")]
	pub right: PathBuf,
	
	#[arg(long, help = "Perform left join")]
	pub left_join: bool,
	
	#[arg(long, help = "Perform right join")]
	pub right_join: bool,
	
	#[arg(long, help = "Join key column name")]
	pub key: Option<String>,
	
	#[arg(long, help = "Key mapping for different column names (format: left_col=right_col)")]
	pub key_mapping: Option<String>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: MergeArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading left table from: {}", args.input.display());
		eprintln!("Reading right table from: {}", args.right.display());
	}
	
	let left_df = read_data(&args.input).await?;
	let right_df = read_data(&args.right).await?;
	
	let join_type = if args.left_join {
		JoinType::Left
	} else if args.right_join {
		JoinType::Right
	} else {
		JoinType::Inner
	};
	
	let (left_key, right_key) = if let Some(key_mapping) = &args.key_mapping {
		parse_key_mapping(key_mapping)?
	} else if let Some(key) = &args.key {
		// Handle case-insensitive key matching
		let left_schema = left_df.schema();
		let right_schema = right_df.schema();
		
		let actual_left_key = left_schema.fields().iter()
			.find(|f| f.name().to_lowercase() == key.to_lowercase())
			.map(|f| f.name().clone())
			.ok_or_else(|| {
				let available_cols: Vec<String> = left_schema.fields().iter()
					.map(|f| f.name().clone())
					.collect();
				NailError::ColumnNotFound(format!(
					"Join key '{}' not found in left table. Available columns: {:?}", 
					key, available_cols
				))
			})?;
			
		let actual_right_key = right_schema.fields().iter()
			.find(|f| f.name().to_lowercase() == key.to_lowercase())
			.map(|f| f.name().clone())
			.ok_or_else(|| {
				let available_cols: Vec<String> = right_schema.fields().iter()
					.map(|f| f.name().clone())
					.collect();
				NailError::ColumnNotFound(format!(
					"Join key '{}' not found in right table. Available columns: {:?}", 
					key, available_cols
				))
			})?;
			
		(actual_left_key, actual_right_key)
	} else {
		return Err(NailError::InvalidArgument("Either --key or --key-mapping must be specified".to_string()));
	};
	
	if args.verbose {
		eprintln!("Performing {:?} join on left.{} = right.{}", join_type, left_key, right_key);
	}
	
	let result_df = perform_join(&left_df, &right_df, &left_key, &right_key, join_type, args.jobs).await?;
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&result_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&result_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

fn parse_key_mapping(mapping: &str) -> NailResult<(String, String)> {
	let parts: Vec<&str> = mapping.split('=').collect();
	if parts.len() != 2 {
		return Err(NailError::InvalidArgument("Key mapping must be in format 'left_col=right_col'".to_string()));
	}
	Ok((parts[0].trim().to_string(), parts[1].trim().to_string()))
}

async fn perform_join(
	left_df: &DataFrame,
	right_df: &DataFrame,
	left_key: &str,
	right_key: &str,
	join_type: JoinType,
	jobs: Option<usize>,
) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	
	ctx.register_table("left_table", left_df.clone().into_view())?;
	ctx.register_table("right_table", right_df.clone().into_view())?;
	
	let left_schema = left_df.schema();
	let right_schema = right_df.schema();
	
	let mut left_cols = Vec::new();
	let mut right_cols = Vec::new();
	
	for field in left_schema.fields() {
		left_cols.push(format!("l.\"{}\"", field.name()));
	}
	
	for field in right_schema.fields() {
		if field.name() != right_key {
			right_cols.push(format!("r.\"{}\" as \"r_{}\"", field.name(), field.name()));
		}
	}
	
	let join_clause = match join_type {
		JoinType::Inner => "INNER JOIN",
		JoinType::Left => "LEFT JOIN",
		JoinType::Right => "RIGHT JOIN",
		_ => "INNER JOIN",
	};
	
	let sql = format!(
		"SELECT {} FROM left_table l {} right_table r ON l.\"{}\" = r.\"{}\"",
		[left_cols, right_cols].concat().join(", "),
		join_clause,
		left_key,
		right_key
	);
	
	let result = ctx.sql(&sql).await?;
	Ok(result)
}


// File: src/lib.rs
// Base: lib

pub mod cli;
pub mod commands;
pub mod error;
pub mod utils;

pub use error::{NailError, NailResult};


// File: src/commands/size.rs
// Base: size

use clap::Args;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::read_data;
use datafusion::arrow::array::Array;

#[derive(Args, Clone)]
pub struct SizeArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Show per-column sizes")]
	pub columns: bool,
	
	#[arg(short, long, help = "Show per-row analysis")]
	pub rows: bool,
	
	#[arg(long, help = "Show raw bits without human-friendly conversion")]
	pub bits: bool,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: SizeArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Analyzing size of: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let batches = df.clone().collect().await?;
	let schema = df.schema();
	
	let row_count = df.clone().count().await?;
	let col_count = schema.fields().len();
	
	let mut total_memory = 0usize;
	let mut column_sizes = Vec::new();
	
	for batch in &batches {
		for (col_idx, field) in schema.fields().iter().enumerate() {
			let column = batch.column(col_idx);
			let size = column.get_buffer_memory_size();
			total_memory += size;
			
			if args.columns {
				if let Some(existing) = column_sizes.iter_mut().find(|(name, _)| name == field.name()) {
					existing.1 += size;
				} else {
					column_sizes.push((field.name().clone(), size));
				}
			}
		}
	}
	
	let file_size = std::fs::metadata(&args.input)?.len();
	
	if args.verbose {
		eprintln!("Analysis complete: {} rows, {} columns", row_count, col_count);
	}
	
	let output = if args.bits {
		format_size_bits(row_count, col_count, total_memory, file_size, &column_sizes, args.columns, args.rows)
	} else {
		format_size_human(row_count, col_count, total_memory, file_size, &column_sizes, args.columns, args.rows)
	};
	
	match &args.output {
		Some(output_path) => {
			std::fs::write(output_path, output)?;
			if args.verbose {
				eprintln!("Size analysis written to: {}", output_path.display());
			}
		},
		None => {
			println!("{}", output);
		},
	}
	
	Ok(())
}

fn format_size_bits(row_count: usize, col_count: usize, memory: usize, file_size: u64, 
				   column_sizes: &[(String, usize)], show_columns: bool, show_rows: bool) -> String {
	let mut output = String::new();
	
	output.push_str(&format!("Total rows: {}\n", row_count));
	output.push_str(&format!("Total columns: {}\n", col_count));
	output.push_str(&format!("File size (bytes): {}\n", file_size));
	output.push_str(&format!("Memory usage (bytes): {}\n", memory));
	output.push_str(&format!("File size (bits): {}\n", file_size * 8));
	output.push_str(&format!("Memory usage (bits): {}\n", memory * 8));
	
	if show_rows && row_count > 0 {
		output.push_str(&format!("Average bytes per row: {}\n", memory / row_count));
		output.push_str(&format!("Average bits per row: {}\n", (memory * 8) / row_count));
	}
	
	if show_columns {
		output.push_str("\nPer-column sizes (bytes):\n");
		for (name, size) in column_sizes {
			output.push_str(&format!("  {}: {} bytes ({} bits)\n", name, size, size * 8));
		}
	}
	
	output
}

fn format_size_human(row_count: usize, col_count: usize, memory: usize, file_size: u64, 
					column_sizes: &[(String, usize)], show_columns: bool, show_rows: bool) -> String {
	let mut output = String::new();
	
	output.push_str(&format!("Total rows: {}\n", row_count));
	output.push_str(&format!("Total columns: {}\n", col_count));
	output.push_str(&format!("File size: {}\n", human_bytes(file_size as usize)));
	output.push_str(&format!("Memory usage: {}\n", human_bytes(memory)));
	
	if show_rows && row_count > 0 {
		output.push_str(&format!("Average per row: {}\n", human_bytes(memory / row_count)));
	}
	
	if show_columns {
		output.push_str("\nPer-column sizes:\n");
		for (name, size) in column_sizes {
			output.push_str(&format!("  {}: {}\n", name, human_bytes(*size)));
		}
	}
	
	output
}

fn human_bytes(bytes: usize) -> String {
	const UNITS: &[&str] = &["B", "KB", "MB", "GB", "TB"];
	let mut size = bytes as f64;
	let mut unit_idx = 0;
	
	while size >= 1024.0 && unit_idx < UNITS.len() - 1 {
		size /= 1024.0;
		unit_idx += 1;
	}
	
	if unit_idx == 0 {
		format!("{} {}", size as usize, UNITS[unit_idx])
	} else {
		format!("{:.2} {}", size, UNITS[unit_idx])
	}
}


// File: src/commands/sample.rs
// Base: sample

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use rand::seq::SliceRandom;
use rand::{rngs::StdRng, SeedableRng};
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;
use datafusion::arrow::array::{StringArray, DictionaryArray, Array};
use datafusion::arrow::datatypes::UInt32Type;

#[derive(Args, Clone)]
pub struct SampleArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Number of samples", default_value = "10")]
	pub number: usize,
	
	#[arg(long, help = "Sampling method", value_enum, default_value = "random")]
	pub method: SampleMethod,
	
	#[arg(long, help = "Column name for stratified sampling")]
	pub stratify_by: Option<String>,
	
	#[arg(short, long, help = "Random seed for reproducible results")]
	pub random: Option<u64>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

#[derive(clap::ValueEnum, Clone, Debug)]
pub enum SampleMethod {
	Random,
	Stratified,
	First,
	Last,
}

pub async fn execute(args: SampleArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let total_rows = df.clone().count().await?;
	
	if args.number >= total_rows {
		if args.verbose {
			eprintln!("Requested {} samples, but only {} rows available. Returning all rows.", args.number, total_rows);
		}
		display_dataframe(&df, args.output.as_deref(), args.format.as_ref()).await?;
		return Ok(());
	}
	
	if args.verbose {
		eprintln!("Sampling {} rows from {} total using {:?} method", args.number, total_rows, args.method);
	}
	
	let sampled_df = match args.method {
		SampleMethod::Random => sample_random(&df, args.number, args.random, args.jobs).await?,
		SampleMethod::Stratified => {
			if let Some(col) = &args.stratify_by {
				sample_stratified(&df, args.number, col, args.random, args.jobs).await?
			} else {
				return Err(NailError::InvalidArgument("--stratify-by required for stratified sampling".to_string()));
			}
		},
		SampleMethod::First => df.limit(0, Some(args.number))?,
		SampleMethod::Last => {
			let skip = total_rows.saturating_sub(args.number);
			df.limit(skip, Some(args.number))?
		},
	};
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&sampled_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&sampled_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

async fn sample_random(df: &DataFrame, n: usize, seed: Option<u64>, jobs: Option<usize>) -> NailResult<DataFrame> {
	let total_rows = df.clone().count().await?;
	let mut rng = match seed {
		Some(s) => StdRng::seed_from_u64(s),
		None => StdRng::from_entropy(),
	};
	
	let mut indices: Vec<usize> = (0..total_rows).collect();
	indices.shuffle(&mut rng);
	indices.truncate(n);
	indices.sort();
	
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let indices_str = indices.iter()
		.map(|&i| (i + 1).to_string())
		.collect::<Vec<_>>()
		.join(",");
	
	// Get the original column names and quote them to preserve case
	let original_columns: Vec<String> = df.schema().fields().iter()
		.map(|f| format!("\"{}\"", f.name()))
		.collect();
	
	let sql = format!(
		"SELECT {} FROM (SELECT {}, ROW_NUMBER() OVER() as rn FROM {}) WHERE rn IN ({})",
		original_columns.join(", "),
		original_columns.join(", "),
		table_name, 
		indices_str
	);
	
	let result = ctx.sql(&sql).await?;
	
	Ok(result)
}

async fn sample_stratified(
    df: &DataFrame,
    n: usize,
    stratify_col: &str,
    _seed: Option<u64>,
    jobs: Option<usize>,
) -> NailResult<DataFrame> {
    use std::collections::HashSet;
    let ctx = crate::utils::create_context_with_jobs(jobs).await?;
    let table_name = "temp_table";
    ctx.register_table(table_name, df.clone().into_view())?;

    // Find the actual column name (case-insensitive matching)
    let schema = df.schema();
    let actual_col_name = schema.fields().iter()
        .find(|f| f.name().to_lowercase() == stratify_col.to_lowercase())
        .map(|f| f.name().clone())
        .ok_or_else(|| {
            let available_cols: Vec<String> = schema.fields().iter()
                .map(|f| f.name().clone())
                .collect();
            NailError::ColumnNotFound(format!(
                "Column '{}' not found. Available columns: {:?}", 
                stratify_col, available_cols
            ))
        })?;

    // First, let's try to get distinct values using SQL which is more robust
    let distinct_sql = format!(
        "SELECT DISTINCT {} FROM {} WHERE {} IS NOT NULL",
        actual_col_name, table_name, actual_col_name
    );
    
    let distinct_df = match ctx.sql(&distinct_sql).await {
        Ok(df) => df,
        Err(e) => {
            return Err(NailError::Statistics(format!("Failed to retrieve categories from column '{}': {}", actual_col_name, e)));
        }
    };
    
    let distinct_batches = distinct_df.clone().collect().await?;
    let mut categories = HashSet::new();
    
    for batch in &distinct_batches {
        if batch.num_columns() > 0 {
            let array_ref = batch.column(0);
        if let Some(arr) = array_ref.as_any().downcast_ref::<StringArray>() {
            for i in 0..arr.len() {
                if arr.is_valid(i) {
                        categories.insert(arr.value(i).to_string());
                }
            }
        } else if let Some(dict) = array_ref.as_any().downcast_ref::<DictionaryArray<UInt32Type>>() {
            let keys = dict.keys();
            if let Some(values) = dict.values().as_any().downcast_ref::<StringArray>() {
                for i in 0..keys.len() {
                    if keys.is_valid(i) {
                        let k = keys.value(i) as usize;
                            if k < values.len() {
                                categories.insert(values.value(k).to_string());
                            }
                    }
                }
            }
        } else {
                // Try to convert to string representation
                let schema = distinct_df.schema();
                if let Some(field) = schema.fields().get(0) {
                    match field.data_type() {
                        datafusion::arrow::datatypes::DataType::Utf8 => {
                            // Already handled above, but this is a fallback
                            for i in 0..array_ref.len() {
                                if !array_ref.is_null(i) {
                                    if let Some(scalar) = datafusion::arrow::compute::cast(array_ref, &datafusion::arrow::datatypes::DataType::Utf8).ok() {
                                        if let Some(str_arr) = scalar.as_any().downcast_ref::<StringArray>() {
                                            if i < str_arr.len() && str_arr.is_valid(i) {
                                                categories.insert(str_arr.value(i).to_string());
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        _ => {
                            return Err(NailError::Statistics(format!("Column '{}' must be of string type for stratified sampling", actual_col_name)));
                        }
                    }
                }
            }
        }
    }
    
    if categories.is_empty() {
        return Err(NailError::Statistics("No categories found for stratified sampling".to_string()));
    }
    
    let categories: Vec<String> = categories.into_iter().collect();
    let per_group = n / categories.len();
    let mut combined: Option<DataFrame> = None;
    
    for cat in &categories {
        // deterministic: take first per_group rows for each category
        let filtered = ctx.table(table_name).await?
            .filter(Expr::Column(datafusion::common::Column::new(None::<String>, &actual_col_name)).eq(lit(cat)))?;
        let limited = filtered.limit(0, Some(per_group))?;
        combined = Some(match combined {
            None => limited,
            Some(prev) => prev.union(limited)?,
        });
    }
    
    let mut result_df = combined.unwrap();
    // Handle remainder samples
    let remainder = n - per_group * categories.len();
    if remainder > 0 {
        // add random remainder from full dataset
        let rem = sample_random(df, remainder, None, None).await?;
        result_df = result_df.union(rem)?;
    }
    Ok(result_df)
}


// File: src/commands/shuffle.rs
// Base: shuffle

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct ShuffleArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Random seed for reproducible results")]
	pub random: Option<u64>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: ShuffleArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	
	if args.verbose {
		let total_rows = df.clone().count().await?;
		eprintln!("Shuffling {} rows", total_rows);
	}
	
	let shuffled_df = shuffle_dataframe(&df, args.random, args.jobs).await?;
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&shuffled_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&shuffled_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

async fn shuffle_dataframe(df: &DataFrame, _seed: Option<u64>, jobs: Option<usize>) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	ctx.register_table("temp_table", df.clone().into_view())?;

	// Simple shuffling using ORDER BY RANDOM() 
	// For now, ignore the seed parameter since DataFusion's RANDOM() doesn't support seeding reliably
	let sql = "SELECT * FROM temp_table ORDER BY RANDOM()";
	
	let result = ctx.sql(sql).await?;
	Ok(result)
}


// File: src/commands/mod.rs
// Base: mod

use clap::Subcommand;

// Data Inspection
pub mod head;
pub mod tail;
pub mod preview;
pub mod headers;
pub mod schema;
pub mod count;
pub mod size;

// Data Analysis
pub mod stats;
pub mod correlations;

// Data Manipulation
pub mod select;
pub mod drop;
pub mod fill;
pub mod filter;
pub mod search;

// Data Transformation
pub mod id;
pub mod shuffle;
pub mod sample;
pub mod dedup;

// Data Combination
pub mod merge;
pub mod append;
pub mod split;

// Format Conversion
pub mod convert;

#[derive(Subcommand)]
pub enum Commands {
	// Data Inspection
	#[command(about = "Display first N rows")]
	#[command(next_help_heading = "Data Inspection")]
	Head(head::HeadArgs),
	
	#[command(about = "Display last N rows")]
	Tail(tail::TailArgs),
	
	#[command(about = "Preview random N rows")]
	Preview(preview::PreviewArgs),
	
	#[command(about = "Display column headers")]
	Headers(headers::HeadersArgs),
	
	#[command(about = "Display schema information")]
	Schema(schema::SchemaArgs),
	
	#[command(about = "Count total rows")]
	Count(count::CountArgs),
	
	#[command(about = "Show data size information")]
	Size(size::SizeArgs),
	
	// Data Analysis
	#[command(about = "Calculate descriptive statistics")]
	#[command(next_help_heading = "Data Analysis")]
	Stats(stats::StatsArgs),
	
	#[command(about = "Calculate correlation matrices")]
	Correlations(correlations::CorrelationsArgs),
	
	// Data Manipulation
	#[command(about = "Select specific columns or rows")]
	#[command(next_help_heading = "Data Manipulation")]
	Select(select::SelectArgs),
	
	#[command(about = "Remove columns or rows")]
	Drop(drop::DropArgs),
	
	#[command(about = "Fill missing values")]
	Fill(fill::FillArgs),
	
	#[command(about = "Filter rows by conditions")]
	Filter(filter::FilterArgs),
	
	#[command(about = "Search for values in data")]
	Search(search::SearchArgs),
	
	// Data Transformation
	#[command(about = "Add unique identifier column")]
	#[command(next_help_heading = "Data Transformation")]
	Id(id::IdArgs),
	
	#[command(about = "Randomly shuffle rows")]
	Shuffle(shuffle::ShuffleArgs),
	
	#[command(about = "Extract data samples")]
	Sample(sample::SampleArgs),
	
	#[command(about = "Remove duplicate rows or columns")]
	Dedup(dedup::DedupArgs),
	
	// Data Combination
	#[command(about = "Join two datasets")]
	#[command(next_help_heading = "Data Combination")]
	Merge(merge::MergeArgs),
	
	#[command(about = "Concatenate multiple datasets")]
	Append(append::AppendArgs),
	
	#[command(about = "Split data into multiple files")]
	Split(split::SplitArgs),
	
	// Format Conversion
	#[command(about = "Convert between file formats")]
	#[command(next_help_heading = "Format Conversion")]
	Convert(convert::ConvertArgs),
}


// File: tests/common/common.rs
// Base: common

use arrow::array::{Float64Array, Int64Array, StringArray};
use arrow::record_batch::RecordBatch;
use arrow_schema::{DataType, Field, Schema};
use parquet::arrow::ArrowWriter;
use std::fs::File;
use std::path::Path;
use std::sync::Arc;

pub fn create_sample_parquet(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
    let schema = Arc::new(Schema::new(vec![
        Field::new("id", DataType::Int64, false),
        Field::new("name", DataType::Utf8, true),
        Field::new("value", DataType::Float64, true),
    ]));

    let id_array = Int64Array::from(vec![1, 2, 3, 4, 5]);
    let name_array = StringArray::from(vec![
        Some("Alice"),
        Some("Bob"),
        Some("Charlie"),
        None,
        Some("Eve"),
    ]);
    let value_array =
        Float64Array::from(vec![Some(100.0), None, Some(300.0), Some(400.0), Some(500.0)]);

    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![
            Arc::new(id_array),
            Arc::new(name_array),
            Arc::new(value_array),
        ],
    )?;

    let file = File::create(path)?;
    let mut writer = ArrowWriter::try_new(file, schema, None)?;
    writer.write(&batch)?;
    writer.close()?;

    Ok(())
}

pub fn create_sample2_parquet(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
    let schema = Arc::new(Schema::new(vec![
        Field::new("id", DataType::Int64, false),
        Field::new("category", DataType::Utf8, false),
        Field::new("score", DataType::Float64, true),
    ]));

    let id_array = Int64Array::from(vec![4, 5, 6, 7]);
    let category_array = StringArray::from(vec!["A", "B", "A", "C"]);
    let score_array = Float64Array::from(vec![Some(88.0), Some(92.5), None, Some(88.0)]);

    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![
            Arc::new(id_array),
            Arc::new(category_array),
            Arc::new(score_array),
        ],
    )?;

    let file = File::create(path)?;
    let mut writer = ArrowWriter::try_new(file, schema, None)?;
    writer.write(&batch)?;
    writer.close()?;

    Ok(())
}


// File: src/commands/fill.rs
// Base: fill

use clap::Args;
use datafusion::prelude::*;
use datafusion::arrow::array::{Float64Array, Int64Array, Array};
use std::path::PathBuf;
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;
use crate::utils::stats::select_columns_by_pattern;

#[derive(Args, Clone)]
pub struct FillArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(long, help = "Fill method", value_enum, default_value = "value")]
	pub method: FillMethod,
	
	#[arg(long, help = "Fill value (required for 'value' method)")]
	pub value: Option<String>,
	
	#[arg(short, long, help = "Comma-separated column names to fill")]
	pub columns: Option<String>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

#[derive(clap::ValueEnum, Clone, Debug)]
pub enum FillMethod {
	Value,
	Mean,
	Median,
	Mode,
	Forward,
	Backward,
}

pub async fn execute(args: FillArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	
	let columns = if let Some(col_spec) = &args.columns {
		let schema = df.schema();
		select_columns_by_pattern(schema.clone().into(), col_spec)?
	} else {
		df.schema().fields().iter().map(|f| f.name().clone()).collect()
	};
	
	if args.verbose {
		eprintln!("Filling missing values in {} columns using {:?} method", columns.len(), args.method);
	}
	
	let result_df = fill_missing_values(&df, &columns, &args.method, args.value.as_deref(), args.jobs).await?;
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&result_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&result_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

async fn fill_missing_values(
	df: &DataFrame,
	columns: &[String],
	method: &FillMethod,
	value: Option<&str>,
	jobs: Option<usize>,
) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let schema = df.schema();
	let mut select_exprs = Vec::new();
	
	for field in schema.fields() {
		let field_name = field.name();
		
		if columns.contains(field_name) {
			let filled_expr = match method {
				FillMethod::Value => {
					let fill_val = value.unwrap();
					match field.data_type() {
						datafusion::arrow::datatypes::DataType::Int64 => {
							let val: i64 = fill_val.parse()
								.map_err(|_| NailError::InvalidArgument(format!("Invalid integer value: {}", fill_val)))?;
							coalesce(vec![Expr::Column(datafusion::common::Column::new(None::<String>, field_name)), lit(val)])
						},
						datafusion::arrow::datatypes::DataType::Float64 => {
							let val: f64 = fill_val.parse()
								.map_err(|_| NailError::InvalidArgument(format!("Invalid float value: {}", fill_val)))?;
							coalesce(vec![Expr::Column(datafusion::common::Column::new(None::<String>, field_name)), lit(val)])
						},
						datafusion::arrow::datatypes::DataType::Utf8 => {
							coalesce(vec![Expr::Column(datafusion::common::Column::new(None::<String>, field_name)), lit(fill_val)])
						},
						_ => Expr::Column(datafusion::common::Column::new(None::<String>, field_name)),
					}
				},
				FillMethod::Mean => {
					// Use DataFusion's built-in avg function instead of manual calculation
					match field.data_type() {
						datafusion::arrow::datatypes::DataType::Float64 | 
						datafusion::arrow::datatypes::DataType::Int64 => {
							// Create a subquery to calculate the mean
							let _mean_sql = format!(
								"SELECT AVG({}) as mean_val FROM {}",
								field_name, table_name
							);
							
							// Use coalesce with a scalar subquery
							coalesce(vec![
								Expr::Column(datafusion::common::Column::new(None::<String>, field_name)),
								lit(0.0) // This will be replaced by actual mean calculation below
							])
						},
						_ => {
							return Err(NailError::Statistics(format!("Mean calculation not supported for column '{}' of type {:?}", field_name, field.data_type())));
						},
					}
				},
				FillMethod::Median => {
					match field.data_type() {
						datafusion::arrow::datatypes::DataType::Float64 | 
						datafusion::arrow::datatypes::DataType::Int64 => {
							coalesce(vec![
								Expr::Column(datafusion::common::Column::new(None::<String>, field_name)),
								lit(0.0) // This will be replaced by actual median calculation below
							])
						},
						_ => {
							return Err(NailError::Statistics(format!("Median calculation not supported for column '{}' of type {:?}", field_name, field.data_type())));
						},
					}
				},
				FillMethod::Mode => {
					// Mode is the most frequent value
					coalesce(vec![
						Expr::Column(datafusion::common::Column::new(None::<String>, field_name)),
						lit("") // This will be replaced by actual mode calculation below
					])
				},
				FillMethod::Forward => {
					// Forward fill - use LAG window function to get previous non-null value
					// This is a simplified implementation
					Expr::Column(datafusion::common::Column::new(None::<String>, field_name))
				},
				FillMethod::Backward => {
					// Backward fill - use LEAD window function to get next non-null value
					// This is a simplified implementation
					Expr::Column(datafusion::common::Column::new(None::<String>, field_name))
				},
			};
			
			select_exprs.push(filled_expr.alias(field_name));
		} else {
			select_exprs.push(Expr::Column(datafusion::common::Column::new(None::<String>, field_name)));
		}
	}
	
	// For statistical methods and fill methods, we need special handling
	if matches!(method, FillMethod::Mean | FillMethod::Median | FillMethod::Mode) {
		let batches = df.clone().collect().await?;
		let mut means = std::collections::HashMap::<String, f64>::new();
		let mut medians = std::collections::HashMap::<String, f64>::new();
		let mut modes = std::collections::HashMap::<String, String>::new();
		
		// Calculate statistical values for each target column
		for col_name in columns {
			let _field = schema.fields().iter()
				.find(|f| f.name() == col_name)
				.ok_or_else(|| NailError::Statistics(format!("Column '{}' not found", col_name)))?;
			
			let idx = schema.fields().iter()
				.position(|f| f.name() == col_name)
				.unwrap();
			
			let mut values = Vec::<f64>::new();
			let mut string_values = Vec::<String>::new();
			
			for batch in &batches {
				let array = batch.column(idx);
				if let Some(farr) = array.as_any().downcast_ref::<Float64Array>() {
					for i in 0..farr.len() {
						if farr.is_valid(i) {
							values.push(farr.value(i));
						}
					}
				} else if let Some(iarr) = array.as_any().downcast_ref::<Int64Array>() {
					for i in 0..iarr.len() {
						if iarr.is_valid(i) {
							values.push(iarr.value(i) as f64);
						}
					}
				} else if let Some(sarr) = array.as_any().downcast_ref::<arrow::array::StringArray>() {
					for i in 0..sarr.len() {
						if sarr.is_valid(i) {
							string_values.push(sarr.value(i).to_string());
						}
					}
				}
			}
			
			if values.is_empty() && string_values.is_empty() {
				return Err(NailError::Statistics(format!("No non-null values found in column '{}'", col_name)));
			}
			
			// Calculate mean
			if !values.is_empty() {
				let sum: f64 = values.iter().sum();
				means.insert(col_name.clone(), sum / values.len() as f64);
				
				// Calculate median
				let mut sorted_values = values.clone();
				sorted_values.sort_by(|a, b| a.partial_cmp(b).unwrap());
				let median = if sorted_values.len() % 2 == 0 {
					let mid = sorted_values.len() / 2;
					(sorted_values[mid - 1] + sorted_values[mid]) / 2.0
				} else {
					sorted_values[sorted_values.len() / 2]
				};
				medians.insert(col_name.clone(), median);
			}
			
			// Calculate mode (most frequent value)
			if !string_values.is_empty() {
				let mut frequency = std::collections::HashMap::<String, usize>::new();
				for val in &string_values {
					*frequency.entry(val.clone()).or_insert(0) += 1;
				}
				if let Some((mode_val, _)) = frequency.iter().max_by_key(|(_, &count)| count) {
					modes.insert(col_name.clone(), mode_val.clone());
				}
			} else if !values.is_empty() {
				// For numeric values, convert to string for mode calculation
				let mut frequency = std::collections::HashMap::<String, usize>::new();
				for val in &values {
					let val_str = val.to_string();
					*frequency.entry(val_str).or_insert(0) += 1;
				}
				if let Some((mode_val, _)) = frequency.iter().max_by_key(|(_, &count)| count) {
					modes.insert(col_name.clone(), mode_val.clone());
				}
			}
		}
		
		// Now rebuild select expressions with actual calculated values
		select_exprs.clear();
		for field in schema.fields() {
			let field_name = field.name();
			
			if columns.contains(field_name) {
				let filled_expr = match method {
					FillMethod::Mean => {
						if let Some(&mean_val) = means.get(field_name) {
							match field.data_type() {
								datafusion::arrow::datatypes::DataType::Float64 => {
									coalesce(vec![Expr::Column(datafusion::common::Column::new(None::<String>, field_name)), lit(mean_val)])
								},
								datafusion::arrow::datatypes::DataType::Int64 => {
									coalesce(vec![Expr::Column(datafusion::common::Column::new(None::<String>, field_name)), lit(mean_val)])
								},
								_ => Expr::Column(datafusion::common::Column::new(None::<String>, field_name)),
							}
						} else {
							Expr::Column(datafusion::common::Column::new(None::<String>, field_name))
						}
					},
					FillMethod::Median => {
						if let Some(&median_val) = medians.get(field_name) {
							match field.data_type() {
								datafusion::arrow::datatypes::DataType::Float64 => {
									coalesce(vec![Expr::Column(datafusion::common::Column::new(None::<String>, field_name)), lit(median_val)])
								},
								datafusion::arrow::datatypes::DataType::Int64 => {
									coalesce(vec![Expr::Column(datafusion::common::Column::new(None::<String>, field_name)), lit(median_val)])
								},
								_ => Expr::Column(datafusion::common::Column::new(None::<String>, field_name)),
							}
						} else {
							Expr::Column(datafusion::common::Column::new(None::<String>, field_name))
						}
					},
					FillMethod::Mode => {
						if let Some(mode_val) = modes.get(field_name) {
							match field.data_type() {
								datafusion::arrow::datatypes::DataType::Utf8 => {
									coalesce(vec![Expr::Column(datafusion::common::Column::new(None::<String>, field_name)), lit(mode_val.as_str())])
								},
								datafusion::arrow::datatypes::DataType::Float64 => {
									if let Ok(num_val) = mode_val.parse::<f64>() {
										coalesce(vec![Expr::Column(datafusion::common::Column::new(None::<String>, field_name)), lit(num_val)])
									} else {
										Expr::Column(datafusion::common::Column::new(None::<String>, field_name))
									}
								},
								datafusion::arrow::datatypes::DataType::Int64 => {
									if let Ok(num_val) = mode_val.parse::<i64>() {
										coalesce(vec![Expr::Column(datafusion::common::Column::new(None::<String>, field_name)), lit(num_val)])
									} else {
										Expr::Column(datafusion::common::Column::new(None::<String>, field_name))
									}
								},
								_ => Expr::Column(datafusion::common::Column::new(None::<String>, field_name)),
							}
						} else {
							Expr::Column(datafusion::common::Column::new(None::<String>, field_name))
						}
					},
					_ => Expr::Column(datafusion::common::Column::new(None::<String>, field_name)),
				};
				select_exprs.push(filled_expr.alias(field_name));
			} else {
				select_exprs.push(Expr::Column(datafusion::common::Column::new(None::<String>, field_name)));
			}
		}
	} else if matches!(method, FillMethod::Forward | FillMethod::Backward) {
		// For forward/backward fill, we need to process the data row by row
		// This is a simplified implementation that just returns the original data
		// A full implementation would require complex window functions
		return Ok(df.clone());
	}
	
	let result = ctx.table(table_name).await?.select(select_exprs)?;
	Ok(result)
}


// File: src/commands/search.rs
// Base: search

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use crate::error::{NailError, NailResult};
use crate::utils::io::read_data;
use crate::utils::format::display_dataframe;
use crate::utils::stats::select_columns_by_pattern;

#[derive(Args, Clone)]
pub struct SearchArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(long, help = "Value to search for")]
	pub value: String,
	
	#[arg(short, long, help = "Comma-separated column names to search in")]
	pub columns: Option<String>,
	
	#[arg(short, long, help = "Return matching row numbers only")]
	pub rows: bool,
	
	#[arg(long, help = "Case-insensitive search")]
	pub ignore_case: bool,
	
	#[arg(long, help = "Exact match only (no partial matches)")]
	pub exact: bool,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: SearchArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Searching in: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let schema = df.schema();
	
	let search_columns = if let Some(col_spec) = &args.columns {
		select_columns_by_pattern(schema.clone().into(), col_spec)?
	} else {
		schema.fields().iter().map(|f| f.name().clone()).collect()
	};
	
	if args.verbose {
		eprintln!("Searching for '{}' in {} columns: {:?}", 
			args.value, search_columns.len(), search_columns);
	}
	
	let result_df = if args.rows {
		search_return_row_numbers(&df, &args.value, &search_columns, args.ignore_case, args.exact, args.jobs).await?
	} else {
		search_return_matching_rows(&df, &args.value, &search_columns, args.ignore_case, args.exact, args.jobs).await?
	};
	
	display_dataframe(&result_df, args.output.as_deref(), args.format.as_ref()).await?;
	
	Ok(())
}

async fn search_return_matching_rows(
	df: &DataFrame,
	search_value: &str,
	columns: &[String],
	ignore_case: bool,
	exact: bool,
	jobs: Option<usize>,
) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let mut conditions = Vec::new();
	
	for column in columns {
		let field = df.schema().field_with_name(None, column)
			.map_err(|_| NailError::ColumnNotFound(column.clone()))?;
		
		let condition = match field.data_type() {
			datafusion::arrow::datatypes::DataType::Utf8 => {
				let search_expr = if ignore_case {
					format!("LOWER(\"{}\")", column)
				} else {
					format!("\"{}\"", column)
				};
				
				let value_expr = if ignore_case {
					search_value.to_lowercase()
				} else {
					search_value.to_string()
				};
				
				if exact {
					format!("{} = '{}'", search_expr, value_expr)
				} else {
					format!("{} LIKE '%{}%'", search_expr, value_expr)
				}
			},
			datafusion::arrow::datatypes::DataType::Int64 | 
			datafusion::arrow::datatypes::DataType::Float64 => {
				if let Ok(num_value) = search_value.parse::<f64>() {
					if exact {
						format!("\"{}\" = {}", column, num_value)
					} else {
						format!("CAST(\"{}\" AS VARCHAR) LIKE '%{}%'", column, search_value)
					}
				} else {
					continue;
				}
			},
			_ => continue,
		};
		
		conditions.push(condition);
	}
	
	if conditions.is_empty() {
		return Err(NailError::InvalidArgument("No searchable columns found".to_string()));
	}
	
	let where_clause = conditions.join(" OR ");
	let sql = format!("SELECT * FROM {} WHERE {}", table_name, where_clause);
	
	let result = ctx.sql(&sql).await?;
	Ok(result)
}

async fn search_return_row_numbers(
	df: &DataFrame,
	search_value: &str,
	columns: &[String],
	ignore_case: bool,
	exact: bool,
	jobs: Option<usize>,
) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let mut conditions = Vec::new();
	
	for column in columns {
		let field = df.schema().field_with_name(None, column)
			.map_err(|_| NailError::ColumnNotFound(column.clone()))?;
		
		let condition = match field.data_type() {
			datafusion::arrow::datatypes::DataType::Utf8 => {
				let search_expr = if ignore_case {
					format!("LOWER(\"{}\")", column)
				} else {
					format!("\"{}\"", column)
				};
				
				let value_expr = if ignore_case {
					search_value.to_lowercase()
				} else {
					search_value.to_string()
				};
				
				if exact {
					format!("{} = '{}'", search_expr, value_expr)
				} else {
					format!("{} LIKE '%{}%'", search_expr, value_expr)
				}
			},
			datafusion::arrow::datatypes::DataType::Int64 | 
			datafusion::arrow::datatypes::DataType::Float64 => {
				if let Ok(num_value) = search_value.parse::<f64>() {
					if exact {
						format!("\"{}\" = {}", column, num_value)
					} else {
						format!("CAST(\"{}\" AS VARCHAR) LIKE '%{}%'", column, search_value)
					}
				} else {
					continue;
				}
			},
			_ => continue,
		};
		
		conditions.push(condition);
	}
	
	if conditions.is_empty() {
		return Err(NailError::InvalidArgument("No searchable columns found".to_string()));
	}
	
	let where_clause = conditions.join(" OR ");
	let sql = format!(
		"SELECT ROW_NUMBER() OVER() as row_number, '{}' as search_value, '{}' as matched_columns 
		 FROM {} WHERE {}",
		search_value, columns.join(","), table_name, where_clause
	);
	
	let result = ctx.sql(&sql).await?;
	Ok(result)
}


// File: src/main.rs
// Base: main

mod cli;
mod commands;
mod error;
mod utils;

use cli::Cli;
use error::NailResult;
pub use crate::commands::select::{select_columns_by_pattern, parse_row_specification};

#[tokio::main]
async fn main() -> NailResult<()> {
	let cli = Cli::parse_with_width();
	
	match cli.command {
		commands::Commands::Head(args) => commands::head::execute(args).await,
		commands::Commands::Tail(args) => commands::tail::execute(args).await,
		commands::Commands::Preview(args) => commands::preview::execute(args).await,
		commands::Commands::Headers(args) => commands::headers::execute(args).await,
		commands::Commands::Schema(args) => commands::schema::execute(args).await,
		commands::Commands::Count(args) => commands::count::execute(args).await,
		commands::Commands::Size(args) => commands::size::execute(args).await,
		commands::Commands::Stats(args) => commands::stats::execute(args).await,
		commands::Commands::Correlations(args) => commands::correlations::execute(args).await,
		commands::Commands::Select(args) => commands::select::execute(args).await,
		commands::Commands::Drop(args) => commands::drop::execute(args).await,
		commands::Commands::Fill(args) => commands::fill::execute(args).await,
		commands::Commands::Filter(args) => commands::filter::execute(args).await,
		commands::Commands::Search(args) => commands::search::execute(args).await,
		commands::Commands::Id(args) => commands::id::execute(args).await,
		commands::Commands::Shuffle(args) => commands::shuffle::execute(args).await,
		commands::Commands::Sample(args) => commands::sample::execute(args).await,
		commands::Commands::Dedup(args) => commands::dedup::execute(args).await,
		commands::Commands::Merge(args) => commands::merge::execute(args).await,
		commands::Commands::Append(args) => commands::append::execute(args).await,
		commands::Commands::Split(args) => commands::split::execute(args).await,
		commands::Commands::Convert(args) => commands::convert::execute(args).await,
	}
}


// File: src/utils/mod.rs
// Base: mod

pub mod io;
pub mod format;
pub mod stats;

use datafusion::prelude::*;
use std::path::Path;
use crate::error::{NailError, NailResult};

pub async fn create_context() -> NailResult<SessionContext> {
	let cpu_count = num_cpus::get();
	let target_partitions = std::cmp::max(1, cpu_count / 2);
	
	let config = SessionConfig::new()
		.with_batch_size(8192)
		.with_target_partitions(target_partitions);
	
	Ok(SessionContext::new_with_config(config))
}

pub async fn create_context_with_jobs(jobs: Option<usize>) -> NailResult<SessionContext> {
	let cpu_count = num_cpus::get();
	let target_partitions = if let Some(j) = jobs {
		std::cmp::max(1, std::cmp::min(j, cpu_count))
	} else {
		std::cmp::max(1, cpu_count / 2)
	};
	
	let config = SessionConfig::new()
		.with_batch_size(8192)
		.with_target_partitions(target_partitions);
	
	Ok(SessionContext::new_with_config(config))
}

pub fn detect_file_format(path: &Path) -> NailResult<FileFormat> {
	match path.extension().and_then(|s| s.to_str()) {
		Some("parquet") => Ok(FileFormat::Parquet),
		Some("csv") => Ok(FileFormat::Csv),
		Some("json") => Ok(FileFormat::Json),
		Some("xlsx") => Ok(FileFormat::Excel),
		_ => Err(NailError::UnsupportedFormat(
			format!("Unable to detect format for file: {}", path.display())
		)),
	}
}

#[derive(Debug, Clone)]
pub enum FileFormat {
	Parquet,
	Csv,
	Json,
	Excel,
}


// File: src/commands/append.rs
// Base: append

use clap::Args;
use datafusion::prelude::*;
use datafusion::common::DFSchemaRef;
use std::path::PathBuf;
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct AppendArgs {
	#[arg(short, long, help = "Input file (base table)")]
	pub input: PathBuf,
	
	#[arg(long, help = "Files to append (comma-separated)")]
	pub files: String,
	
	#[arg(long, help = "Ignore schema mismatches")]
	pub ignore_schema: bool,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: AppendArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading base table from: {}", args.input.display());
	}
	
	let mut base_df = read_data(&args.input).await?;
	let base_schema: DFSchemaRef = base_df.schema().clone().into();
	
	let append_files: Vec<&str> = args.files.split(',').map(|s| s.trim()).collect();
	
	if args.verbose {
		eprintln!("Appending {} files", append_files.len());
	}
	
	for file_path in append_files {
		let path = PathBuf::from(file_path);
		
		if args.verbose {
			eprintln!("Appending: {}", path.display());
		}
		
		let append_df = read_data(&path).await?;
		let append_schema: DFSchemaRef = append_df.schema().clone().into();
		
		if !args.ignore_schema && !schemas_compatible(&base_schema, &append_schema) {
			return Err(NailError::InvalidArgument(format!(
				"Schema mismatch in file: {}. Use --ignore-schema to force append.",
				path.display()
			)));
		}
		
		let aligned_df = if args.ignore_schema {
			align_schemas(&append_df, &base_schema, args.jobs).await?
		} else {
			append_df
		};
		
		base_df = base_df.union(aligned_df)?;
	}
	
	if args.verbose {
		let total_rows = base_df.clone().count().await?;
		eprintln!("Final dataset contains {} rows", total_rows);
	}
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&base_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&base_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

fn schemas_compatible(schema1: &datafusion::common::DFSchemaRef, schema2: &datafusion::common::DFSchemaRef) -> bool {
	if schema1.fields().len() != schema2.fields().len() {
		return false;
	}
	
	for (field1, field2) in schema1.fields().iter().zip(schema2.fields().iter()) {
		if field1.name() != field2.name() || field1.data_type() != field2.data_type() {
			return false;
		}
	}
	
	true
}

async fn align_schemas(df: &DataFrame, target_schema: &datafusion::common::DFSchemaRef, jobs: Option<usize>) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let current_schema = df.schema();
	let mut select_exprs = Vec::new();
	
	for target_field in target_schema.fields() {
		let target_name = target_field.name();
		
		if let Ok(_current_field) = current_schema.field_with_name(None, target_name) {
			select_exprs.push(Expr::Column(datafusion::common::Column::new(None::<String>, target_name)));
		} else {
			let null_expr = match target_field.data_type() {
				datafusion::arrow::datatypes::DataType::Int64 => lit(0i64).alias(target_name),
				datafusion::arrow::datatypes::DataType::Float64 => lit(0.0f64).alias(target_name),
				datafusion::arrow::datatypes::DataType::Utf8 => lit("").alias(target_name),
				datafusion::arrow::datatypes::DataType::Boolean => lit(false).alias(target_name),
				_ => lit("").alias(target_name),
			};
			select_exprs.push(null_expr);
		}
	}
	
	let result = ctx.table(table_name).await?.select(select_exprs)?;
	Ok(result)
}


// File: src/cli.rs
// Base: cli

use clap::{Parser, ColorChoice, CommandFactory, FromArgMatches};
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "nail")]
#[command(about = "A fast data utility written in Rust")]
#[command(long_about = "Nail, a parquet command-line tool for data manipulation, analysis, and format conversion.")]
#[command(version = "1.3.0")]
#[command(author = "Johan HG Natter")]
#[command(color = ColorChoice::Auto)]
#[command(styles = clap::builder::Styles::styled()
	.header(clap::builder::styling::AnsiColor::Yellow.on_default().bold())
	.usage(clap::builder::styling::AnsiColor::Green.on_default().bold())
	.literal(clap::builder::styling::AnsiColor::Blue.on_default().bold())
	.placeholder(clap::builder::styling::AnsiColor::Cyan.on_default())
	.error(clap::builder::styling::AnsiColor::Red.on_default().bold())
	.valid(clap::builder::styling::AnsiColor::Green.on_default().bold())
	.invalid(clap::builder::styling::AnsiColor::Red.on_default().bold())
)]
pub struct Cli {
	#[command(subcommand)]
	pub command: crate::commands::Commands,
	
	#[arg(short, long, global = true, help = "Enable verbose output")]
	pub verbose: bool,
	
	#[arg(short, long, global = true, help = "Number of parallel jobs (default: half of CPU cores)")]
	pub jobs: Option<usize>,
}

impl Cli {
	pub fn parse_with_width() -> Self {
		let width = if let Some((w, _)) = term_size::dimensions() {
			Some(w.max(80).min(200))
		} else {
			Some(120)
		};
		
		let mut cmd = Self::command();
		if let Some(w) = width {
			cmd = cmd.term_width(w);
		}
		
		Self::from_arg_matches(&cmd.get_matches()).unwrap()
	}
}

#[derive(clap::Args, Clone)]
pub struct GlobalArgs {
	#[arg(short, long, help = "Input file")]
	pub input: Option<PathBuf>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format (auto-detect by default)", value_enum)]
	pub format: Option<OutputFormat>,
	
	#[arg(long, help = "Random seed for reproducible results")]
	pub random: Option<u64>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
}

#[derive(clap::ValueEnum, Clone, Debug)]
pub enum OutputFormat {
	Json,
	Text,
	Csv,
	Parquet,
}


// File: src/commands/count.rs
// Base: count

use clap::Args;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::read_data;

#[derive(Args, Clone)]
pub struct CountArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: CountArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let row_count = df.clone().count().await?;
	
	if args.verbose {
		eprintln!("Counted {} rows", row_count);
	}
	
	// Output the count
	if let Some(output_path) = &args.output {
		match args.format.as_ref().unwrap_or(&crate::cli::OutputFormat::Text) {
			crate::cli::OutputFormat::Json => {
				let json_output = format!(r#"{{"row_count": {}}}"#, row_count);
				std::fs::write(output_path, json_output)?;
			},
			crate::cli::OutputFormat::Csv => {
				let csv_output = format!("row_count\n{}", row_count);
				std::fs::write(output_path, csv_output)?;
			},
			_ => {
				std::fs::write(output_path, row_count.to_string())?;
			}
		}
		if args.verbose {
			eprintln!("Count written to: {}", output_path.display());
		}
	} else {
		println!("{}", row_count);
	}
	
	Ok(())
} 


// File: src/commands/headers.rs
// Base: headers

use clap::Args;
use std::path::PathBuf;
use regex::Regex;
use crate::error::{NailError, NailResult};
use crate::utils::io::read_data;

#[derive(Args, Clone)]
pub struct HeadersArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(long, help = "Filter headers with regex pattern")]
	pub filter: Option<String>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: HeadersArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading schema from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let schema = df.schema();
	let field_names: Vec<String> = schema.fields().iter()
		.map(|f| f.name().clone())
		.collect();
	
	let filtered_names = if let Some(pattern) = &args.filter {
		let regex = Regex::new(pattern)
			.map_err(|e| NailError::InvalidArgument(format!("Invalid regex pattern: {}", e)))?;
		
		field_names.into_iter()
			.filter(|name| regex.is_match(name))
			.collect()
	} else {
		field_names
	};
	
	if args.verbose {
		eprintln!("Found {} headers", filtered_names.len());
	}
	
	match &args.output {
		Some(output_path) => {
			let content = match args.format {
				Some(crate::cli::OutputFormat::Json) => {
					serde_json::to_string_pretty(&filtered_names)?
				},
				_ => filtered_names.join("\n"),
			};
			std::fs::write(output_path, content)?;
		},
		None => {
			for name in filtered_names {
				println!("{}", name);
			}
		},
	}
	
	Ok(())
}


// File: src/commands/head.rs
// Base: head

use clap::Args;

use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::read_data;
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct HeadArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Number of rows to display", default_value = "5")]
	pub number: usize,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: HeadArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let limited_df = df.limit(0, Some(args.number))?;
	
	if args.verbose {
		eprintln!("Displaying first {} rows", args.number);
	}
	
	display_dataframe(&limited_df, args.output.as_deref(), args.format.as_ref()).await?;
	
	Ok(())
}


// File: src/commands/stats.rs
// Base: stats

use clap::Args;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::read_data;
use crate::utils::format::display_dataframe;
use crate::utils::stats::{calculate_basic_stats, calculate_exhaustive_stats, calculate_hypothesis_tests, select_columns_by_pattern};

#[derive(Args, Clone)]
pub struct StatsArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Comma-separated column names or regex patterns")]
	pub columns: Option<String>,
	
	#[arg(short = 't', long, help = "Statistics type", value_enum, default_value = "basic")]
	pub stats_type: StatsType,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

#[derive(clap::ValueEnum, Clone, Debug)]
pub enum StatsType {
	Basic,
	Exhaustive,
	Hypothesis,
}

pub async fn execute(args: StatsArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let schema = df.schema();
	
	let target_columns = if let Some(col_spec) = &args.columns {
		select_columns_by_pattern(schema.clone().into(), col_spec)?
	} else {
		schema.fields().iter().map(|f| f.name().clone()).collect()
	};
	
	if args.verbose {
		eprintln!("Computing {:?} statistics for {} columns", args.stats_type, target_columns.len());
	}
	
	let stats_df = match args.stats_type {
		StatsType::Basic => calculate_basic_stats(&df, &target_columns).await?,
		StatsType::Exhaustive => calculate_exhaustive_stats(&df, &target_columns).await?,
		StatsType::Hypothesis => calculate_hypothesis_tests(&df, &target_columns).await?,
	};
	
	display_dataframe(&stats_df, args.output.as_deref(), args.format.as_ref()).await?;
	
	// Print overall row count for basic stats when outputting to console
	if args.output.is_none() && args.format.is_none() {
		let total_rows = read_data(&args.input).await?.clone().count().await?;
		println!("count | {}", total_rows);
	}
	
	Ok(())
}


// File: src/commands/drop.rs
// Base: drop

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;
use crate::commands::select::{select_columns_by_pattern, parse_row_specification};

#[derive(Args, Clone)]
pub struct DropArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Column names or regex patterns to drop (comma-separated)")]
	pub columns: Option<String>,
	
	#[arg(short, long, help = "Row numbers or ranges to drop (e.g., 1,3,5-10)")]
	pub rows: Option<String>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: DropArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let mut result_df = df;
	
	if let Some(col_spec) = &args.columns {
		let schema = result_df.schema();
		let columns_to_drop = select_columns_by_pattern(schema.clone().into(), col_spec)?;
		
		if args.verbose {
			eprintln!("Dropping {} columns: {:?}", columns_to_drop.len(), columns_to_drop);
		}
		
		let remaining_columns: Vec<Expr> = result_df.schema().fields().iter()
			.filter(|f| !columns_to_drop.contains(f.name()))
			.map(|f| Expr::Column(datafusion::common::Column::new(None::<String>, f.name())))
			.collect();
		
		result_df = result_df.select(remaining_columns)?;
	}
	
	if let Some(row_spec) = &args.rows {
		let row_indices = parse_row_specification(row_spec)?;
		
		if args.verbose {
			eprintln!("Dropping {} rows", row_indices.len());
		}
		
		result_df = drop_rows_by_indices(&result_df, &row_indices, args.jobs).await?;
	}
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&result_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&result_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

async fn drop_rows_by_indices(df: &DataFrame, indices: &[usize], jobs: Option<usize>) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let indices_str = indices.iter()
		.map(|&i| (i + 1).to_string())
		.collect::<Vec<_>>()
		.join(",");
	
	// Get the original column names and quote them to preserve case
	let original_columns: Vec<String> = df.schema().fields().iter()
		.map(|f| format!("\"{}\"", f.name()))
		.collect();
	
	let sql = format!(
		"SELECT {} FROM (SELECT {}, ROW_NUMBER() OVER() as rn FROM {}) WHERE rn NOT IN ({})",
		original_columns.join(", "),
		original_columns.join(", "),
		table_name, 
		indices_str
	);
	
	let result = ctx.sql(&sql).await?;
	Ok(result)
}


// File: src/commands/preview.rs
// Base: preview

use clap::Args;
use std::path::PathBuf;
use rand::seq::SliceRandom;
use rand::SeedableRng;
use rand::rngs::StdRng;
use crate::error::NailResult;
use crate::utils::io::read_data;
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct PreviewArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Number of rows to display", default_value = "5")]
	pub number: usize,
	
	#[arg(short, long, help = "Random seed for reproducible results")]
	pub random: Option<u64>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: PreviewArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let total_rows = df.clone().count().await?;
	
	if total_rows <= args.number {
		display_dataframe(&df, args.output.as_deref(), args.format.as_ref()).await?;
		return Ok(());
	}
	
	let mut rng = match args.random {
		Some(seed) => StdRng::seed_from_u64(seed),
		None => StdRng::from_entropy(),
	};
	
	let mut indices: Vec<usize> = (0..total_rows).collect();
	indices.shuffle(&mut rng);
	indices.truncate(args.number);
	indices.sort();
	
	if args.verbose {
		eprintln!("Randomly sampling {} rows from {} total rows", args.number, total_rows);
	}
	
	let ctx = crate::utils::create_context_with_jobs(args.jobs).await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let indices_str = indices.iter()
		.map(|&i| (i + 1).to_string())
		.collect::<Vec<_>>()
		.join(",");
	
	// Get the original column names and quote them to preserve case
	let original_columns: Vec<String> = df.schema().fields().iter()
		.map(|f| format!("\"{}\"", f.name()))
		.collect();
	
	let sql = format!(
		"SELECT {} FROM (SELECT {}, ROW_NUMBER() OVER() as rn FROM {}) WHERE rn IN ({})",
		original_columns.join(", "),
		original_columns.join(", "),
		table_name, 
		indices_str
	);
	
	if args.verbose {
		eprintln!("Executing SQL: {}", sql);
	}
	
	let result = ctx.sql(&sql).await?;
	
	display_dataframe(&result, args.output.as_deref(), args.format.as_ref()).await?;
	
	Ok(())
}


// File: src/commands/split.rs
// Base: split

use clap::Args;
use std::path::PathBuf;
use std::collections::HashMap;
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};

#[derive(Args, Clone)]
pub struct SplitArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(long, help = "Split ratios (e.g., '0.7,0.2,0.1' or '70,20,10')")]
	pub ratio: String,
	
	#[arg(long, help = "Output file names (comma-separated)")]
	pub names: Option<String>,
	
	#[arg(long, help = "Prefix for auto-generated split file names", default_value = "split")]
	pub splits_prefix: String,
	
	#[arg(long, help = "Output directory for split files", default_value = ".")]
	pub output_dir: PathBuf,
	
	#[arg(long, help = "Column for stratified splitting")]
	pub stratified_by: Option<String>,
	
	#[arg(short, long, help = "Random seed for reproducible splits")]
	pub random: Option<u64>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: SplitArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	// Create output directory if it doesn't exist
	if !args.output_dir.exists() {
		std::fs::create_dir_all(&args.output_dir)?;
		if args.verbose {
			eprintln!("Created output directory: {}", args.output_dir.display());
		}
	}
	
	let df = read_data(&args.input).await?;
	let total_rows = df.clone().count().await?;
	
	let ratios = parse_ratios(&args.ratio)?;
	let file_format = determine_output_format(&args.format, &args.input);
	let extension = get_extension_for_format(&file_format);
	
	let output_names = if let Some(names) = &args.names {
		let base_names = parse_names(names)?;
		// Add extensions and output directory to the provided names
		base_names.into_iter()
			.map(|name| {
				let name_with_ext = if name.contains('.') {
					name // Keep existing extension
				} else {
					format!("{}.{}", name, extension) // Add extension
				};
				args.output_dir.join(name_with_ext)
			})
			.collect()
	} else {
		generate_names(&args.splits_prefix, ratios.len(), &args.input, &args.output_dir, &extension)
	};
	
	if ratios.len() != output_names.len() {
		return Err(NailError::InvalidArgument(
			format!("Number of ratios ({}) must match number of names ({})", 
				ratios.len(), output_names.len())
		));
	}
	
	if let Some(stratify_col) = &args.stratified_by {
		if args.verbose {
			eprintln!("Performing stratified split by column '{}' with ratios: {:?}", 
				stratify_col, ratios);
		}
		stratified_split(&df, &ratios, &output_names, stratify_col, args.random, &file_format, args.verbose, args.jobs).await?;
	} else {
		if args.verbose {
			eprintln!("Splitting {} rows into {} parts with ratios: {:?}", 
				total_rows, ratios.len(), ratios);
		}
		random_split(&df, &ratios, &output_names, args.random, &file_format, args.verbose, args.jobs).await?;
	}
	
	if args.verbose {
		eprintln!("Split complete: {} files created in {}", output_names.len(), args.output_dir.display());
		for (i, output_name) in output_names.iter().enumerate() {
			eprintln!("  Split {}: {}", i + 1, output_name.display());
		}
	}
	
	Ok(())
}

async fn stratified_split(
	df: &datafusion::prelude::DataFrame,
	ratios: &[f64],
	output_names: &[PathBuf],
	stratify_col: &str,
	seed: Option<u64>,
	file_format: &Option<crate::utils::FileFormat>,
	verbose: bool,
	jobs: Option<usize>,
) -> NailResult<()> {
	use datafusion::prelude::*;
	
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let schema = df.schema();
	let actual_col_name = schema.fields().iter()
		.find(|f| f.name().to_lowercase() == stratify_col.to_lowercase())
		.map(|f| f.name().clone())
		.ok_or_else(|| {
			let available_cols: Vec<String> = schema.fields().iter()
				.map(|f| f.name().clone())
				.collect();
			NailError::ColumnNotFound(format!(
				"Stratification column '{}' not found. Available columns: {:?}", 
				stratify_col, available_cols
			))
		})?;
	
	let distinct_sql = format!(
		"SELECT DISTINCT {} as category, COUNT(*) as count FROM {} WHERE {} IS NOT NULL GROUP BY {}",
		actual_col_name, table_name, actual_col_name, actual_col_name
	);
	
	let distinct_df = ctx.sql(&distinct_sql).await?;
	let categories_batches = distinct_df.collect().await?;
	
	let mut category_counts = HashMap::new();
	for batch in &categories_batches {
		let category_array = batch.column(0);
		let count_array = batch.column(1);
		
		for i in 0..batch.num_rows() {
			if !category_array.is_null(i) && !count_array.is_null(i) {
				let category = format!("{:?}", category_array.slice(i, 1))
					.trim_start_matches('[')
					.trim_end_matches(']')
					.trim_matches('"')
					.to_string();
				
				if let Some(count_arr) = count_array.as_any().downcast_ref::<datafusion::arrow::array::Int64Array>() {
					category_counts.insert(category, count_arr.value(i) as usize);
				}
			}
		}
	}
	
	if verbose {
		eprintln!("Found {} categories for stratification:", category_counts.len());
		for (cat, count) in &category_counts {
			eprintln!("  {}: {} rows", cat, count);
		}
	}
	
	let mut split_dfs: Vec<Option<DataFrame>> = vec![None; ratios.len()];
	
	for (category, _count) in &category_counts {
		let category_sql = format!(
			"SELECT * FROM {} WHERE {} = '{}'",
			table_name, actual_col_name, category
		);
		
		let category_df = ctx.sql(&category_sql).await?;
		let category_rows = category_df.clone().count().await?;
		
		let shuffled_category = if let Some(s) = seed {
			shuffle_dataframe_with_seed(&category_df, s + category.len() as u64, jobs).await?
		} else {
			category_df
		};
		
		let mut current_offset = 0;
		for (i, ratio) in ratios.iter().enumerate() {
			let split_size = if i == ratios.len() - 1 {
				category_rows - current_offset
			} else {
				(category_rows as f64 * ratio).round() as usize
			};
			
			if split_size > 0 {
				let category_split = shuffled_category.clone().limit(current_offset, Some(split_size))?;
				
				split_dfs[i] = Some(match &split_dfs[i] {
					None => category_split,
					Some(existing) => existing.clone().union(category_split)?,
				});
			}
			
			current_offset += split_size;
		}
	}
	
	for (i, (split_df, output_name)) in split_dfs.iter().zip(output_names.iter()).enumerate() {
		if let Some(df) = split_df {
			let row_count = df.clone().count().await?;
			if verbose {
				eprintln!("Writing split {}: {} rows -> {}", i + 1, row_count, output_name.display());
			}
			write_data(df, output_name, file_format.as_ref()).await?;
		} else {
			if verbose {
				eprintln!("Warning: Split {} is empty -> {}", i + 1, output_name.display());
			}
			let empty_df = df.clone().limit(0, Some(0))?;
			write_data(&empty_df, output_name, file_format.as_ref()).await?;
		}
	}
	
	Ok(())
}

async fn random_split(
	df: &datafusion::prelude::DataFrame,
	ratios: &[f64],
	output_names: &[PathBuf],
	seed: Option<u64>,
	file_format: &Option<crate::utils::FileFormat>,
	verbose: bool,
	jobs: Option<usize>,
) -> NailResult<()> {
	let total_rows = df.clone().count().await?;
	
	let shuffled_df = if let Some(s) = seed {
		shuffle_dataframe_with_seed(df, s, jobs).await?
	} else {
		df.clone()
	};
	
	let mut current_offset = 0;
	
	for (i, (ratio, output_name)) in ratios.iter().zip(output_names.iter()).enumerate() {
		let split_size = if i == ratios.len() - 1 {
			total_rows - current_offset
		} else {
			(total_rows as f64 * ratio).round() as usize
		};
		
		if verbose {
			eprintln!("Creating split {}: {} rows -> {}", i + 1, split_size, output_name.display());
		}
		
		let split_df = shuffled_df.clone().limit(current_offset, Some(split_size))?;
		write_data(&split_df, output_name, file_format.as_ref()).await?;
		
		current_offset += split_size;
	}
	
	Ok(())
}

fn parse_ratios(ratio_str: &str) -> NailResult<Vec<f64>> {
	let parts: Vec<&str> = ratio_str.split(',').map(|s| s.trim()).collect();
	let mut ratios = Vec::new();
	
	for part in parts {
		let ratio: f64 = part.parse()
			.map_err(|_| NailError::InvalidArgument(format!("Invalid ratio: {}", part)))?;
		if ratio <= 0.0 {
			return Err(NailError::InvalidArgument(format!("Ratio must be positive: {}", ratio)));
		}
		ratios.push(ratio);
	}
	
	let sum: f64 = ratios.iter().sum();
	
	if (sum - 1.0).abs() < 0.001 {
		Ok(ratios)
	} else if (sum - 100.0).abs() < 0.001 {
		Ok(ratios.into_iter().map(|r| r / 100.0).collect())
	} else {
		Err(NailError::InvalidArgument(
			format!("Ratios must sum to 1.0 or 100.0, got: {}", sum)
		))
	}
}

fn parse_names(names_str: &str) -> NailResult<Vec<String>> {
	Ok(names_str.split(',').map(|s| s.trim().to_string()).collect())
}

fn generate_names(prefix: &str, count: usize, _input_path: &PathBuf, output_dir: &PathBuf, extension: &str) -> Vec<PathBuf> {
	(0..count)
		.map(|i| output_dir.join(format!("{}_{}.{}", prefix, i + 1, extension)))
		.collect()
}

fn determine_output_format(format: &Option<crate::cli::OutputFormat>, input_path: &PathBuf) -> Option<crate::utils::FileFormat> {
	match format {
		Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
		Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
		Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
		_ => crate::utils::detect_file_format(input_path).ok(),
	}
}

fn get_extension_for_format(format: &Option<crate::utils::FileFormat>) -> String {
	match format {
		Some(crate::utils::FileFormat::Json) => "json".to_string(),
		Some(crate::utils::FileFormat::Csv) => "csv".to_string(),
		Some(crate::utils::FileFormat::Parquet) => "parquet".to_string(),
		Some(crate::utils::FileFormat::Excel) => "xlsx".to_string(),
		None => "parquet".to_string(), // Default
	}
}

async fn shuffle_dataframe_with_seed(df: &datafusion::prelude::DataFrame, seed: u64, jobs: Option<usize>) -> NailResult<datafusion::prelude::DataFrame> {
	use rand::{SeedableRng, seq::SliceRandom};
	use rand::rngs::StdRng;
	
	// Use the context with jobs parameter
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let total_rows = df.clone().count().await?;
	let mut rng = StdRng::seed_from_u64(seed);
	let mut indices: Vec<usize> = (0..total_rows).collect();
	indices.shuffle(&mut rng);
	
	let indices_str = indices.iter()
		.map(|&i| (i + 1).to_string())
		.collect::<Vec<_>>()
		.join(",");
	
	let original_columns: Vec<String> = df.schema().fields().iter()
		.map(|f| format!("\"{}\"", f.name()))
		.collect();
	
	let sql = format!(
		"SELECT {} FROM (SELECT {}, ROW_NUMBER() OVER() as rn FROM {}) WHERE rn IN ({})",
		original_columns.join(", "),
		original_columns.join(", "),
		table_name, 
		indices_str
	);
	
	let result = ctx.sql(&sql).await?;
	Ok(result)
}


// File: src/commands/schema.rs
// Base: schema

use clap::Args;

use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::read_data;

#[derive(Args, Clone)]
pub struct SchemaArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: SchemaArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading schema from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let schema = df.schema();
	
	let schema_info: Vec<SchemaField> = schema.fields().iter()
		.map(|field| SchemaField {
			name: field.name().clone(),
			data_type: format!("{:?}", field.data_type()),
			nullable: field.is_nullable(),
		})
		.collect();
	
	if args.verbose {
		eprintln!("Schema contains {} fields", schema_info.len());
	}
	
	match &args.output {
		Some(output_path) => {
			let content = match args.format {
				Some(crate::cli::OutputFormat::Json) => {
					serde_json::to_string_pretty(&schema_info)?
				},
				_ => {
					format!("{}
-----------------------------------
LEGEND:
  Column|Type|Nullable
-----------------------------------
{}
","GOP API Registry v1.0", schema_info.iter().map(|f| format!("{}|{}|{}", f.name, f.data_type, f.nullable)).collect::<Vec<_>>().join("\n"))
				}
			};
			std::fs::write(output_path, content)?;
		},
		None => {
			match args.format {
				Some(crate::cli::OutputFormat::Json) => {
					println!("{}", serde_json::to_string_pretty(&schema_info)?);
				},
				_ => {
					println!("{}
-----------------------------------
LEGEND:
  Column|Type|Nullable
-----------------------------------
{}
","GOP API Registry v1.0", schema_info.iter().map(|f| format!("{}|{}|{}", f.name, f.data_type, f.nullable)).collect::<Vec<_>>().join("\n"));
				}
			}
		},
	}
	
	Ok(())
}

#[derive(serde::Serialize)]
struct SchemaField {
	name: String,
	data_type: String,
	nullable: bool,
}


// File: src/commands/dedup.rs
// Base: dedup

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use std::collections::{HashMap, HashSet};
use arrow::array::*;
use arrow::record_batch::RecordBatch;
use std::sync::Arc;
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct DedupArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(long, help = "Remove duplicate rows", conflicts_with = "col_wise")]
	pub row_wise: bool,
	
	#[arg(long, help = "Remove duplicate columns", conflicts_with = "row_wise")]
	pub col_wise: bool,
	
	#[arg(short, long, help = "Columns to consider for row-wise deduplication")]
	pub columns: Option<String>,
	
	#[arg(long, help = "Keep first occurrence (default) vs last", default_value = "first")]
	pub keep: String,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: DedupArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	
	if !args.row_wise && !args.col_wise {
		return Err(NailError::InvalidArgument(
			"Must specify either --row-wise or --col-wise".to_string()
		));
	}
	
	let result_df = if args.row_wise {
		if args.verbose {
			eprintln!("Removing duplicate rows");
		}
		deduplicate_rows(&df, args.columns.as_deref(), &args.keep, args.jobs).await?
	} else {
		if args.verbose {
			eprintln!("Removing duplicate columns");
		}
		deduplicate_columns(&df).await?
	};
	
	if args.verbose {
		let original_rows = df.clone().count().await?;
		let new_rows = result_df.clone().count().await?;
		let original_cols = df.schema().fields().len();
		let new_cols = result_df.schema().fields().len();
		
		if args.row_wise {
			eprintln!("Removed {} duplicate rows ({} -> {})", 
				original_rows - new_rows, original_rows, new_rows);
		} else {
			eprintln!("Removed {} duplicate columns ({} -> {})", 
				original_cols - new_cols, original_cols, new_cols);
		}
	}
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&result_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&result_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

async fn deduplicate_rows(df: &DataFrame, columns: Option<&str>, keep: &str, _jobs: Option<usize>) -> NailResult<DataFrame> {
	// Collect the data into RecordBatches
	let batches = df.clone().collect().await?;
	if batches.is_empty() {
		return Ok(df.clone());
	}
	
	let schema = batches[0].schema();
	
	// Determine which columns to use for deduplication
	let dedup_columns = if let Some(col_spec) = columns {
		// Convert Arrow Schema to DFSchema
		let df_schema = datafusion::common::DFSchema::try_from(schema.as_ref().clone())
			.map_err(|e| NailError::InvalidArgument(format!("Failed to convert schema: {}", e)))?;
		crate::utils::stats::select_columns_by_pattern(Arc::new(df_schema), col_spec)?
	} else {
		schema.fields().iter().map(|f| f.name().clone()).collect()
	};
	
	// Find column indices
	let dedup_indices: Vec<usize> = dedup_columns.iter()
		.map(|col_name| {
			schema.fields().iter().position(|f| f.name() == col_name)
				.ok_or_else(|| NailError::InvalidArgument(format!("Column '{}' not found", col_name)))
		})
		.collect::<Result<Vec<_>, _>>()?;
	
	// Track seen row hashes and their first/last occurrence
	let mut seen_rows: HashMap<String, usize> = HashMap::new();
	let mut row_indices_to_keep: Vec<usize> = Vec::new();
	let mut current_row_index = 0;
	
	// Process each batch
	for batch in &batches {
		let num_rows = batch.num_rows();
		
		for row_idx in 0..num_rows {
			// Create a hash key from the deduplication columns
			let mut row_key = String::new();
			for &col_idx in &dedup_indices {
				let array = batch.column(col_idx);
				let value_str = format_array_value(array, row_idx);
				row_key.push_str(&value_str);
				row_key.push('|'); // Separator
			}
			
			match keep {
				"first" => {
					if !seen_rows.contains_key(&row_key) {
						seen_rows.insert(row_key, current_row_index);
						row_indices_to_keep.push(current_row_index);
					}
				},
				"last" => {
					if let Some(&existing_idx) = seen_rows.get(&row_key) {
						// Remove the previous occurrence
						if let Some(pos) = row_indices_to_keep.iter().position(|&x| x == existing_idx) {
							row_indices_to_keep.remove(pos);
						}
					}
					seen_rows.insert(row_key, current_row_index);
					row_indices_to_keep.push(current_row_index);
				},
				_ => return Err(NailError::InvalidArgument("keep must be 'first' or 'last'".to_string())),
			}
			
			current_row_index += 1;
		}
	}
	
	// Sort indices to maintain order
	row_indices_to_keep.sort_unstable();
	
	// Create new batches with only the selected rows
	let mut result_batches = Vec::new();
	let mut global_row_idx = 0;
	let mut keep_idx = 0;
	
	for batch in &batches {
		let num_rows = batch.num_rows();
		let mut rows_to_take = Vec::new();
		
		for local_row_idx in 0..num_rows {
			if keep_idx < row_indices_to_keep.len() && row_indices_to_keep[keep_idx] == global_row_idx {
				rows_to_take.push(local_row_idx);
				keep_idx += 1;
			}
			global_row_idx += 1;
		}
		
		if !rows_to_take.is_empty() {
			let filtered_batch = take_rows_from_batch(batch, &rows_to_take)?;
			result_batches.push(filtered_batch);
		}
	}
	
	// Convert back to DataFrame
	let ctx = crate::utils::create_context_with_jobs(_jobs).await?;
	let result_df = ctx.read_batches(result_batches)?;
	Ok(result_df)
}

async fn deduplicate_columns(df: &DataFrame) -> NailResult<DataFrame> {
	let schema = df.schema();
	let mut unique_columns = Vec::new();
	let mut seen_names = HashSet::new();
	
	for field in schema.fields() {
		let field_name = field.name();
		if !seen_names.contains(field_name) {
			seen_names.insert(field_name.clone());
			unique_columns.push(Expr::Column(datafusion::common::Column::new(None::<String>, field_name)));
		}
	}
	
	let result = df.clone().select(unique_columns)?;
	Ok(result)
}

fn format_array_value(array: &dyn Array, row_idx: usize) -> String {
	if array.is_null(row_idx) {
		return "NULL".to_string();
	}
	
	match array.data_type() {
		arrow_schema::DataType::Int64 => {
			let arr = array.as_any().downcast_ref::<Int64Array>().unwrap();
			arr.value(row_idx).to_string()
		},
		arrow_schema::DataType::Float64 => {
			let arr = array.as_any().downcast_ref::<Float64Array>().unwrap();
			arr.value(row_idx).to_string()
		},
		arrow_schema::DataType::Utf8 => {
			let arr = array.as_any().downcast_ref::<StringArray>().unwrap();
			arr.value(row_idx).to_string()
		},
		arrow_schema::DataType::Boolean => {
			let arr = array.as_any().downcast_ref::<BooleanArray>().unwrap();
			arr.value(row_idx).to_string()
		},
		_ => format!("UNSUPPORTED_TYPE_{}", row_idx),
	}
}

fn take_rows_from_batch(batch: &RecordBatch, row_indices: &[usize]) -> NailResult<RecordBatch> {
	let indices = UInt32Array::from(row_indices.iter().map(|&i| i as u32).collect::<Vec<_>>());
	let indices_ref = &indices as &dyn Array;
	
	let mut new_columns = Vec::new();
	for column in batch.columns() {
		let taken = arrow::compute::take(column.as_ref(), indices_ref, None)
			.map_err(|e| NailError::Arrow(e))?;
		new_columns.push(taken);
	}
	
	let result_batch = RecordBatch::try_new(batch.schema(), new_columns)
		.map_err(|e| NailError::Arrow(e))?;
	
	Ok(result_batch)
}


// File: src/error.rs
// Base: error

use thiserror::Error;

pub type NailResult<T> = Result<T, NailError>;

#[derive(Error, Debug)]
pub enum NailError {
	#[error("IO error: {0}")]
	Io(#[from] std::io::Error),
	
	#[error("DataFusion error: {0}")]
	DataFusion(#[from] datafusion::error::DataFusionError),
	
	#[error("Arrow error: {0}")]
	Arrow(#[from] arrow::error::ArrowError),
	
	#[error("Parquet error: {0}")]
	Parquet(#[from] parquet::errors::ParquetError),
	
	#[error("Regex error: {0}")]
	Regex(#[from] regex::Error),
	
	#[error("Serde JSON error: {0}")]
	SerdeJson(#[from] serde_json::Error),
	
	#[error("Invalid argument: {0}")]
	InvalidArgument(String),
	
	#[error("File not found: {0}")]
	FileNotFound(String),
	
	#[error("Unsupported format: {0}")]
	UnsupportedFormat(String),
	
	#[error("Column not found: {0}")]
	ColumnNotFound(String),
	
	#[error("Statistics error: {0}")]
	Statistics(String),
}


// File: tests/common/mod.rs
// Base: mod

use arrow::array::{Float64Array, Int64Array, StringArray, BooleanArray};
use arrow::record_batch::RecordBatch;
use arrow_schema::{DataType, Field, Schema};
use parquet::arrow::ArrowWriter;
use std::fs::{self, File};
use std::path::Path;
use std::sync::Arc;

pub fn create_sample_parquet(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
	let schema = Arc::new(Schema::new(vec![
		Field::new("id", DataType::Int64, false),
		Field::new("name", DataType::Utf8, true),
		Field::new("value", DataType::Float64, true),
		Field::new("category", DataType::Utf8, true),
	]));

	let id_array = Int64Array::from(vec![1, 2, 3, 4, 5]);
	let name_array = StringArray::from(vec![Some("Alice"), Some("Bob"), Some("Charlie"), None, Some("Eve")]);
	let value_array = Float64Array::from(vec![Some(100.0), None, Some(300.0), Some(400.0), Some(500.0)]);
	let category_array = StringArray::from(vec![Some("A"), Some("B"), Some("A"), Some("B"), Some("A")]);

	let batch = RecordBatch::try_new(schema.clone(), vec![
		Arc::new(id_array),
		Arc::new(name_array),
		Arc::new(value_array),
		Arc::new(category_array),
	])?;

	let file = File::create(path)?;
	let mut writer = ArrowWriter::try_new(file, schema, None)?;
	writer.write(&batch)?;
	writer.close()?;
	Ok(())
}

pub fn create_sample2_parquet(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
	let schema = Arc::new(Schema::new(vec![
		Field::new("id", DataType::Int64, false),
		Field::new("score", DataType::Float64, true),
	]));

	let id_array = Int64Array::from(vec![4, 5, 6, 7]);
	let score_array = Float64Array::from(vec![Some(88.0), Some(92.5), None, Some(88.0)]);

	let batch = RecordBatch::try_new(schema.clone(), vec![Arc::new(id_array), Arc::new(score_array)])?;

	let file = File::create(path)?;
	let mut writer = ArrowWriter::try_new(file, schema, None)?;
	writer.write(&batch)?;
	writer.close()?;
	Ok(())
}

pub fn create_sample3_parquet(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
	let schema = Arc::new(Schema::new(vec![
		Field::new("person_id", DataType::Int64, false),
		Field::new("status", DataType::Utf8, false),
	]));

	let id_array = Int64Array::from(vec![1, 3, 5]);
	let status_array = StringArray::from(vec!["active", "inactive", "active"]);

	let batch = RecordBatch::try_new(schema.clone(), vec![Arc::new(id_array), Arc::new(status_array)])?;

	let file = File::create(path)?;
	let mut writer = ArrowWriter::try_new(file, schema, None)?;
	writer.write(&batch)?;
	writer.close()?;
	Ok(())
}

pub fn create_sample_csv(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
	let content = "id,name,value\n10,Frank,1000.0\n11,Grace,1100.0\n";
	fs::write(path, content)?;
	Ok(())
}

pub fn create_sample_with_duplicates(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
	let schema = Arc::new(Schema::new(vec![
		Field::new("id", DataType::Int64, false),
		Field::new("name", DataType::Utf8, true),
		Field::new("value", DataType::Float64, true),
		Field::new("category", DataType::Utf8, true),
	]));

	let id_array = Int64Array::from(vec![1, 2, 2, 3, 3, 3, 4]);
	let name_array = StringArray::from(vec![
		Some("Alice"), 
		Some("Bob"), 
		Some("Bob"),
		Some("Charlie"), 
		Some("Charlie"),
		Some("Charlie"),
		Some("David")
	]);
	let value_array = Float64Array::from(vec![
		Some(100.0), 
		Some(200.0), 
		Some(200.0),
		Some(300.0), 
		Some(300.0),
		Some(300.0),
		Some(400.0)
	]);
	let category_array = StringArray::from(vec![
		Some("A"), 
		Some("B"), 
		Some("B"),
		Some("A"), 
		Some("A"),
		Some("A"),
		Some("B")
	]);

	let batch = RecordBatch::try_new(schema.clone(), vec![
		Arc::new(id_array),
		Arc::new(name_array),
		Arc::new(value_array),
		Arc::new(category_array),
	])?;

	let file = File::create(path)?;
	let mut writer = ArrowWriter::try_new(file, schema, None)?;
	writer.write(&batch)?;
	writer.close()?;
	Ok(())
}

pub fn create_empty_dataset(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
	let schema = Arc::new(Schema::new(vec![
		Field::new("id", DataType::Int64, false),
		Field::new("name", DataType::Utf8, true),
		Field::new("value", DataType::Float64, true),
	]));

	let empty_batch = RecordBatch::try_new(
		schema.clone(),
		vec![
			Arc::new(Int64Array::from(Vec::<i64>::new())),
			Arc::new(StringArray::from(Vec::<Option<String>>::new())),
			Arc::new(Float64Array::from(Vec::<Option<f64>>::new())),
		]
	)?;

	let file = File::create(path)?;
	let mut writer = ArrowWriter::try_new(file, schema, None)?;
	writer.write(&empty_batch)?;
	writer.close()?;
	Ok(())
}

pub fn create_single_row_dataset(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
	let schema = Arc::new(Schema::new(vec![
		Field::new("value", DataType::Float64, false),
		Field::new("category", DataType::Utf8, false),
	]));

	let batch = RecordBatch::try_new(
		schema.clone(),
		vec![
			Arc::new(Float64Array::from(vec![42.0])),
			Arc::new(StringArray::from(vec!["test"])),
		]
	)?;

	let file = File::create(path)?;
	let mut writer = ArrowWriter::try_new(file, schema, None)?;
	writer.write(&batch)?;
	writer.close()?;
	Ok(())
}

pub fn create_large_dataset(path: &Path, num_rows: usize) -> Result<(), Box<dyn std::error::Error>> {
	let schema = Arc::new(Schema::new(vec![
		Field::new("id", DataType::Int64, false),
		Field::new("value", DataType::Float64, true),
		Field::new("category", DataType::Utf8, true),
	]));

	let ids: Vec<i64> = (0..num_rows as i64).collect();
	let values: Vec<Option<f64>> = (0..num_rows).map(|i| {
		if i % 10 == 0 { None } else { Some(i as f64 * 1.5) }
	}).collect();
	let categories: Vec<Option<String>> = (0..num_rows).map(|i| {
		Some(format!("cat_{}", i % 5))
	}).collect();

	let batch = RecordBatch::try_new(
		schema.clone(),
		vec![
			Arc::new(Int64Array::from(ids)),
			Arc::new(Float64Array::from(values)),
			Arc::new(StringArray::from(categories)),
		]
	)?;

	let file = File::create(path)?;
	let mut writer = ArrowWriter::try_new(file, schema, None)?;
	writer.write(&batch)?;
	writer.close()?;
	Ok(())
}

pub fn create_mixed_types_dataset(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
	let schema = Arc::new(Schema::new(vec![
		Field::new("int_col", DataType::Int64, false),
		Field::new("float_col", DataType::Float64, true),
		Field::new("string_col", DataType::Utf8, true),
		Field::new("bool_col", DataType::Boolean, true),
	]));

	let batch = RecordBatch::try_new(
		schema.clone(),
		vec![
			Arc::new(Int64Array::from(vec![1, 2, 3, 4, 5])),
			Arc::new(Float64Array::from(vec![Some(1.1), None, Some(3.3), Some(4.4), Some(5.5)])),
			Arc::new(StringArray::from(vec![Some("a"), Some("b"), None, Some("d"), Some("e")])),
			Arc::new(BooleanArray::from(vec![Some(true), Some(false), Some(true), None, Some(false)])),
		]
	)?;

	let file = File::create(path)?;
	let mut writer = ArrowWriter::try_new(file, schema, None)?;
	writer.write(&batch)?;
	writer.close()?;
	Ok(())
}

pub fn create_statistical_test_data(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
	let schema = Arc::new(Schema::new(vec![
		Field::new("id", DataType::Int64, false),
		Field::new("normal_dist", DataType::Float64, false),
		Field::new("uniform_dist", DataType::Float64, false),
		Field::new("integer_seq", DataType::Int64, false),
	]));

	let ids = (1..=100).collect::<Vec<i64>>();
	let normal_data: Vec<f64> = vec![
		1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0,
		2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0,
		3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0,
		4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0,
		5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0,
		6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0,
		7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0,
		8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0,
		9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0,
		10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0,
	];
	
	let uniform_data: Vec<f64> = (1..=100).map(|i| i as f64).collect();
	let integer_seq: Vec<i64> = (1..=100).collect();

	let batch = RecordBatch::try_new(schema.clone(), vec![
		Arc::new(Int64Array::from(ids)),
		Arc::new(Float64Array::from(normal_data)),
		Arc::new(Float64Array::from(uniform_data)),
		Arc::new(Int64Array::from(integer_seq)),
	])?;

	let file = File::create(path)?;
	let mut writer = ArrowWriter::try_new(file, schema, None)?;
	writer.write(&batch)?;
	writer.close()?;
	Ok(())
}

pub fn create_time_series_data(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
	let schema = Arc::new(Schema::new(vec![
		Field::new("timestamp", DataType::Int64, false),
		Field::new("value", DataType::Float64, true),
		Field::new("category", DataType::Utf8, false),
	]));

	let timestamps: Vec<i64> = (0..50).map(|i| 1640995200 + i * 3600).collect();
	let values: Vec<Option<f64>> = (0..50).map(|i| {
		if i % 7 == 0 { None } else { Some((i as f64).sin() * 100.0 + 100.0) }
	}).collect();
	let categories: Vec<&str> = (0..50).map(|i| {
		match i % 3 {
			0 => "A",
			1 => "B",
			_ => "C",
		}
	}).collect();

	let batch = RecordBatch::try_new(schema.clone(), vec![
		Arc::new(Int64Array::from(timestamps)),
		Arc::new(Float64Array::from(values)),
		Arc::new(StringArray::from(categories)),
	])?;

	let file = File::create(path)?;
	let mut writer = ArrowWriter::try_new(file, schema, None)?;
	writer.write(&batch)?;
	writer.close()?;
	Ok(())
}


// File: src/utils/format.rs
// Base: format

use datafusion::prelude::*;
use datafusion::arrow::array::*;
use datafusion::arrow::datatypes::DataType;
use std::path::Path;
use crate::error::NailResult;
use crate::cli::OutputFormat;
use crate::utils::io::write_data;
use crate::utils::FileFormat;

// ANSI color codes
const RESET: &str = "\x1b[0m";
const BOLD: &str = "\x1b[1m";
const DIM: &str = "\x1b[2m";
const HEADER_COLOR: &str = "\x1b[1;36m"; // Bold cyan
const NUMERIC_COLOR: &str = "\x1b[33m";  // Yellow
const STRING_COLOR: &str = "\x1b[32m";   // Green
const NULL_COLOR: &str = "\x1b[2;37m";   // Dim white
const BORDER_COLOR: &str = "\x1b[2;90m"; // Dim gray

pub async fn display_dataframe(
	df: &DataFrame,
	output_path: Option<&Path>,
	format: Option<&OutputFormat>,
) -> NailResult<()> {
	match output_path {
		Some(path) => {
			let file_format = match format {
				Some(OutputFormat::Json) => Some(FileFormat::Json),
				Some(OutputFormat::Csv) => Some(FileFormat::Csv),
				Some(OutputFormat::Parquet) => Some(FileFormat::Parquet),
				Some(OutputFormat::Text) | None => {
					match path.extension().and_then(|s| s.to_str()) {
						Some("json") => Some(FileFormat::Json),
						Some("csv") => Some(FileFormat::Csv),
						Some("parquet") => Some(FileFormat::Parquet),
						_ => Some(FileFormat::Parquet),
					}
				},
			};
			
			write_data(df, path, file_format.as_ref()).await
		},
		None => {
			match format {
				Some(OutputFormat::Json) => {
					display_as_json(df).await?;
				},
				Some(OutputFormat::Text) | None => {
					display_as_table(df).await?;
				},
				_ => {
					return Err(crate::error::NailError::InvalidArgument(
						"CSV and Parquet formats require an output file".to_string()
					));
				},
			}
			
			Ok(())
		},
	}
}

async fn display_as_json(df: &DataFrame) -> NailResult<()> {
	let batches = df.clone().collect().await?;
	let schema = df.schema();
	
	println!("[");
	let mut first_record = true;
	
	for batch in &batches {
		for row_idx in 0..batch.num_rows() {
			if !first_record {
				println!(",");
			}
			first_record = false;
			
			print!("  {{");
			let mut first_field = true;
			
			for (col_idx, field) in schema.fields().iter().enumerate() {
				if !first_field {
					print!(", ");
				}
				first_field = false;
				
				let column = batch.column(col_idx);
				let value = format_json_value(column, row_idx, field.data_type());
				print!("\"{}\": {}", field.name(), value);
			}
			print!("}}");
		}
	}
	
	println!("\n]");
	Ok(())
}

async fn display_as_table(df: &DataFrame) -> NailResult<()> {
	let batches = df.clone().collect().await?;
	let schema = df.schema();
	
	if batches.is_empty() {
		println!("{}No data to display{}", DIM, RESET);
		return Ok(());
	}
	
	// Print data in card format
	let mut row_count = 0;
	for batch in &batches {
		for row_idx in 0..batch.num_rows() {
			row_count += 1;
			
			// Card header
			println!("{}┌─ Record {} ─{}", BORDER_COLOR, row_count, "─".repeat(60));
			println!("│{}", RESET);
			
			// Print each field as a key-value pair
			for (col_idx, field) in schema.fields().iter().enumerate() {
				let column = batch.column(col_idx);
				let value = format_cell_value(column, row_idx, field.data_type(), true);
				
				// Format field name
				let field_name = format!("{}{:<20}{}", HEADER_COLOR, field.name(), RESET);
				
				// Handle long values by wrapping them
				let wrapped_value = wrap_text(&value, 80, 22);
				let lines: Vec<&str> = wrapped_value.lines().collect();
				
				if lines.len() == 1 {
					println!("{}│{} {} : {}", BORDER_COLOR, RESET, field_name, lines[0]);
				} else {
					println!("{}│{} {} : {}", BORDER_COLOR, RESET, field_name, lines[0]);
					for line in &lines[1..] {
						println!("{}│{} {:<20} : {}", BORDER_COLOR, RESET, "", line);
					}
				}
			}
			
			// Card footer
			println!("{}│{}", BORDER_COLOR, RESET);
			println!("{}└{}{}", BORDER_COLOR, "─".repeat(70), RESET);
			
			// Add spacing between records
			if row_count < batches.iter().map(|b| b.num_rows()).sum::<usize>() {
				println!();
			}
		}
	}
	
	// Print summary
	println!("{}Total records: {}{}{}", DIM, BOLD, row_count, RESET);
	
	Ok(())
}

fn wrap_text(text: &str, max_width: usize, _indent: usize) -> String {
	let clean_text = strip_ansi_codes(text);
	
	// First, normalize the text by replacing all newlines with spaces
	// This handles cases where the original data has embedded newlines
	let normalized_text = clean_text.replace('\n', " ").replace('\r', " ");
	
	// Remove multiple consecutive spaces
	let normalized_text = normalized_text.split_whitespace().collect::<Vec<_>>().join(" ");
	
	if normalized_text.len() <= max_width {
		// If the original text had colors, preserve them for short text
		if text.contains('\x1b') {
			return text.replace('\n', " ").replace('\r', " ");
		} else {
			return normalized_text;
		}
	}
	
	let mut result = Vec::new();
	let mut current_line = String::new();
	
	for word in normalized_text.split_whitespace() {
		if current_line.is_empty() {
			current_line = word.to_string();
		} else if current_line.len() + word.len() + 1 <= max_width {
			current_line.push(' ');
			current_line.push_str(word);
		} else {
			result.push(current_line);
			current_line = word.to_string();
		}
	}
	
	if !current_line.is_empty() {
		result.push(current_line);
	}
	
	// Reconstruct with original formatting for first line, plain for wrapped lines
	if result.len() == 1 {
		// For single line, preserve original colors but normalize whitespace
		if text.contains('\x1b') {
			text.replace('\n', " ").replace('\r', " ")
		} else {
			result[0].clone()
		}
	} else {
		let mut output = Vec::new();
		output.push(result[0].clone());
		
		for line in &result[1..] {
			// For wrapped lines, apply the same color as the original if it had color
			if text.contains('\x1b') {
				// Extract color from original text
				let color = extract_color_code(text);
				output.push(format!("{}{}{}", color, line, RESET));
			} else {
				output.push(line.clone());
			}
		}
		
		output.join("\n")
	}
}

fn strip_ansi_codes(text: &str) -> String {
	let mut result = String::new();
	let mut in_escape = false;
	
	for ch in text.chars() {
		if ch == '\x1b' {
			in_escape = true;
		} else if in_escape && ch == 'm' {
			in_escape = false;
		} else if !in_escape {
			result.push(ch);
		}
	}
	
	result
}

fn extract_color_code(text: &str) -> &str {
	if text.contains(NUMERIC_COLOR) {
		NUMERIC_COLOR
	} else if text.contains(STRING_COLOR) {
		STRING_COLOR
	} else if text.contains(NULL_COLOR) {
		NULL_COLOR
	} else {
		""
	}
}

fn extract_numeric_from_debug(debug_str: &str) -> String {
	// Try to extract numeric value from debug representation like "PrimitiveArray<Float64>\n[\n  4.5,\n]"
	if let Some(start) = debug_str.find('[') {
		if let Some(end) = debug_str.find(']') {
			let content = &debug_str[start+1..end];
			// Look for numeric values in the content
			for line in content.lines() {
				let trimmed = line.trim().trim_end_matches(',');
				if let Ok(val) = trimmed.parse::<f64>() {
					return format!("{:.2}", val);
				}
				if let Ok(val) = trimmed.parse::<i64>() {
					return val.to_string();
				}
			}
		}
	}
	
	// Fallback to original approach
	debug_str
		.lines()
		.next()
		.unwrap_or("unknown")
		.trim_start_matches('[')
		.trim_end_matches(']')
		.trim()
		.to_string()
}

fn format_cell_value(column: &dyn Array, row_idx: usize, data_type: &DataType, with_color: bool) -> String {
	if column.is_null(row_idx) {
		let value = "NULL";
		if with_color {
			format!("{}{}{}", NULL_COLOR, value, RESET)
		} else {
			value.to_string()
		}
	} else {
		let value = match data_type {
			DataType::Utf8 => {
				let array = column.as_any().downcast_ref::<StringArray>().unwrap();
				let val = array.value(row_idx);
				if with_color {
					format!("{}{}{}", STRING_COLOR, val, RESET)
				} else {
					val.to_string()
				}
			},
			DataType::Int64 => {
				if let Some(array) = column.as_any().downcast_ref::<Int64Array>() {
					let val = array.value(row_idx).to_string();
					if with_color {
						format!("{}{}{}", NUMERIC_COLOR, val, RESET)
					} else {
						val
					}
				} else {
					// Fallback for when the actual type doesn't match the schema type
					let debug_str = format!("{:?}", column.slice(row_idx, 1));
					let val = extract_numeric_from_debug(&debug_str);
					if with_color {
						format!("{}{}{}", NUMERIC_COLOR, val, RESET)
					} else {
						val
					}
				}
			},
			DataType::Float64 => {
				if let Some(array) = column.as_any().downcast_ref::<Float64Array>() {
					let val = format!("{:.2}", array.value(row_idx));
					if with_color {
						format!("{}{}{}", NUMERIC_COLOR, val, RESET)
					} else {
						val
					}
				} else {
					// Fallback for when the actual type doesn't match the schema type
					let debug_str = format!("{:?}", column.slice(row_idx, 1));
					let val = extract_numeric_from_debug(&debug_str);
					if with_color {
						format!("{}{}{}", NUMERIC_COLOR, val, RESET)
					} else {
						val
					}
				}
			},
			DataType::Int32 => {
				if let Some(array) = column.as_any().downcast_ref::<Int32Array>() {
					let val = array.value(row_idx).to_string();
					if with_color {
						format!("{}{}{}", NUMERIC_COLOR, val, RESET)
					} else {
						val
					}
				} else {
					// Fallback for when the actual type doesn't match the schema type
					let debug_str = format!("{:?}", column.slice(row_idx, 1));
					let val = extract_numeric_from_debug(&debug_str);
					if with_color {
						format!("{}{}{}", NUMERIC_COLOR, val, RESET)
					} else {
						val
					}
				}
			},
			DataType::Float32 => {
				if let Some(array) = column.as_any().downcast_ref::<Float32Array>() {
					let val = format!("{:.2}", array.value(row_idx));
					if with_color {
						format!("{}{}{}", NUMERIC_COLOR, val, RESET)
					} else {
						val
					}
				} else {
					// Fallback for when the actual type doesn't match the schema type
					let debug_str = format!("{:?}", column.slice(row_idx, 1));
					let val = extract_numeric_from_debug(&debug_str);
					if with_color {
						format!("{}{}{}", NUMERIC_COLOR, val, RESET)
					} else {
						val
					}
				}
			},
			DataType::Boolean => {
				let array = column.as_any().downcast_ref::<BooleanArray>().unwrap();
				let val = array.value(row_idx).to_string();
				if with_color {
					format!("{}{}{}", NUMERIC_COLOR, val, RESET)
				} else {
					val
				}
			},
			DataType::Date32 => {
				let array = column.as_any().downcast_ref::<Date32Array>().unwrap();
				let days_since_epoch = array.value(row_idx);
				// Convert days since epoch to a readable date
				let date = chrono::NaiveDate::from_num_days_from_ce_opt(days_since_epoch + 719163)
					.unwrap_or_else(|| chrono::NaiveDate::from_ymd_opt(1970, 1, 1)
						.unwrap_or_else(|| chrono::NaiveDate::default()));
				let val = date.format("%Y-%m-%d").to_string();
				if with_color {
					format!("{}{}{}", STRING_COLOR, val, RESET)
				} else {
					val
				}
			},
			DataType::Date64 => {
				let array = column.as_any().downcast_ref::<Date64Array>().unwrap();
				let millis_since_epoch = array.value(row_idx);
				let datetime = chrono::DateTime::from_timestamp_millis(millis_since_epoch)
					.unwrap_or_else(|| {
						chrono::DateTime::from_timestamp(0, 0)
							.unwrap_or_else(|| chrono::DateTime::UNIX_EPOCH)
					});
				let val = datetime.format("%Y-%m-%d").to_string();
				if with_color {
					format!("{}{}{}", STRING_COLOR, val, RESET)
				} else {
					val
				}
			},
			DataType::Timestamp(_, _) => {
				// Handle timestamp types
				let val = "timestamp"; // Simplified for now
				if with_color {
					format!("{}{}{}", STRING_COLOR, val, RESET)
				} else {
					val.to_string()
				}
			},
			_ => {
				// Fallback for other types - try to get a string representation
				let val = format!("{:?}", column.slice(row_idx, 1))
					.lines()
					.next()
					.unwrap_or("unknown")
					.trim_start_matches('[')
					.trim_end_matches(']')
					.trim_start_matches("\"")
					.trim_end_matches("\"")
					.trim()
					.to_string();
				if with_color {
					format!("{}{}{}", STRING_COLOR, val, RESET)
				} else {
					val
				}
			},
		};
		value
	}
}

fn format_json_value(column: &dyn Array, row_idx: usize, data_type: &DataType) -> String {
	if column.is_null(row_idx) {
		"null".to_string()
	} else {
		match data_type {
			DataType::Utf8 => {
				if let Some(array) = column.as_any().downcast_ref::<StringArray>() {
					format!("\"{}\"", array.value(row_idx).replace("\"", "\\\""))
				} else {
					"\"unknown\"".to_string()
				}
			},
			DataType::Int64 => {
				if let Some(array) = column.as_any().downcast_ref::<Int64Array>() {
					array.value(row_idx).to_string()
				} else {
					"0".to_string()
				}
			},
			DataType::Float64 => {
				if let Some(array) = column.as_any().downcast_ref::<Float64Array>() {
					let val = array.value(row_idx);
					if val.is_finite() {
						val.to_string()
					} else {
						"null".to_string()
					}
				} else {
					"0.0".to_string()
				}
			},
			DataType::Int32 => {
				if let Some(array) = column.as_any().downcast_ref::<Int32Array>() {
					array.value(row_idx).to_string()
				} else {
					"0".to_string()
				}
			},
			DataType::Float32 => {
				if let Some(array) = column.as_any().downcast_ref::<Float32Array>() {
					let val = array.value(row_idx);
					if val.is_finite() {
						val.to_string()
					} else {
						"null".to_string()
					}
				} else {
					"0.0".to_string()
				}
			},
			DataType::Boolean => {
				if let Some(array) = column.as_any().downcast_ref::<BooleanArray>() {
					array.value(row_idx).to_string()
				} else {
					"false".to_string()
				}
			},
			DataType::Date32 => {
				if let Some(array) = column.as_any().downcast_ref::<Date32Array>() {
					let days_since_epoch = array.value(row_idx);
					let date = chrono::NaiveDate::from_num_days_from_ce_opt(days_since_epoch + 719163)
						.unwrap_or_else(|| chrono::NaiveDate::from_ymd_opt(1970, 1, 1)
							.unwrap_or_else(|| chrono::NaiveDate::default()));
					format!("\"{}\"", date.format("%Y-%m-%d"))
				} else {
					"\"1970-01-01\"".to_string()
				}
			},
			DataType::Date64 => {
				if let Some(array) = column.as_any().downcast_ref::<Date64Array>() {
					let millis_since_epoch = array.value(row_idx);
					let datetime = chrono::DateTime::from_timestamp_millis(millis_since_epoch)
						.unwrap_or_else(|| {
							chrono::DateTime::from_timestamp(0, 0)
								.unwrap_or_else(|| chrono::DateTime::UNIX_EPOCH)
						});
					format!("\"{}\"", datetime.format("%Y-%m-%d"))
				} else {
					"\"1970-01-01\"".to_string()
				}
			},
			DataType::Timestamp(_, _) => {
				"\"timestamp\"".to_string()
			},
			_ => {
				// Safe fallback for any other type
				let debug_str = format!("{:?}", column.slice(row_idx, 1));
				let val = debug_str
					.lines()
					.next()
					.unwrap_or("unknown")
					.trim_start_matches('[')
					.trim_end_matches(']')
					.trim_start_matches("\"")
					.trim_end_matches("\"")
					.trim()
					.to_string();
				format!("\"{}\"", val.replace("\"", "\\\""))
			},
		}
	}
}




// File: src/utils/stats.rs
// Base: stats

use datafusion::prelude::*;
use datafusion::common::DFSchemaRef;
use regex::Regex;
use crate::error::{NailError, NailResult};

#[derive(clap::ValueEnum, Clone, Debug, PartialEq)]
pub enum CorrelationType {
	Pearson,
	Kendall,
	Spearman,
}

pub fn select_columns_by_pattern(schema: DFSchemaRef, pattern: &str) -> NailResult<Vec<String>> {
	let patterns: Vec<&str> = pattern.split(',').map(|s| s.trim()).collect();
	let mut selected = Vec::new();
	let mut not_found = Vec::new();
	
	for pattern in &patterns {
		let mut found = false;
		
		for field in schema.fields() {
			let field_name = field.name();
			
			if pattern.contains('*') || pattern.contains('^') || pattern.contains('$') {
				let regex = Regex::new(pattern)?;
				if regex.is_match(field_name) {
					selected.push(field_name.clone());
					found = true;
				}
			} else if field_name == *pattern {
				selected.push(field_name.clone());
				found = true;
				break;
			}
		}
		
		if !found {
			for field in schema.fields() {
				let field_name = field.name();
				
				if pattern.contains('*') || pattern.contains('^') || pattern.contains('$') {
					let case_insensitive_pattern = format!("(?i){}", pattern);
					if let Ok(regex) = Regex::new(&case_insensitive_pattern) {
						if regex.is_match(field_name) {
							selected.push(field_name.clone());
							found = true;
						}
					}
				} else if field_name.to_lowercase() == pattern.to_lowercase() {
					selected.push(field_name.clone());
					found = true;
					break;
				}
			}
		}
		
		if !found {
			not_found.push(*pattern);
		}
	}
	
	if !not_found.is_empty() {
		let available_columns: Vec<String> = schema.fields().iter()
			.map(|f| f.name().clone())
			.collect();
		return Err(NailError::ColumnNotFound(format!(
			"Columns not found: {:?}. Available columns: {:?}", 
			not_found, available_columns
		)));
	}
	
	let mut unique_selected = Vec::new();
	for col in selected {
		if !unique_selected.contains(&col) {
			unique_selected.push(col);
		}
	}
	
	if unique_selected.is_empty() {
		return Err(NailError::ColumnNotFound(format!("No columns matched pattern: {}", pattern)));
	}
	
	Ok(unique_selected)
}

pub async fn calculate_basic_stats(df: &DataFrame, columns: &[String]) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let mut stats_rows = Vec::new();
	
	for column in columns {
		let field = df.schema().field_with_name(None, column)
			.map_err(|_| NailError::ColumnNotFound(column.clone()))?;
		
		match field.data_type() {
			datafusion::arrow::datatypes::DataType::Int64 | 
			datafusion::arrow::datatypes::DataType::Float64 | 
			datafusion::arrow::datatypes::DataType::Int32 | 
			datafusion::arrow::datatypes::DataType::Float32 => {
				let stats_sql = format!(
					"SELECT 
						'{}' as column_name,
						COUNT(\"{}\") as count,
						AVG(\"{}\") as mean,
						APPROX_PERCENTILE_CONT(\"{}\", 0.25) as q25,
						APPROX_PERCENTILE_CONT(\"{}\", 0.5) as q50,
						APPROX_PERCENTILE_CONT(\"{}\", 0.75) as q75,
						COUNT(DISTINCT \"{}\") as num_classes
					FROM {}",
					column, column, column, column, column, column, column, table_name
				);
				
				let stats_df = ctx.sql(&stats_sql).await?;
				stats_rows.push(stats_df);
			},
			datafusion::arrow::datatypes::DataType::Utf8 => {
				let stats_sql = format!(
					"SELECT 
						'{}' as column_name,
						COUNT(\"{}\") as count,
						NULL as mean,
						NULL as q25,
						NULL as q50,
						NULL as q75,
						COUNT(DISTINCT \"{}\") as num_classes
					FROM {}",
					column, column, column, table_name
				);
				
				let stats_df = ctx.sql(&stats_sql).await?;
				stats_rows.push(stats_df);
			},
			_ => continue,
		}
	}
	
	if stats_rows.is_empty() {
		return Err(NailError::Statistics("No suitable columns for statistics".to_string()));
	}
	
	let mut iter = stats_rows.into_iter();
	let mut combined = iter.next().unwrap();
	for df in iter {
		combined = combined.union(df)?;
	}
	
	Ok(combined)
}

pub async fn calculate_exhaustive_stats(df: &DataFrame, columns: &[String]) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let mut stats_rows = Vec::new();
	
	for column in columns {
		let field = df.schema().field_with_name(None, column)
			.map_err(|_| NailError::ColumnNotFound(column.clone()))?;
		
		match field.data_type() {
			datafusion::arrow::datatypes::DataType::Int64 | 
			datafusion::arrow::datatypes::DataType::Float64 | 
			datafusion::arrow::datatypes::DataType::Int32 | 
			datafusion::arrow::datatypes::DataType::Float32 => {
				let stats_sql = format!(
					"SELECT 
						'{}' as column_name,
						COUNT(\"{}\") as count,
						AVG(\"{}\") as mean,
						STDDEV(\"{}\") as std_dev,
						MIN(\"{}\") as min_val,
						APPROX_PERCENTILE_CONT(\"{}\", 0.25) as q25,
						APPROX_PERCENTILE_CONT(\"{}\", 0.5) as median,
						APPROX_PERCENTILE_CONT(\"{}\", 0.75) as q75,
						MAX(\"{}\") as max_val,
						VAR_POP(\"{}\") as variance,
						COUNT(DISTINCT \"{}\") as num_classes,
						(COUNT(\"{}\") - COUNT(DISTINCT \"{}\")) as duplicates
					FROM {}",
					column, column, column, column, column, column, column, column, 
					column, column, column, column, column, table_name
				);
				
				let stats_df = ctx.sql(&stats_sql).await?;
				stats_rows.push(stats_df);
			},
			datafusion::arrow::datatypes::DataType::Utf8 => {
				let stats_sql = format!(
					"SELECT 
						'{}' as column_name,
						COUNT(\"{}\") as count,
						NULL as mean,
						NULL as std_dev,
						NULL as min_val,
						NULL as q25,
						NULL as median,
						NULL as q75,
						NULL as max_val,
						NULL as variance,
						COUNT(DISTINCT \"{}\") as num_classes,
						(COUNT(\"{}\") - COUNT(DISTINCT \"{}\")) as duplicates
					FROM {}",
					column, column, column, column, column, table_name
				);
				
				let stats_df = ctx.sql(&stats_sql).await?;
				stats_rows.push(stats_df);
			},
			_ => continue,
		}
	}
	
	if stats_rows.is_empty() {
		return Err(NailError::Statistics("No suitable columns for statistics".to_string()));
	}
	
	let mut iter = stats_rows.into_iter();
	let mut combined = iter.next().unwrap();
	for df in iter {
		combined = combined.union(df)?;
	}
	
	Ok(combined)
}

pub async fn calculate_hypothesis_tests(_df: &DataFrame, _columns: &[String]) -> NailResult<DataFrame> {
	Err(NailError::Statistics("Hypothesis tests not yet implemented".to_string()))
}

pub async fn calculate_correlations(
	df: &DataFrame,
	columns: &[String],
	correlation_type: &CorrelationType,
	matrix_format: bool,
	_include_tests: bool,
	digits: usize,
) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	if matrix_format {
		calculate_correlation_matrix(ctx, table_name, columns, correlation_type, digits).await
	} else {
		calculate_correlation_pairs(ctx, table_name, columns, correlation_type, digits).await
	}
}

async fn calculate_correlation_matrix(
	ctx: SessionContext,
	table_name: &str,
	columns: &[String],
	correlation_type: &CorrelationType,
	digits: usize,
) -> NailResult<DataFrame> {
	let mut correlation_queries = Vec::new();
	
	for col1 in columns {
		let mut row_values = Vec::new();
		row_values.push(format!("'{}' as variable", col1));
		
		for col2 in columns {
			if col1 == col2 {
				row_values.push(format!("1.0 as corr_with_{}", col2.replace(".", "_")));
			} else {
				let corr_expr = match correlation_type {
					CorrelationType::Pearson => {
						format!("ROUND((SELECT CORR(\"{}\", \"{}\") FROM {}), {})", col1, col2, table_name, digits)
					},
					CorrelationType::Spearman => {
						format!("ROUND((SELECT CORR(\"{}\", \"{}\") FROM {}), {})", col1, col2, table_name, digits)
					},
					CorrelationType::Kendall => {
						format!("ROUND((SELECT CORR(\"{}\", \"{}\") * 0.816 FROM {}), {})", col1, col2, table_name, digits)
					}
				};
				row_values.push(format!("{} as corr_with_{}", corr_expr, col2.replace(".", "_")));
			}
		}
		
		let row_sql = format!("SELECT {}", row_values.join(", "));
		correlation_queries.push(row_sql);
	}
	
	if correlation_queries.is_empty() {
		return Err(NailError::Statistics("No columns for correlation".to_string()));
	}
	
	let mut combined = ctx.sql(&correlation_queries[0]).await?;
	for query in correlation_queries.into_iter().skip(1) {
		let df = ctx.sql(&query).await?;
		combined = combined.union(df)?;
	}
	
	Ok(combined)
}

async fn calculate_correlation_pairs(
	ctx: SessionContext,
	table_name: &str,
	columns: &[String],
	correlation_type: &CorrelationType,
	digits: usize,
) -> NailResult<DataFrame> {
	let mut pair_queries = Vec::new();
	
	for (i, col1) in columns.iter().enumerate() {
		for col2 in columns.iter().skip(i + 1) {
			let pair_sql = match correlation_type {
				CorrelationType::Pearson => {
					format!(
						"SELECT '{}' as column1, '{}' as column2, ROUND(CORR(\"{}\", \"{}\"), {}) as correlation FROM {}",
						col1, col2, col1, col2, digits, table_name
					)
				},
				CorrelationType::Spearman => {
					format!(
						"WITH ranked_data AS (
							SELECT 
								ROW_NUMBER() OVER (ORDER BY \"{}\") as rank1,
								ROW_NUMBER() OVER (ORDER BY \"{}\") as rank2
							FROM {}
						) 
						SELECT '{}' as column1, '{}' as column2, ROUND(CORR(rank1, rank2), {}) as correlation FROM ranked_data",
						col1, col2, table_name, col1, col2, digits
					)
				},
				CorrelationType::Kendall => {
					format!(
						"WITH ranked_data AS (
							SELECT 
								ROW_NUMBER() OVER (ORDER BY \"{}\") as rank1,
								ROW_NUMBER() OVER (ORDER BY \"{}\") as rank2
							FROM {}
						) 
						SELECT '{}' as column1, '{}' as column2, ROUND(CORR(rank1, rank2) * 0.816, {}) as correlation FROM ranked_data",
						col1, col2, table_name, col1, col2, digits
					)
				}
			};
			pair_queries.push(pair_sql);
		}
	}
	
	if pair_queries.is_empty() {
		return Err(NailError::Statistics("Need at least 2 columns for correlation".to_string()));
	}
	
	let mut combined = ctx.sql(&pair_queries[0]).await?;
	for query in pair_queries.into_iter().skip(1) {
		let df = ctx.sql(&query).await?;
		combined = combined.union(df)?;
	}
	
	Ok(combined)
}


// File: src/commands/filter.rs
// Base: filter

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct FilterArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Column filter conditions (e.g., 'age>25,salary<50000')")]
	pub columns: Option<String>,
	
	#[arg(short, long, help = "Row filter type", value_enum)]
	pub rows: Option<RowFilter>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

#[derive(clap::ValueEnum, Clone, Debug)]
pub enum RowFilter {
	NoNan,
	NumericOnly,
	CharOnly,
	NoZeros,
}

pub async fn execute(args: FilterArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let mut result_df = df;
	
	if let Some(col_conditions) = &args.columns {
		if args.verbose {
			eprintln!("Applying column filters: {}", col_conditions);
		}
		result_df = apply_column_filters(&result_df, col_conditions, args.jobs).await?;
	}
	
	if let Some(row_filter) = &args.rows {
		if args.verbose {
			eprintln!("Applying row filter: {:?}", row_filter);
		}
		result_df = apply_row_filter(&result_df, row_filter, args.jobs).await?;
	}
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&result_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&result_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

async fn apply_column_filters(df: &DataFrame, conditions: &str, jobs: Option<usize>) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let schema = df.schema().clone().into();
	let mut filter_conditions = Vec::new();
	
	for condition in conditions.split(',') {
		let condition = condition.trim();
		let filter_expr = parse_condition_with_schema(condition, &schema).await?;
		filter_conditions.push(filter_expr);
	}
	
	let combined_filter = filter_conditions.into_iter()
		.reduce(|acc, expr| acc.and(expr))
		.unwrap();
	
	let result = ctx.table(table_name).await?.filter(combined_filter)?;
	Ok(result)
}

async fn parse_condition_with_schema(condition: &str, schema: &datafusion::common::DFSchemaRef) -> NailResult<Expr> {
	let operators = [">=", "<=", "!=", "=", ">", "<"];
	
	for op in &operators {
		if let Some(pos) = condition.find(op) {
			let column_name_input = condition[..pos].trim();
			let value_str = condition[pos + op.len()..].trim();
			
			// Find the actual column name (case-insensitive matching)
			let actual_column_name = schema.fields().iter()
				.find(|f| f.name().to_lowercase() == column_name_input.to_lowercase())
				.map(|f| f.name().clone())
				.ok_or_else(|| {
					let available_cols: Vec<String> = schema.fields().iter()
						.map(|f| f.name().clone())
						.collect();
					NailError::ColumnNotFound(format!(
						"Column '{}' not found. Available columns: {:?}", 
						column_name_input, available_cols
					))
				})?;
			
			let value_expr = if let Ok(int_val) = value_str.parse::<i64>() {
				lit(int_val)
			} else if let Ok(float_val) = value_str.parse::<f64>() {
				lit(float_val)
			} else {
				lit(value_str)
			};
			
			// Use quoted column name to preserve case sensitivity
			let column_expr = Expr::Column(datafusion::common::Column::new(None::<String>, &actual_column_name));
			
			return Ok(match *op {
				"=" => column_expr.eq(value_expr),
				"!=" => column_expr.not_eq(value_expr),
				">" => column_expr.gt(value_expr),
				">=" => column_expr.gt_eq(value_expr),
				"<" => column_expr.lt(value_expr),
				"<=" => column_expr.lt_eq(value_expr),
				_ => unreachable!(),
			});
		}
	}
	
	Err(NailError::InvalidArgument(format!("Invalid condition: {}", condition)))
}

async fn apply_row_filter(df: &DataFrame, filter: &RowFilter, jobs: Option<usize>) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let schema = df.schema();
	let filter_expr = match filter {
		RowFilter::NoNan => {
			let conditions: Vec<Expr> = schema.fields().iter()
				.map(|f| Expr::Column(datafusion::common::Column::new(None::<String>, f.name())).is_not_null())
				.collect();
			conditions.into_iter().reduce(|acc, expr| acc.and(expr)).unwrap()
		},
		RowFilter::NumericOnly => {
			let numeric_columns: Vec<String> = schema.fields().iter()
				.filter(|f| matches!(f.data_type(), 
					datafusion::arrow::datatypes::DataType::Int64 | 
					datafusion::arrow::datatypes::DataType::Float64 | 
					datafusion::arrow::datatypes::DataType::Int32 | 
					datafusion::arrow::datatypes::DataType::Float32
				))
				.map(|f| f.name().clone())
				.collect();
			
			if numeric_columns.is_empty() {
				return Err(NailError::InvalidArgument("No numeric columns found".to_string()));
			}
			
			return Ok(df.clone().select(numeric_columns.iter().map(|name| Expr::Column(datafusion::common::Column::new(None::<String>, name))).collect())?);
		},
		RowFilter::CharOnly => {
			let char_columns: Vec<String> = schema.fields().iter()
				.filter(|f| matches!(f.data_type(), datafusion::arrow::datatypes::DataType::Utf8))
				.map(|f| f.name().clone())
				.collect();
			
			if char_columns.is_empty() {
				return Err(NailError::InvalidArgument("No string columns found".to_string()));
			}
			
			return Ok(df.clone().select(char_columns.iter().map(|name| Expr::Column(datafusion::common::Column::new(None::<String>, name))).collect())?);
		},
		RowFilter::NoZeros => {
			let conditions: Vec<Expr> = schema.fields().iter()
				.filter_map(|f| {
					match f.data_type() {
						datafusion::arrow::datatypes::DataType::Int64 | 
						datafusion::arrow::datatypes::DataType::Int32 => {
							Some(Expr::Column(datafusion::common::Column::new(None::<String>, f.name())).not_eq(lit(0)))
						},
						datafusion::arrow::datatypes::DataType::Float64 | 
						datafusion::arrow::datatypes::DataType::Float32 => {
							Some(Expr::Column(datafusion::common::Column::new(None::<String>, f.name())).not_eq(lit(0.0)))
						},
						_ => None,
					}
				})
				.collect();
			
			if conditions.is_empty() {
				return Ok(df.clone());
			}
			
			conditions.into_iter().reduce(|acc, expr| acc.and(expr)).unwrap()
		},
	};
	
	let result = ctx.table(table_name).await?.filter(filter_expr)?;
	Ok(result)
}


// File: src/commands/correlations.rs
// Base: correlations

use clap::Args;
use std::path::PathBuf;
use crate::error::{NailError, NailResult};
use crate::utils::io::read_data;
use crate::utils::format::display_dataframe;
use crate::utils::stats::{calculate_correlations, CorrelationType, select_columns_by_pattern};

#[derive(Args, Clone)]
pub struct CorrelationsArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Comma-separated column names or regex patterns")]
	pub columns: Option<String>,
	
	#[arg(short = 't', long, help = "Correlation type", value_enum, default_value = "pearson")]
	pub correlation_type: CorrelationType,
	
	#[arg(long, help = "Output correlation matrix format")]
	pub correlation_matrix: bool,
	
	#[arg(long, help = "Include statistical significance tests")]
	pub stats_tests: bool,
	
	#[arg(long, help = "Number of decimal places for correlation values", default_value = "4")]
	pub digits: usize,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: CorrelationsArgs) -> NailResult<()> {
    if args.verbose {
        eprintln!("Reading data from: {}", args.input.display());
    }
    
    let df = read_data(&args.input).await?;
    let schema = df.schema();
    
    let target_columns = if let Some(col_spec) = &args.columns {
        let selected = select_columns_by_pattern(schema.clone().into(), col_spec)?;
        
        let mut numeric_columns = Vec::new();
        let mut non_numeric_columns = Vec::new();
        
        for col_name in &selected {
            if let Ok(field) = schema.field_with_name(None, col_name) {
                match field.data_type() {
                    datafusion::arrow::datatypes::DataType::Int64 | 
                    datafusion::arrow::datatypes::DataType::Float64 | 
                    datafusion::arrow::datatypes::DataType::Int32 | 
                    datafusion::arrow::datatypes::DataType::Float32 |
                    datafusion::arrow::datatypes::DataType::Int16 |
                    datafusion::arrow::datatypes::DataType::Int8 |
                    datafusion::arrow::datatypes::DataType::UInt64 |
                    datafusion::arrow::datatypes::DataType::UInt32 |
                    datafusion::arrow::datatypes::DataType::UInt16 |
                    datafusion::arrow::datatypes::DataType::UInt8 => {
                        numeric_columns.push(col_name.clone());
                    },
                    _ => {
                        non_numeric_columns.push((col_name.clone(), field.data_type().clone()));
                    }
                }
            }
        }
        
        if !non_numeric_columns.is_empty() {
            let non_numeric_info: Vec<String> = non_numeric_columns.iter()
                .map(|(name, dtype)| format!("'{}' ({:?})", name, dtype))
                .collect();
            return Err(NailError::Statistics(
                format!("Correlation requires numeric columns only. Non-numeric columns found: {}. Available numeric columns: {:?}", 
                    non_numeric_info.join(", "), numeric_columns)
            ));
        }
        
        if numeric_columns.len() < 2 {
            return Err(NailError::Statistics(
                format!("Need at least 2 numeric columns for correlation. Found {} numeric columns: {:?}", 
                    numeric_columns.len(), numeric_columns)
            ));
        }
        
        numeric_columns
    } else {
        let numeric_columns: Vec<String> = schema.fields().iter()
            .filter(|f| matches!(f.data_type(), 
                datafusion::arrow::datatypes::DataType::Int64 | 
                datafusion::arrow::datatypes::DataType::Float64 | 
                datafusion::arrow::datatypes::DataType::Int32 | 
                datafusion::arrow::datatypes::DataType::Float32 |
                datafusion::arrow::datatypes::DataType::Int16 |
                datafusion::arrow::datatypes::DataType::Int8 |
                datafusion::arrow::datatypes::DataType::UInt64 |
                datafusion::arrow::datatypes::DataType::UInt32 |
                datafusion::arrow::datatypes::DataType::UInt16 |
                datafusion::arrow::datatypes::DataType::UInt8
            ))
            .map(|f| f.name().clone())
            .collect();
            
        if numeric_columns.len() < 2 {
            return Err(NailError::Statistics(
                format!("Need at least 2 numeric columns for correlation. Found {} numeric columns: {:?}", 
                    numeric_columns.len(), numeric_columns)
            ));
        }
        
        numeric_columns
    };
    
    if args.verbose {
        eprintln!("Computing {:?} correlations for {} numeric columns: {:?}", 
            args.correlation_type, target_columns.len(), target_columns);
        eprintln!("Using {} decimal places for correlation values", args.digits);
    }
    
    let corr_df = calculate_correlations(
        &df, 
        &target_columns, 
        &args.correlation_type,
        args.correlation_matrix,
        args.stats_tests,
        args.digits
    ).await?;
    
    display_dataframe(&corr_df, args.output.as_deref(), args.format.as_ref()).await?;
    
    Ok(())
}


// File: tests/statistical_validation_tests.rs
// Base: statistical_validation_tests

use assert_cmd::Command;
use std::fs;
use std::path::PathBuf;
use tempfile::{tempdir, TempDir};
use serde_json::Value;

#[path = "common/mod.rs"]
mod common;

fn setup_statistical_data() -> (TempDir, PathBuf) {
	let temp_dir = tempdir().unwrap();
	let stats_path = temp_dir.path().join("statistical_data.parquet");
	common::create_statistical_test_data(&stats_path).unwrap();
	(temp_dir, stats_path)
}

fn setup_time_series_data() -> (TempDir, PathBuf) {
	let temp_dir = tempdir().unwrap();
	let ts_path = temp_dir.path().join("time_series.parquet");
	common::create_time_series_data(&ts_path).unwrap();
	(temp_dir, ts_path)
}

#[tokio::test]
async fn test_known_distribution_statistics() {
	let (td, input) = setup_statistical_data();
	let output = td.path().join("dist_stats.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["stats", "-i", input.to_str().unwrap(), "-t", "exhaustive",
			   "-c", "uniform_dist", "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let stats: Value = serde_json::from_str(&content).unwrap();
	
	// For uniform distribution 1-100:
	let mean = stats[0]["mean"].as_f64().unwrap();
	let min_val = stats[0]["min_val"].as_f64().unwrap();
	let max_val = stats[0]["max_val"].as_f64().unwrap();
	let median = stats[0]["median"].as_f64().unwrap();
	
	assert!((mean - 50.5).abs() < 1.0, "Mean should be ~50.5 for uniform 1-100, got {}", mean);
	assert_eq!(min_val, 1.0, "Min should be 1.0");
	assert_eq!(max_val, 100.0, "Max should be 100.0");
	assert!((median - 50.5).abs() < 1.0, "Median should be ~50.5, got {}", median);
}

#[tokio::test]
async fn test_correlation_with_known_relationship() {
	let (td, input) = setup_statistical_data();
	let output = td.path().join("corr_known.json");
	
	// Test correlation between id and uniform_dist (should be perfect 1.0)
	Command::cargo_bin("nail").unwrap()
		.args(["correlations", "-i", input.to_str().unwrap(),
			   "-c", "id,uniform_dist", "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let correlations: Value = serde_json::from_str(&content).unwrap();
	
	let correlation = correlations[0]["correlation"].as_f64().unwrap();
	assert!((correlation - 1.0).abs() < 0.001, 
		"Correlation between id and uniform_dist should be 1.0, got {}", correlation);
}

#[tokio::test]
async fn test_time_series_null_handling() {
	let (td, input) = setup_time_series_data();
	let output = td.path().join("ts_stats.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["stats", "-i", input.to_str().unwrap(), "-t", "basic",
			   "-c", "value", "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let stats: Value = serde_json::from_str(&content).unwrap();
	
	let count = stats[0]["count"].as_i64().unwrap();
	
	// Should have 43 non-null values (50 total - 7 nulls at positions 0,7,14,21,28,35,42,49)
	assert_eq!(count, 43, "Count should exclude NULL values, expected 43, got {}", count);
}

#[tokio::test]
async fn test_large_dataset_statistical_accuracy() {
	let (td, input) = setup_statistical_data();
	let output = td.path().join("large_stats.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["stats", "-i", input.to_str().unwrap(), "-t", "exhaustive",
			   "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let stats: Vec<Value> = serde_json::from_str(&content).unwrap();
	
	// Verify we have stats for all numeric columns
	assert_eq!(stats.len(), 3, "Should have stats for 3 numeric columns");
	
	// Each stat record should have all required fields
	for stat in &stats {
		assert!(stat["mean"].is_number(), "Mean should be present");
		assert!(stat["count"].is_number(), "Count should be present");
		assert!(stat["min_val"].is_number(), "Min should be present");
		assert!(stat["max_val"].is_number(), "Max should be present");
	}
}

#[test]
fn test_sample_size_validation() {
	let (td, input) = setup_statistical_data();
	let output = td.path().join("sample_output.json");
	
	// Test sampling exactly 25% of data
	Command::cargo_bin("nail").unwrap()
		.args(["sample", "-i", input.to_str().unwrap(), "-n", "25",
			   "--method", "random", "--random", "42",
			   "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let samples: Vec<Value> = serde_json::from_str(&content).unwrap();
	
	assert_eq!(samples.len(), 25, "Sample should contain exactly 25 records");
	
	// Verify sample contains valid data
	for sample in &samples {
		assert!(sample["id"].is_number(), "Sample should have valid id");
		assert!(sample["uniform_dist"].is_number(), "Sample should have valid uniform_dist");
	}
}

#[test]
fn test_stratified_sampling_balance() {
	let (td, input) = setup_time_series_data();
	let output = td.path().join("stratified_sample.json");
	
	// Test stratified sampling by category
	Command::cargo_bin("nail").unwrap()
		.args(["sample", "-i", input.to_str().unwrap(), "-n", "15",
			   "--method", "stratified", "--stratify-by", "category",
			   "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let samples: Vec<Value> = serde_json::from_str(&content).unwrap();
	
	// Count samples per category
	let mut category_counts = std::collections::HashMap::new();
	for sample in &samples {
		let category = sample["category"].as_str().unwrap();
		*category_counts.entry(category).or_insert(0) += 1;
	}
	
	// Should have roughly equal representation from each category
	assert!(category_counts.len() >= 2, "Should have samples from multiple categories");
	
	// Each category should have at least one sample
	for (category, count) in &category_counts {
		assert!(*count > 0, "Category {} should have at least one sample", category);
	}
}

#[tokio::test]
async fn test_filter_numerical_precision() {
	let (td, input) = setup_statistical_data();
	let output = td.path().join("filtered_precise.json");
	
	// Filter for exact value match
	Command::cargo_bin("nail").unwrap()
		.args(["filter", "-i", input.to_str().unwrap(), "-c", "uniform_dist=50.0",
			   "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let filtered: Vec<Value> = serde_json::from_str(&content).unwrap();
	
	assert_eq!(filtered.len(), 1, "Should find exactly one row with uniform_dist=50.0");
	assert_eq!(filtered[0]["uniform_dist"].as_f64().unwrap(), 50.0, "Filtered value should be exactly 50.0");
	assert_eq!(filtered[0]["id"].as_i64().unwrap(), 50, "Corresponding id should be 50");
}

#[tokio::test]
async fn test_aggregate_operations_accuracy() {
	let (td, input) = setup_time_series_data();
	
	// Test grouping and aggregation (simulated through filtering)
	let output_a = td.path().join("filtered_a.json");
	let output_b = td.path().join("filtered_b.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["filter", "-i", input.to_str().unwrap(), "-c", "category=A",
			   "-o", output_a.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	Command::cargo_bin("nail").unwrap()
		.args(["filter", "-i", input.to_str().unwrap(), "-c", "category=B",
			   "-o", output_b.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content_a = fs::read_to_string(&output_a).unwrap();
	let content_b = fs::read_to_string(&output_b).unwrap();
	
	let records_a: Vec<Value> = serde_json::from_str(&content_a).unwrap();
	let records_b: Vec<Value> = serde_json::from_str(&content_b).unwrap();
	
	// Verify category filtering worked correctly
	for record in &records_a {
		assert_eq!(record["category"].as_str().unwrap(), "A", "All records should be category A");
	}
	
	for record in &records_b {
		assert_eq!(record["category"].as_str().unwrap(), "B", "All records should be category B");
	}
	
	// Total should account for the 3-way split (A, B, C)
	assert!(records_a.len() > 0, "Should have some category A records");
	assert!(records_b.len() > 0, "Should have some category B records");
}

#[test]
fn test_mathematical_consistency() {
	let (td, input) = setup_statistical_data();
	let output = td.path().join("math_consistency.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["stats", "-i", input.to_str().unwrap(), "-t", "exhaustive",
			   "-c", "uniform_dist", "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let stats: Value = serde_json::from_str(&content).unwrap();
	
	let mean = stats[0]["mean"].as_f64().unwrap();
	let median = stats[0]["median"].as_f64().unwrap();
	let q25 = stats[0]["q25"].as_f64().unwrap();
	let q75 = stats[0]["q75"].as_f64().unwrap();
	let min_val = stats[0]["min_val"].as_f64().unwrap();
	let max_val = stats[0]["max_val"].as_f64().unwrap();
	
	// Mathematical consistency checks
	assert!(min_val <= q25, "Min should be <= Q25: min={}, q25={}", min_val, q25);
	assert!(q25 <= median, "Q25 should be <= median: q25={}, median={}", q25, median);
	assert!(median <= q75, "Median should be <= Q75: median={}, q75={}", median, q75);
	assert!(q75 <= max_val, "Q75 should be <= max: q75={}, max={}", q75, max_val);
	
	// For uniform distribution, mean should approximately equal median
	assert!((mean - median).abs() < 1.0, 
		"For uniform distribution, mean and median should be close: mean={}, median={}", mean, median);
}


// File: src/commands/select.rs
// Base: select

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use regex::Regex;
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct SelectArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Column names or regex patterns (comma-separated)")]
	pub columns: Option<String>,
	
	#[arg(short, long, help = "Row numbers or ranges (e.g., 1,3,5-10)")]
	pub rows: Option<String>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Number of parallel jobs")]
	pub jobs: Option<usize>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: SelectArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let mut result_df = df;
	
	if let Some(col_spec) = &args.columns {
		let schema = result_df.schema();
		let selected_columns = select_columns_by_pattern(schema.clone().into(), col_spec)?;
		
		if args.verbose {
			eprintln!("Selecting {} columns: {:?}", selected_columns.len(), selected_columns);
		}
		
		let select_exprs: Vec<Expr> = selected_columns.into_iter()
			.map(|name| Expr::Column(datafusion::common::Column::new(None::<String>, &name)))
			.collect();
		
		result_df = result_df.select(select_exprs)?;
	}
	
	if let Some(row_spec) = &args.rows {
		let row_indices = parse_row_specification(row_spec)?;
		
		if args.verbose {
			eprintln!("Selecting {} rows", row_indices.len());
		}
		
		result_df = select_rows_by_indices(&result_df, &row_indices, args.jobs).await?;
	}
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&result_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&result_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

pub fn select_columns_by_pattern(schema: datafusion::common::DFSchemaRef, pattern: &str) -> NailResult<Vec<String>> {
	let patterns: Vec<&str> = pattern.split(',').map(|s| s.trim()).collect();
	let mut selected = Vec::new();
	let mut not_found = Vec::new();
	
	for pattern in &patterns {
		let mut found = false;
		
		// First try exact match (case-sensitive)
		for field in schema.fields() {
			let field_name = field.name();
			
			if pattern.contains('*') || pattern.contains('^') || pattern.contains('$') {
				let regex = Regex::new(pattern)?;
				if regex.is_match(field_name) {
					selected.push(field_name.clone());
					found = true;
				}
			} else if field_name == *pattern {
				selected.push(field_name.clone());
				found = true;
				break;
			}
		}
		
		// If not found, try case-insensitive match
		if !found {
			for field in schema.fields() {
				let field_name = field.name();
				
				if pattern.contains('*') || pattern.contains('^') || pattern.contains('$') {
					// For regex patterns, create case-insensitive version
					let case_insensitive_pattern = format!("(?i){}", pattern);
					if let Ok(regex) = Regex::new(&case_insensitive_pattern) {
						if regex.is_match(field_name) {
							selected.push(field_name.clone());
							found = true;
						}
					}
				} else if field_name.to_lowercase() == pattern.to_lowercase() {
					selected.push(field_name.clone());
					found = true;
					break;
				}
			}
		}
		
		if !found {
			not_found.push(*pattern);
		}
	}
	
	if !not_found.is_empty() {
		let available_columns: Vec<String> = schema.fields().iter()
			.map(|f| f.name().clone())
			.collect();
		return Err(NailError::ColumnNotFound(format!(
			"Columns not found: {:?}. Available columns: {:?}", 
			not_found, available_columns
		)));
	}
	
	// Remove duplicates while preserving order
	let mut unique_selected = Vec::new();
	for col in selected {
		if !unique_selected.contains(&col) {
			unique_selected.push(col);
		}
	}
	
	if unique_selected.is_empty() {
		return Err(NailError::ColumnNotFound(format!("No columns matched pattern: {}", pattern)));
	}
	
	Ok(unique_selected)
}

pub fn parse_row_specification(spec: &str) -> NailResult<Vec<usize>> {
	let mut indices = Vec::new();
	
	for part in spec.split(',') {
		let part = part.trim();
		
		if part.contains('-') {
			let range_parts: Vec<&str> = part.split('-').collect();
			if range_parts.len() != 2 {
				return Err(NailError::InvalidArgument(format!("Invalid range: {}", part)));
			}
			
			let start: usize = range_parts[0].parse()
				.map_err(|_| NailError::InvalidArgument(format!("Invalid start index: {}", range_parts[0])))?;
			let end: usize = range_parts[1].parse()
				.map_err(|_| NailError::InvalidArgument(format!("Invalid end index: {}", range_parts[1])))?;
			
			if start > end {
				return Err(NailError::InvalidArgument(format!("Start index {} greater than end index {}", start, end)));
			}
			
			for i in start..=end {
				indices.push(i.saturating_sub(1));
			}
		} else {
			let index: usize = part.parse()
				.map_err(|_| NailError::InvalidArgument(format!("Invalid index: {}", part)))?;
			indices.push(index.saturating_sub(1));
		}
	}
	
	indices.sort();
	indices.dedup();
	Ok(indices)
}

async fn select_rows_by_indices(df: &DataFrame, indices: &[usize], jobs: Option<usize>) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context_with_jobs(jobs).await?;
	
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let indices_str = indices.iter()
		.map(|&i| (i + 1).to_string())
		.collect::<Vec<_>>()
		.join(",");
	
	// Get the original column names and quote them to preserve case
	let original_columns: Vec<String> = df.schema().fields().iter()
		.map(|f| format!("\"{}\"", f.name()))
		.collect();
	
	let sql = format!(
		"SELECT {} FROM (SELECT {}, ROW_NUMBER() OVER() as rn FROM {}) WHERE rn IN ({})",
		original_columns.join(", "),
		original_columns.join(", "),
		table_name, 
		indices_str
	);
	
	let result = ctx.sql(&sql).await?;
	
	Ok(result)
}


// File: src/utils/io.rs
// Base: io

use datafusion::prelude::*;
use datafusion::dataframe::DataFrameWriteOptions;
use std::path::Path;
use crate::error::{NailError, NailResult};
use crate::utils::{create_context, detect_file_format, FileFormat};
use datafusion::arrow::array::{ArrayRef, StringArray, Float64Array, Int64Array, BooleanArray, RecordBatch};
use datafusion::arrow::datatypes::{DataType, Field, Schema};
use calamine::{Reader, Xlsx, open_workbook, Data};
use std::sync::Arc;

pub async fn read_data(path: &Path) -> NailResult<DataFrame> {
	let ctx = create_context().await?;
	let format = detect_file_format(path)?;
	
	let result = match format {
		FileFormat::Parquet => {
			ctx.read_parquet(path.to_str().unwrap(), ParquetReadOptions::default()).await
		},
		FileFormat::Csv => {
			ctx.read_csv(path.to_str().unwrap(), CsvReadOptions::default()).await
		},
		FileFormat::Json => {
			ctx.read_json(path.to_str().unwrap(), NdJsonReadOptions::default()).await
		},
		FileFormat::Excel => {
			read_excel_file(path, &ctx).await
		},
	};
	
	result.map_err(NailError::DataFusion)
}

async fn read_excel_file(path: &Path, ctx: &SessionContext) -> Result<DataFrame, datafusion::error::DataFusionError> {
	let mut workbook: Xlsx<_> = open_workbook(path)
		.map_err(|e| datafusion::error::DataFusionError::External(Box::new(e)))?;
	
	// Get the first worksheet
	let sheet_names = workbook.sheet_names();
	if sheet_names.is_empty() {
		return Err(datafusion::error::DataFusionError::External(
			"No worksheets found in Excel file".into()
		));
	}
	
	let sheet_name = &sheet_names[0];
	let range = workbook.worksheet_range(sheet_name)
		.map_err(|e| datafusion::error::DataFusionError::External(Box::new(e)))?;
	
	if range.is_empty() {
		return Err(datafusion::error::DataFusionError::External(
			"Empty worksheet".into()
		));
	}
	
	// Extract headers from first row
	let mut headers = Vec::new();
	let (rows, cols) = range.get_size();
	
	for col in 0..cols {
		let cell_value = range.get_value((0, col as u32)).unwrap_or(&Data::Empty);
		let header = match cell_value {
			Data::String(s) => s.clone(),
			Data::Int(i) => i.to_string(),
			Data::Float(f) => f.to_string(),
			_ => format!("Column_{}", col + 1),
		};
		headers.push(header);
	}
	
	// Determine column types by sampling data
	let mut column_types = vec![DataType::Utf8; cols];
	for col in 0..cols {
		let mut has_int = false;
		let mut has_float = false;
		let mut has_bool = false;
		
		// Sample up to 10 rows to determine type
		let sample_rows = std::cmp::min(rows, 11);
		for row in 1..sample_rows {
			let cell_value = range.get_value((row as u32, col as u32)).unwrap_or(&Data::Empty);
			match cell_value {
				Data::Int(_) => has_int = true,
				Data::Float(_) => has_float = true,
				Data::Bool(_) => has_bool = true,
				_ => {}
			}
		}
		
		// Determine type priority: Bool > Float > Int > String
		if has_bool && !has_int && !has_float {
			column_types[col] = DataType::Boolean;
		} else if has_float {
			column_types[col] = DataType::Float64;
		} else if has_int && !has_float {
			column_types[col] = DataType::Int64;
		}
	}
	
	// Create schema
	let fields: Vec<Field> = headers.iter().zip(column_types.iter())
		.map(|(name, dtype)| Field::new(name, dtype.clone(), true))
		.collect();
	let schema = Arc::new(Schema::new(fields));
	
	// Extract data
	let mut columns: Vec<ArrayRef> = Vec::new();
	
	for col in 0..cols {
		let dtype = &column_types[col];
		let mut values = Vec::new();
		
		for row in 1..rows {
			let cell_value = range.get_value((row as u32, col as u32)).unwrap_or(&Data::Empty);
			values.push(cell_value.clone());
		}
		
		let array: ArrayRef = match dtype {
			DataType::Boolean => {
				let bool_values: Vec<Option<bool>> = values.iter().map(|v| match v {
					Data::Bool(b) => Some(*b),
					Data::String(s) => {
						match s.to_lowercase().as_str() {
							"true" | "1" | "yes" => Some(true),
							"false" | "0" | "no" => Some(false),
							_ => None,
						}
					},
					_ => None,
				}).collect();
				Arc::new(BooleanArray::from(bool_values))
			},
			DataType::Int64 => {
				let int_values: Vec<Option<i64>> = values.iter().map(|v| match v {
					Data::Int(i) => Some(*i),
					Data::Float(f) => Some(*f as i64),
					Data::String(s) => s.parse().ok(),
					_ => None,
				}).collect();
				Arc::new(Int64Array::from(int_values))
			},
			DataType::Float64 => {
				let float_values: Vec<Option<f64>> = values.iter().map(|v| match v {
					Data::Float(f) => Some(*f),
					Data::Int(i) => Some(*i as f64),
					Data::String(s) => s.parse().ok(),
					_ => None,
				}).collect();
				Arc::new(Float64Array::from(float_values))
			},
			_ => { // String
				let string_values: Vec<Option<String>> = values.iter().map(|v| match v {
					Data::String(s) => Some(s.clone()),
					Data::Int(i) => Some(i.to_string()),
					Data::Float(f) => Some(f.to_string()),
					Data::Bool(b) => Some(b.to_string()),
					Data::DateTime(dt) => Some(format!("{}", dt)),
					Data::DateTimeIso(dt) => Some(dt.clone()),
					Data::DurationIso(d) => Some(d.clone()),
					Data::Error(e) => Some(format!("Error: {:?}", e)),
					Data::Empty => None,
				}).collect();
				Arc::new(StringArray::from(string_values))
			}
		};
		
		columns.push(array);
	}
	
	// Create record batch
	let batch = RecordBatch::try_new(schema.clone(), columns)
		.map_err(|e| datafusion::error::DataFusionError::External(Box::new(e)))?;
	
	// Convert to DataFrame
	ctx.read_batch(batch)
}

pub async fn write_data(df: &DataFrame, path: &Path, format: Option<&FileFormat>) -> NailResult<()> {
	let output_format = format.map(|f| f.clone()).unwrap_or_else(|| detect_file_format(path).unwrap_or(FileFormat::Parquet));
	
	match output_format {
		FileFormat::Parquet => {
			df.clone().write_parquet(
				path.to_str().unwrap(),
				DataFrameWriteOptions::new(),
				None,
			).await.map_err(NailError::DataFusion)?;
		},
		FileFormat::Csv => {
			df.clone().write_csv(
				path.to_str().unwrap(),
				DataFrameWriteOptions::new(),
				None,
			).await.map_err(NailError::DataFusion)?;
		},
		FileFormat::Json => {
			df.clone().write_json(
				path.to_str().unwrap(),
				DataFrameWriteOptions::new(),
				None,
			).await.map_err(NailError::DataFusion)?;
		},
		FileFormat::Excel => {
			return Err(NailError::UnsupportedFormat("Writing to Excel format is not yet supported".to_string()));
		},
	};
	
	Ok(())
}


// File: src/build.rs
// Base: build

fn main() {
	println!("cargo:rerun-if-changed=build.rs");
}


// File: tests/integration_tests.rs
// Base: integration_tests

use assert_cmd::Command;
use predicates::prelude::*;
use std::fs;
use std::path::PathBuf;
use tempfile::{tempdir, TempDir};
use serde_json::Value;
use arrow::array::{Array, Float64Array, Int64Array, StringArray};
use datafusion::prelude::*;

#[path = "common/mod.rs"]
mod common;

fn setup() -> (TempDir, PathBuf, PathBuf, PathBuf, PathBuf) {
	let temp_dir = tempdir().unwrap();
	let input_path = temp_dir.path().join("sample.parquet");
	let input2_path = temp_dir.path().join("sample2.parquet");
	let input3_path = temp_dir.path().join("sample3.parquet");
	let input_csv_path = temp_dir.path().join("sample.csv");

	common::create_sample_parquet(&input_path).unwrap();
	common::create_sample2_parquet(&input2_path).unwrap();
	common::create_sample3_parquet(&input3_path).unwrap();
	common::create_sample_csv(&input_csv_path).unwrap();

	(temp_dir, input_path, input2_path, input3_path, input_csv_path)
}

fn setup_with_duplicates() -> (TempDir, PathBuf) {
	let temp_dir = tempdir().unwrap();
	let input_path = temp_dir.path().join("sample_with_duplicates.parquet");
	common::create_sample_with_duplicates(&input_path).unwrap();
	(temp_dir, input_path)
}

fn create_empty_dataset(temp_dir: &TempDir) -> PathBuf {
	let empty_file = temp_dir.path().join("empty.parquet");
	common::create_empty_dataset(&empty_file).unwrap();
	empty_file
}

fn create_single_row_dataset(temp_dir: &TempDir) -> PathBuf {
	let single_row_file = temp_dir.path().join("single.parquet");
	common::create_single_row_dataset(&single_row_file).unwrap();
	single_row_file
}

fn create_large_dataset(temp_dir: &TempDir, num_rows: usize) -> PathBuf {
	let large_file = temp_dir.path().join("large.parquet");
	common::create_large_dataset(&large_file, num_rows).unwrap();
	large_file
}

fn create_mixed_types_dataset(temp_dir: &TempDir) -> PathBuf {
	let mixed_file = temp_dir.path().join("mixed_types.parquet");
	common::create_mixed_types_dataset(&mixed_file).unwrap();
	mixed_file
}

// Statistical Validation Tests
#[tokio::test]
async fn test_mean_calculation_accuracy() {
	let (td, input, _, _, _) = setup();
	let output = td.path().join("stats.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["stats", "-i", input.to_str().unwrap(), "-t", "basic", 
			   "-c", "value", "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let stats: Value = serde_json::from_str(&content).unwrap();
	
	// Test data: [100.0, NULL, 300.0, 400.0, 500.0]
	// Valid values: [100.0, 300.0, 400.0, 500.0]
	// Expected mean: (100 + 300 + 400 + 500) / 4 = 325.0
	let mean = stats[0]["mean"].as_f64().unwrap();
	assert!((mean - 325.0).abs() < 0.001, "Mean calculation incorrect: got {}, expected 325.0", mean);
	
	let count = stats[0]["count"].as_i64().unwrap();
	assert_eq!(count, 4, "Count should exclude NULL values, got {}", count);
}

#[tokio::test]
async fn test_correlation_mathematical_correctness() {
	let (td, input, _, _, _) = setup();
	let output = td.path().join("corr.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["correlations", "-i", input.to_str().unwrap(), 
			   "-c", "id,value", "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let correlations: Value = serde_json::from_str(&content).unwrap();
	
	// Calculate expected correlation manually
	// Valid pairs: (1,100), (3,300), (4,400), (5,500) - excluding NULL row
	let correlation = correlations[0]["correlation"].as_f64().unwrap();
	
	// Perfect positive correlation expected
	assert!(correlation > 0.99, "Correlation should be nearly perfect positive, got {}", correlation);
	assert!(correlation <= 1.0, "Correlation cannot exceed 1.0, got {}", correlation);
}

#[tokio::test]
async fn test_percentile_calculations() {
	let (td, input, _, _, _) = setup();
	let output = td.path().join("stats.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["stats", "-i", input.to_str().unwrap(), "-t", "exhaustive",
			   "-c", "value", "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let stats: Value = serde_json::from_str(&content).unwrap();
	
	// For sorted values [100, 300, 400, 500]:
	let q25 = stats[0]["q25"].as_f64().unwrap();
	let median = stats[0]["median"].as_f64().unwrap();
	let q75 = stats[0]["q75"].as_f64().unwrap();
	let min_val = stats[0]["min_val"].as_f64().unwrap();
	let max_val = stats[0]["max_val"].as_f64().unwrap();
	
	assert_eq!(min_val, 100.0, "Minimum should be 100.0");
	assert_eq!(max_val, 500.0, "Maximum should be 500.0");
	assert!(q25 >= 100.0 && q25 <= 350.0, "Q25 out of reasonable range: {}", q25);
	assert!(median >= 300.0 && median <= 400.0, "Median out of reasonable range: {}", median);
	assert!(q75 >= 350.0 && q75 <= 500.0, "Q75 out of reasonable range: {}", q75);
	
	// Verify ordering
	assert!(min_val <= q25, "Min should be <= Q25");
	assert!(q25 <= median, "Q25 should be <= median");
	assert!(median <= q75, "Median should be <= Q75");
	assert!(q75 <= max_val, "Q75 should be <= max");
}

#[tokio::test]
async fn test_standard_deviation_calculation() {
	let (td, input, _, _, _) = setup();
	let output = td.path().join("stats.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["stats", "-i", input.to_str().unwrap(), "-t", "exhaustive",
			   "-c", "value", "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let stats: Value = serde_json::from_str(&content).unwrap();
	
	let std_dev = stats[0]["std_dev"].as_f64().unwrap();
	let variance = stats[0]["variance"].as_f64().unwrap();
	
	// Verify relationship between variance and standard deviation
	assert!((std_dev * std_dev - variance).abs() < 0.001, 
		"Standard deviation squared should equal variance: std_dev={}, variance={}", std_dev, variance);
	
	// For values [100, 300, 400, 500], std dev should be > 0
	assert!(std_dev > 0.0, "Standard deviation should be positive for varied data");
}

// Data Integrity Tests
#[tokio::test]
async fn test_join_preserves_data_relationships() {
	let (td, input1, input2, _, _) = setup();
	let output = td.path().join("joined.parquet");
	
	// Perform inner join
	Command::cargo_bin("nail").unwrap()
		.args(["merge", "-i", input1.to_str().unwrap(), "--right", input2.to_str().unwrap(),
			   "--key", "id", "-o", output.to_str().unwrap()])
		.assert().success();
	
	// Verify join results by reading back data
	let ctx = SessionContext::new();
	let df = ctx.read_parquet(output.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap();
	let batches = df.collect().await.unwrap();
	
	// Inner join should only have rows where id exists in both tables
	// sample1: ids [1,2,3,4,5], sample2: ids [4,5,6,7]
	// Expected matches: ids [4,5] -> 2 rows
	let total_rows: usize = batches.iter().map(|b| b.num_rows()).sum();
	assert_eq!(total_rows, 2, "Inner join should return exactly 2 matching rows, got {}", total_rows);
	
	// Verify specific data integrity
	let mut found_eve = false;
	for batch in &batches {
		let name_col = batch.column(1).as_any().downcast_ref::<StringArray>().unwrap();
		
		for i in 0..batch.num_rows() {
			if name_col.is_valid(i) && name_col.value(i) == "Eve" {
				found_eve = true;
				// Eve should have id=5 and score=92.5
				let id_col = batch.column(0).as_any().downcast_ref::<Int64Array>().unwrap();
				assert_eq!(id_col.value(i), 5, "Eve should have id=5");
			}
		}
	}
	assert!(found_eve, "Inner join should contain Eve's record");
}

#[tokio::test]
async fn test_left_join_preserves_all_left_records() {
	let (td, input1, input2, _, _) = setup();
	let output = td.path().join("left_joined.parquet");
	
	Command::cargo_bin("nail").unwrap()
		.args(["merge", "-i", input1.to_str().unwrap(), "--right", input2.to_str().unwrap(),
			   "--left-join", "--key", "id", "-o", output.to_str().unwrap()])
		.assert().success();
	
	let ctx = SessionContext::new();
	let df = ctx.read_parquet(output.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap();
	let batches = df.collect().await.unwrap();
	
	// Left join should preserve all left table rows
	let total_rows: usize = batches.iter().map(|b| b.num_rows()).sum();
	assert_eq!(total_rows, 5, "Left join should preserve all 5 left table rows, got {}", total_rows);
}

#[tokio::test]
async fn test_fill_operation_completeness() {
	let (td, input, _, _, _) = setup();
	let output = td.path().join("filled.parquet");
	
	Command::cargo_bin("nail").unwrap()
		.args(["fill", "-i", input.to_str().unwrap(), "--method", "value",
			   "--value", "999", "-c", "value", "-o", output.to_str().unwrap()])
		.assert().success();
	
	// Verify no NULL values remain in the target column
	let ctx = SessionContext::new();
	let df = ctx.read_parquet(output.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap();
	let batches = df.collect().await.unwrap();
	
	let mut found_fill_value = false;
	for batch in &batches {
		let value_col = batch.column(2).as_any().downcast_ref::<Float64Array>().unwrap();
		
		for i in 0..batch.num_rows() {
			assert!(value_col.is_valid(i), "Found NULL value after fill operation at row {}", i);
			
			// Check if we found our fill value
			if value_col.value(i) == 999.0 {
				found_fill_value = true;
			}
		}
	}
	assert!(found_fill_value, "Fill value 999.0 should appear in the data");
}

#[tokio::test]
async fn test_fill_mean_calculation() {
	let (td, input, _, _, _) = setup();
	let output = td.path().join("filled_mean.parquet");
	
	Command::cargo_bin("nail").unwrap()
		.args(["fill", "-i", input.to_str().unwrap(), "--method", "mean",
			   "-c", "value", "-o", output.to_str().unwrap()])
		.assert().success();
	
	let ctx = SessionContext::new();
	let df = ctx.read_parquet(output.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap();
	let batches = df.collect().await.unwrap();
	
	// Verify mean fill (325.0) appears where NULL was
	let mut found_mean_fill = false;
	for batch in &batches {
		let value_col = batch.column(2).as_any().downcast_ref::<Float64Array>().unwrap();
		
		for i in 0..batch.num_rows() {
			if (value_col.value(i) - 325.0).abs() < 0.001 {
				found_mean_fill = true;
			}
		}
	}
	assert!(found_mean_fill, "Mean fill value should appear in data");
}

#[tokio::test]
async fn test_deduplication_correctness() {
	let (td, input) = setup_with_duplicates();
	let output = td.path().join("deduped.parquet");
	
	Command::cargo_bin("nail").unwrap()
		.args(["dedup", "-i", input.to_str().unwrap(), "--row-wise",
			   "-o", output.to_str().unwrap()])
		.assert().success();
	
	// Verify deduplication worked
	let ctx = SessionContext::new();
	let original_df = ctx.read_parquet(input.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap();
	let deduped_df = ctx.read_parquet(output.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap();
	
	let original_count = original_df.clone().count().await.unwrap();
	let deduped_count = deduped_df.clone().count().await.unwrap();
	
	// Original has duplicates, deduped should have fewer rows
	assert!(deduped_count < original_count, 
		"Deduplication should reduce row count: original={}, deduped={}", 
		original_count, deduped_count);
	
	// Check that each name appears only once
	let batches = deduped_df.collect().await.unwrap();
	let mut names = Vec::new();
	
	for batch in &batches {
		let name_col = batch.column(1).as_any().downcast_ref::<StringArray>().unwrap();
		for i in 0..batch.num_rows() {
			if name_col.is_valid(i) {
				let name = name_col.value(i).to_string();
				assert!(!names.contains(&name), "Duplicate name found after deduplication: {}", name);
				names.push(name);
			}
		}
	}
}

#[tokio::test]
async fn test_dedup_keep_last_option() {
	let (td, input) = setup_with_duplicates();
	let output = td.path().join("deduped_last.parquet");
	
	Command::cargo_bin("nail").unwrap()
		.args(["dedup", "-i", input.to_str().unwrap(), "--row-wise",
			   "--keep", "last", "-o", output.to_str().unwrap()])
		.assert().success();
	
	let ctx = SessionContext::new();
	let deduped_df = ctx.read_parquet(output.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap();
	let batches = deduped_df.collect().await.unwrap();
	
	// Verify unique names
	let mut names = std::collections::HashSet::new();
	for batch in &batches {
		let name_col = batch.column(1).as_any().downcast_ref::<StringArray>().unwrap();
		for i in 0..batch.num_rows() {
			if name_col.is_valid(i) {
				let name = name_col.value(i).to_string();
				assert!(names.insert(name.clone()), "Found duplicate name: {}", name);
			}
		}
	}
}

// Format Conversion Tests
#[tokio::test]
async fn test_roundtrip_data_preservation() {
	let (td, input, _, _, _) = setup();
	let csv_output = td.path().join("converted.csv");
	let parquet_output = td.path().join("roundtrip.parquet");
	
	// Convert to CSV
	Command::cargo_bin("nail").unwrap()
		.args(["convert", "-i", input.to_str().unwrap(), 
			   "-o", csv_output.to_str().unwrap()])
		.assert().success();
	
	// Convert back to Parquet
	Command::cargo_bin("nail").unwrap()
		.args(["convert", "-i", csv_output.to_str().unwrap(),
			   "-o", parquet_output.to_str().unwrap()])
		.assert().success();
	
	// Compare original and roundtrip data
	let ctx = SessionContext::new();
	let original_df = ctx.read_parquet(input.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap();
	let roundtrip_df = ctx.read_parquet(parquet_output.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap();
	
	let original_count = original_df.clone().count().await.unwrap();
	let roundtrip_count = roundtrip_df.clone().count().await.unwrap();
	
	assert_eq!(original_count, roundtrip_count, 
		"Row count should be preserved in roundtrip conversion: original={}, roundtrip={}", 
		original_count, roundtrip_count);
	
	// Verify non-null string values are preserved
	let original_batches = original_df.collect().await.unwrap();
	let roundtrip_batches = roundtrip_df.collect().await.unwrap();
	
	assert_eq!(original_batches.len(), roundtrip_batches.len(), "Batch count should match");
	
	for (orig_batch, rt_batch) in original_batches.iter().zip(roundtrip_batches.iter()) {
		let orig_names = orig_batch.column(1).as_any().downcast_ref::<StringArray>().unwrap();
		let rt_names = rt_batch.column(1).as_any().downcast_ref::<StringArray>().unwrap();
		
		for i in 0..orig_batch.num_rows() {
			if orig_names.is_valid(i) && rt_names.is_valid(i) {
				assert_eq!(orig_names.value(i), rt_names.value(i), 
						   "String value mismatch at row {}: '{}' vs '{}'", 
						   i, orig_names.value(i), rt_names.value(i));
			}
		}
	}
}

#[tokio::test]
async fn test_json_conversion_structure() {
	let (td, input, _, _, _) = setup();
	let json_output = td.path().join("converted.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["convert", "-i", input.to_str().unwrap(), 
			   "-o", json_output.to_str().unwrap()])
		.assert().success();
	
	let content = fs::read_to_string(&json_output).unwrap();
	let json_data: Vec<Value> = serde_json::from_str(&content).unwrap();
	
	assert_eq!(json_data.len(), 5, "JSON should contain 5 records");
	
	// Verify structure of first record
	let first_record = &json_data[0];
	assert!(first_record["id"].is_number(), "id should be a number");
	assert!(first_record["name"].is_string(), "name should be a string");
	assert!(first_record["category"].is_string(), "category should be a string");
}

// Error Handling Tests
#[test]
fn test_invalid_column_reference_provides_helpful_error() {
	let (_td, input, _, _, _) = setup();
	
	let result = Command::cargo_bin("nail").unwrap()
		.args(["select", "-i", input.to_str().unwrap(), "-c", "nonexistent_column"])
		.assert().failure();
	
	let stderr = String::from_utf8(result.get_output().stderr.clone()).unwrap();
	assert!(stderr.contains("not found") || stderr.contains("Column"), 
			"Error message should indicate column not found, got: {}", stderr);
	assert!(stderr.contains("Available columns"), 
			"Error should list available columns, got: {}", stderr);
}

#[test]
fn test_correlation_insufficient_numeric_columns() {
	let (_td, input, _, _, _) = setup();
	
	// Try correlation with only string columns
	let result = Command::cargo_bin("nail").unwrap()
		.args(["correlations", "-i", input.to_str().unwrap(), "-c", "name,category"])
		.assert().failure();
	
	let stderr = String::from_utf8(result.get_output().stderr.clone()).unwrap();
	assert!(stderr.contains("numeric") || stderr.contains("at least 2"), 
			"Should explain need for numeric columns, got: {}", stderr);
}

#[test]
fn test_file_not_found_error_handling() {
	Command::cargo_bin("nail").unwrap()
		.args(["head", "-i", "/nonexistent/path/file.parquet"])
		.assert().failure()
		.stderr(predicate::str::contains("No such file").or(predicate::str::contains("not found")));
}

#[test]
fn test_invalid_regex_pattern_error() {
	let (_td, input, _, _, _) = setup();
	
	Command::cargo_bin("nail").unwrap()
		.args(["headers", "-i", input.to_str().unwrap(), "--filter", "[unclosed"])
		.assert().failure()
		.stderr(predicate::str::contains("regex").or(predicate::str::contains("pattern")));
}

#[test]
fn test_fill_without_value_parameter() {
	let (_td, input, _, _, _) = setup();
	
	Command::cargo_bin("nail").unwrap()
		.args(["fill", "-i", input.to_str().unwrap(), "--method", "value"])
		.assert().failure()
		.stderr(predicate::str::contains("value").or(predicate::str::contains("required")));
}

#[test]
fn test_invalid_split_ratios() {
	let (td, input, _, _, _) = setup();
	let output_dir = td.path().join("splits");
	std::fs::create_dir_all(&output_dir).unwrap();
	
	// Test ratios that sum to > 1.0
	Command::cargo_bin("nail").unwrap()
		.args(["split", "-i", input.to_str().unwrap(), "--ratio", "0.7,0.8",
			   "--output-dir", output_dir.to_str().unwrap()])
		.assert().failure()
		.stderr(predicate::str::contains("sum"));
}

#[test]
fn test_merge_missing_key_error() {
	let (_td, input1, input2, _, _) = setup();
	
	Command::cargo_bin("nail").unwrap()
		.args(["merge", "-i", input1.to_str().unwrap(), "--right", input2.to_str().unwrap()])
		.assert().failure()
		.stderr(predicate::str::contains("key").or(predicate::str::contains("required")));
}

// Edge Case Tests
#[test]
fn test_empty_dataset_handling() {
	let temp_dir = tempdir().unwrap();
	let empty_file = create_empty_dataset(&temp_dir);
	
	// Test count on empty dataset
	Command::cargo_bin("nail").unwrap()
		.args(["count", "-i", empty_file.to_str().unwrap()])
		.assert().success()
		.stdout("0\n");
	
	// Test stats on empty dataset - should handle gracefully
	Command::cargo_bin("nail").unwrap()
		.args(["stats", "-i", empty_file.to_str().unwrap()])
		.assert().success();
	
	// Test head on empty dataset
	Command::cargo_bin("nail").unwrap()
		.args(["head", "-i", empty_file.to_str().unwrap()])
		.assert().success();
}

#[tokio::test]
async fn test_single_row_statistical_operations() {
	let temp_dir = tempdir().unwrap();
	let single_row_file = create_single_row_dataset(&temp_dir);
	let output = temp_dir.path().join("single_stats.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["stats", "-i", single_row_file.to_str().unwrap(),
			   "-c", "value", "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let stats: Value = serde_json::from_str(&content).unwrap();
	
	// For single value, statistics should equal the value itself
	let mean = stats[0]["mean"].as_f64().unwrap();
	let count = stats[0]["count"].as_i64().unwrap();
	
	assert_eq!(mean, 42.0, "Mean of single value should be the value itself");
	assert_eq!(count, 1, "Count should be 1 for single row");
}

#[tokio::test]
async fn test_large_dataset_performance() {
	let temp_dir = tempdir().unwrap();
	let large_file = create_large_dataset(&temp_dir, 1000);
	
	// Test that operations complete successfully on larger datasets
	Command::cargo_bin("nail").unwrap()
		.args(["count", "-i", large_file.to_str().unwrap()])
		.assert().success()
		.stdout("1000\n");
	
	Command::cargo_bin("nail").unwrap()
		.args(["stats", "-i", large_file.to_str().unwrap(), "-c", "value"])
		.assert().success();
	
	// Test sample on large dataset
	Command::cargo_bin("nail").unwrap()
		.args(["sample", "-i", large_file.to_str().unwrap(), "-n", "100"])
		.assert().success();
}

#[tokio::test]
async fn test_mixed_data_types_handling() {
	let temp_dir = tempdir().unwrap();
	let mixed_file = create_mixed_types_dataset(&temp_dir);
	let output = temp_dir.path().join("mixed_stats.json");
	
	// Test stats on mixed types
	Command::cargo_bin("nail").unwrap()
		.args(["stats", "-i", mixed_file.to_str().unwrap(),
			   "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let stats: Vec<Value> = serde_json::from_str(&content).unwrap();
	
	// Should have stats for numeric columns only
	let numeric_columns: Vec<_> = stats.iter()
		.filter(|s| s["column_name"].as_str().unwrap().contains("col"))
		.collect();
	
	assert!(numeric_columns.len() >= 2, "Should have stats for numeric columns");
}

// Reproducibility Tests
#[test]
fn test_seeded_operations_reproducibility() {
	let (_td, input, _, _, _) = setup();
	
	// Run same seeded operation twice
	let result1 = Command::cargo_bin("nail").unwrap()
		.args(["sample", "-i", input.to_str().unwrap(), "-n", "3", 
			   "--method", "random", "--random", "12345"])
		.assert().success()
		.get_output().stdout.clone();
	
	let result2 = Command::cargo_bin("nail").unwrap()
		.args(["sample", "-i", input.to_str().unwrap(), "-n", "3",
			   "--method", "random", "--random", "12345"])
		.assert().success()
		.get_output().stdout.clone();
	
	assert_eq!(result1, result2, "Seeded random operations should be reproducible");
}

#[test]
fn test_different_seeds_produce_different_results() {
	let (_td, input, _, _, _) = setup();
	
	let result1 = Command::cargo_bin("nail").unwrap()
		.args(["sample", "-i", input.to_str().unwrap(), "-n", "3", 
			   "--method", "random", "--random", "111"])
		.assert().success()
		.get_output().stdout.clone();
	
	let result2 = Command::cargo_bin("nail").unwrap()
		.args(["sample", "-i", input.to_str().unwrap(), "-n", "3",
			   "--method", "random", "--random", "222"])
		.assert().success()
		.get_output().stdout.clone();
	
	assert_ne!(result1, result2, "Different seeds should produce different results");
}

// Functional Correctness Tests
#[test]
fn test_column_selection_accuracy() {
	let (_td, input, _, _, _) = setup();
	let temp_dir = tempdir().unwrap();
	let output = temp_dir.path().join("selected.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["select", "-i", input.to_str().unwrap(), "-c", "id,name", 
			   "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	
	// Should contain id and name but not value or category
	assert!(content.contains("\"id\""), "Output should contain id column");
	assert!(content.contains("\"name\""), "Output should contain name column");
	assert!(!content.contains("\"value\""), "Output should not contain value column");
	assert!(!content.contains("\"category\""), "Output should not contain category column");
}

#[test]
fn test_filter_condition_accuracy() {
	let (_td, input, _, _, _) = setup();
	let temp_dir = tempdir().unwrap();
	let output = temp_dir.path().join("filtered.json");
	
	// Filter for id > 3
	Command::cargo_bin("nail").unwrap()
		.args(["filter", "-i", input.to_str().unwrap(), "-c", "id>3", 
			   "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let rows: Vec<Value> = serde_json::from_str(&content).unwrap();
	
	// Should only contain rows with id 4 and 5
	assert_eq!(rows.len(), 2, "Filter should return exactly 2 rows");
	
	for row in &rows {
		let id = row["id"].as_i64().unwrap();
		assert!(id > 3, "All filtered rows should have id > 3, found id: {}", id);
	}
}

#[test]
fn test_multiple_filter_conditions() {
	let (_td, input, _, _, _) = setup();
	let temp_dir = tempdir().unwrap();
	let output = temp_dir.path().join("multi_filtered.json");
	
	// Filter for id > 2 AND category = A
	Command::cargo_bin("nail").unwrap()
		.args(["filter", "-i", input.to_str().unwrap(), "-c", "id>2,category=A", 
			   "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let rows: Vec<Value> = serde_json::from_str(&content).unwrap();
	
	// Should contain Charlie (id=3, cat=A) and Eve (id=5, cat=A)
	assert_eq!(rows.len(), 2, "Multi-condition filter should return exactly 2 rows");
	
	for row in &rows {
		let id = row["id"].as_i64().unwrap();
		let category = row["category"].as_str().unwrap();
		assert!(id > 2, "All rows should have id > 2");
		assert_eq!(category, "A", "All rows should have category = A");
	}
}

#[tokio::test]
async fn test_split_ratio_accuracy() {
	let (td, input, _, _, _) = setup();
	let output_dir = td.path().join("splits");
	std::fs::create_dir_all(&output_dir).unwrap();
	
	Command::cargo_bin("nail").unwrap()
		.args(["split", "-i", input.to_str().unwrap(), "--ratio", "0.6,0.4", 
			   "--output-dir", output_dir.to_str().unwrap()])
		.assert().success();
	
	let split1 = output_dir.join("split_1.parquet");
	let split2 = output_dir.join("split_2.parquet");
	
	assert!(split1.exists(), "Split 1 file should exist");
	assert!(split2.exists(), "Split 2 file should exist");
	
	// Verify split ratios
	let ctx = SessionContext::new();
	let df1 = ctx.read_parquet(split1.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap();
	let df2 = ctx.read_parquet(split2.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap();
	
	let count1 = df1.count().await.unwrap();
	let count2 = df2.count().await.unwrap();
	let total = count1 + count2;
	
	// Original has 5 rows, so 60% should be 3 rows, 40% should be 2 rows
	assert_eq!(total, 5, "Total split rows should equal original");
	assert_eq!(count1, 3, "First split should have 3 rows (60% of 5)");
	assert_eq!(count2, 2, "Second split should have 2 rows (40% of 5)");
}

#[tokio::test]
async fn test_split_three_way_accuracy() {
	let (td, input, _, _, _) = setup();
	let output_dir = td.path().join("splits");
	std::fs::create_dir_all(&output_dir).unwrap();
	
	Command::cargo_bin("nail").unwrap()
		.args(["split", "-i", input.to_str().unwrap(), "--ratio", "0.4,0.4,0.2",
			   "--names", "train,val,test", "--output-dir", output_dir.to_str().unwrap()])
		.assert().success();
	
	let train_file = output_dir.join("train.parquet");
	let val_file = output_dir.join("val.parquet");
	let test_file = output_dir.join("test.parquet");
	
	assert!(train_file.exists(), "Train file should exist");
	assert!(val_file.exists(), "Validation file should exist");
	assert!(test_file.exists(), "Test file should exist");
	
	let ctx = SessionContext::new();
	let train_count = ctx.read_parquet(train_file.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap().count().await.unwrap();
	let val_count = ctx.read_parquet(val_file.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap().count().await.unwrap();
	let test_count = ctx.read_parquet(test_file.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap().count().await.unwrap();
	
	assert_eq!(train_count + val_count + test_count, 5, "Total should equal original");
}

#[test]
fn test_count_accuracy() {
	let (_td, input, _, _, _) = setup();
	
	let result = Command::cargo_bin("nail").unwrap()
		.args(["count", "-i", input.to_str().unwrap()])
		.assert().success()
		.get_output().stdout.clone();
	
	let count_str = String::from_utf8(result).unwrap().trim().to_string();
	let count: i32 = count_str.parse().unwrap();
	
	// Test data has 5 rows
	assert_eq!(count, 5, "Count should return exactly 5 rows");
}

#[test]
fn test_search_functionality_accuracy() {
	let (_td, input, _, _, _) = setup();
	let temp_dir = tempdir().unwrap();
	let output = temp_dir.path().join("search_results.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["search", "-i", input.to_str().unwrap(), "--value", "Alice", 
			   "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let results: Vec<Value> = serde_json::from_str(&content).unwrap();
	
	// Should find exactly one row containing "Alice"
	assert_eq!(results.len(), 1, "Search should find exactly 1 row containing 'Alice'");
	assert_eq!(results[0]["name"].as_str().unwrap(), "Alice", "Found row should have name 'Alice'");
	assert_eq!(results[0]["id"].as_i64().unwrap(), 1, "Alice should have id 1");
}

#[test]
fn test_search_case_insensitive() {
	let (_td, input, _, _, _) = setup();
	let temp_dir = tempdir().unwrap();
	let output = temp_dir.path().join("search_results.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["search", "-i", input.to_str().unwrap(), "--value", "alice", "--ignore-case",
			   "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let results: Vec<Value> = serde_json::from_str(&content).unwrap();
	
	assert_eq!(results.len(), 1, "Case-insensitive search should find Alice");
	assert_eq!(results[0]["name"].as_str().unwrap(), "Alice", "Should find Alice with lowercase search");
}

#[test]
fn test_search_partial_match() {
	let (_td, input, _, _, _) = setup();
	let temp_dir = tempdir().unwrap();
	let output = temp_dir.path().join("search_results.json");
	
	Command::cargo_bin("nail").unwrap()
		.args(["search", "-i", input.to_str().unwrap(), "--value", "li", "-c", "name",
			   "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	let results: Vec<Value> = serde_json::from_str(&content).unwrap();
	
	// Should find Alice and Charlie (both contain "li")
	assert_eq!(results.len(), 2, "Partial search should find 2 names containing 'li'");
}

#[test]
fn test_regex_column_selection() {
	let (_td, input, _, _, _) = setup();
	let temp_dir = tempdir().unwrap();
	let output = temp_dir.path().join("regex_selected.json");
	
	// Select columns matching pattern "^(id|name)$"
	Command::cargo_bin("nail").unwrap()
		.args(["select", "-i", input.to_str().unwrap(), "-c", "^(id|name)$",
			   "-o", output.to_str().unwrap(), "-f", "json"])
		.assert().success();
	
	let content = fs::read_to_string(&output).unwrap();
	
	assert!(content.contains("\"id\""), "Should contain id column");
	assert!(content.contains("\"name\""), "Should contain name column");
	assert!(!content.contains("\"value\""), "Should not contain value column");
	assert!(!content.contains("\"category\""), "Should not contain category column");
}

#[test]
fn test_headers_regex_filtering() {
	let (_td, input, _, _, _) = setup();
	
	let result = Command::cargo_bin("nail").unwrap()
		.args(["headers", "-i", input.to_str().unwrap(), "--filter", "^(id|name)$"])
		.assert().success()
		.get_output().stdout.clone();
	
	let headers = String::from_utf8(result).unwrap();
	let lines: Vec<&str> = headers.trim().split('\n').collect();
	
	assert_eq!(lines.len(), 2, "Should have exactly 2 matching headers");
	assert!(lines.contains(&"id"), "Should contain id header");
	assert!(lines.contains(&"name"), "Should contain name header");
}

#[tokio::test]
async fn test_append_schema_alignment() {
	let (td, input1, input2, _, _) = setup();
	let output = td.path().join("appended.parquet");
	
	// Append with schema differences (should work with --ignore-schema)
	Command::cargo_bin("nail").unwrap()
		.args(["append", "-i", input1.to_str().unwrap(), "--files", input2.to_str().unwrap(),
			   "--ignore-schema", "-o", output.to_str().unwrap()])
		.assert().success();
	
	let ctx = SessionContext::new();
	let df = ctx.read_parquet(output.to_str().unwrap(), ParquetReadOptions::default()).await.unwrap();
	let count = df.count().await.unwrap();
	
	// Should have rows from both files
	assert!(count > 5, "Appended dataset should have more than 5 rows");
}

#[test]
fn test_parallel_jobs_parameter() {
	let (_td, input, _, _, _) = setup();
	
	// Test with different job counts
	Command::cargo_bin("nail").unwrap()
		.args(["stats", "-i", input.to_str().unwrap(), "--jobs", "1"])
		.assert().success();
	
	Command::cargo_bin("nail").unwrap()
		.args(["stats", "-i", input.to_str().unwrap(), "--jobs", "4"])
		.assert().success();
}

#[test]
fn test_verbose_output_provides_information() {
	let (_td, input, _, _, _) = setup();
	
	let result = Command::cargo_bin("nail").unwrap()
		.args(["head", "-i", input.to_str().unwrap(), "--verbose"])
		.assert().success();
	
	let stderr = String::from_utf8(result.get_output().stderr.clone()).unwrap();
	assert!(stderr.contains("Reading") || stderr.contains("Displaying"), 
		"Verbose output should provide informational messages");
}


// File: tests/fixtures/mod.rs
// Base: mod

use std::path::Path;

pub fn setup_test_fixtures() {
	let fixtures_dir = Path::new("tests/fixtures");
	std::fs::create_dir_all(fixtures_dir).unwrap();
	
	let sample_parquet = fixtures_dir.join("sample.parquet");
	if !sample_parquet.exists() {
		crate::common::create_sample_parquet(&sample_parquet).unwrap();
	}
}

