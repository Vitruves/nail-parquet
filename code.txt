use clap::Parser;

mod cli;
mod commands;
mod error;
mod utils;

use cli::Cli;
use error::NailResult;
pub use crate::commands::select::{select_columns_by_pattern, parse_row_specification};


#[tokio::main]
async fn main() -> NailResult<()> {
	let cli = Cli::parse();
	
	match cli.command {
		commands::Commands::Head(args) => commands::head::execute(args).await,
		commands::Commands::Tail(args) => commands::tail::execute(args).await,
		commands::Commands::Preview(args) => commands::preview::execute(args).await,
		commands::Commands::Headers(args) => commands::headers::execute(args).await,
		commands::Commands::Stats(args) => commands::stats::execute(args).await,
		commands::Commands::Correlations(args) => commands::correlations::execute(args).await,
		commands::Commands::Select(args) => commands::select::execute(args).await,
		commands::Commands::Drop(args) => commands::drop::execute(args).await,
		commands::Commands::Fill(args) => commands::fill::execute(args).await,
		commands::Commands::Filter(args) => commands::filter::execute(args).await,
		commands::Commands::Id(args) => commands::id::execute(args).await,
		commands::Commands::Merge(args) => commands::merge::execute(args).await,
		commands::Commands::Append(args) => commands::append::execute(args).await,
		commands::Commands::Schema(args) => commands::schema::execute(args).await,
		commands::Commands::Sample(args) => commands::sample::execute(args).await,
		commands::Commands::Convert(args) => commands::convert::execute(args).await,
		commands::Commands::Shuffle(args) => commands::shuffle::execute(args).await,
	}
}

use thiserror::Error;

pub type NailResult<T> = Result<T, NailError>;

#[derive(Error, Debug)]
pub enum NailError {
	#[error("IO error: {0}")]
	Io(#[from] std::io::Error),
	
	#[error("DataFusion error: {0}")]
	DataFusion(#[from] datafusion::error::DataFusionError),
	
	#[error("Arrow error: {0}")]
	Arrow(#[from] arrow::error::ArrowError),
	
	#[error("Parquet error: {0}")]
	Parquet(#[from] parquet::errors::ParquetError),
	
	#[error("Regex error: {0}")]
	Regex(#[from] regex::Error),
	
	#[error("Serde JSON error: {0}")]
	SerdeJson(#[from] serde_json::Error),
	
	#[error("Invalid argument: {0}")]
	InvalidArgument(String),
	
	#[error("File not found: {0}")]
	FileNotFound(String),
	
	#[error("Unsupported format: {0}")]
	UnsupportedFormat(String),
	
	#[error("Column not found: {0}")]
	ColumnNotFound(String),
	
	#[error("Statistics error: {0}")]
	Statistics(String),
}

use clap::Args;

use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::read_data;
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct HeadArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Number of rows to display", default_value = "5")]
	pub number: usize,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: HeadArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let limited_df = df.limit(0, Some(args.number))?;
	
	if args.verbose {
		eprintln!("Displaying first {} rows", args.number);
	}
	
	display_dataframe(&limited_df, args.output.as_deref(), args.format.as_ref()).await?;
	
	Ok(())
}

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use rand::seq::SliceRandom;
use rand::{rngs::StdRng, SeedableRng};
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;
use datafusion::arrow::array::{StringArray, DictionaryArray, Array};
use datafusion::arrow::datatypes::UInt32Type;

#[derive(Args, Clone)]
pub struct SampleArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Number of samples", default_value = "10")]
	pub number: usize,
	
	#[arg(long, help = "Sampling method", value_enum, default_value = "random")]
	pub method: SampleMethod,
	
	#[arg(long, help = "Column name for stratified sampling")]
	pub stratify_by: Option<String>,
	
	#[arg(short, long, help = "Random seed for reproducible results")]
	pub random: Option<u64>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

#[derive(clap::ValueEnum, Clone, Debug)]
pub enum SampleMethod {
	Random,
	Stratified,
	First,
	Last,
}

pub async fn execute(args: SampleArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let total_rows = df.clone().count().await?;
	
	if args.number >= total_rows {
		if args.verbose {
			eprintln!("Requested {} samples, but only {} rows available. Returning all rows.", args.number, total_rows);
		}
		display_dataframe(&df, args.output.as_deref(), args.format.as_ref()).await?;
		return Ok(());
	}
	
	if args.verbose {
		eprintln!("Sampling {} rows from {} total using {:?} method", args.number, total_rows, args.method);
	}
	
	let sampled_df = match args.method {
		SampleMethod::Random => sample_random(&df, args.number, args.random).await?,
		SampleMethod::Stratified => {
			if let Some(col) = &args.stratify_by {
				sample_stratified(&df, args.number, col, args.random).await?
			} else {
				return Err(NailError::InvalidArgument("--stratify-by required for stratified sampling".to_string()));
			}
		},
		SampleMethod::First => df.limit(0, Some(args.number))?,
		SampleMethod::Last => {
			let skip = total_rows.saturating_sub(args.number);
			df.limit(skip, Some(args.number))?
		},
	};
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&sampled_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&sampled_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

async fn sample_random(df: &DataFrame, n: usize, seed: Option<u64>) -> NailResult<DataFrame> {
	let total_rows = df.clone().count().await?;
	let mut rng = match seed {
		Some(s) => StdRng::seed_from_u64(s),
		None => StdRng::from_entropy(),
	};
	
	let mut indices: Vec<usize> = (0..total_rows).collect();
	indices.shuffle(&mut rng);
	indices.truncate(n);
	indices.sort();
	
	let ctx = crate::utils::create_context().await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let indices_str = indices.iter()
		.map(|&i| (i + 1).to_string())
		.collect::<Vec<_>>()
		.join(",");
	
	// Get the original column names and quote them to preserve case
	let original_columns: Vec<String> = df.schema().fields().iter()
		.map(|f| format!("\"{}\"", f.name()))
		.collect();
	
	let sql = format!(
		"SELECT {} FROM (SELECT {}, ROW_NUMBER() OVER() as rn FROM {}) WHERE rn IN ({})",
		original_columns.join(", "),
		original_columns.join(", "),
		table_name, 
		indices_str
	);
	
	let result = ctx.sql(&sql).await?;
	
	Ok(result)
}

async fn sample_stratified(
    df: &DataFrame,
    n: usize,
    stratify_col: &str,
    _seed: Option<u64>,
) -> NailResult<DataFrame> {
    use std::collections::HashSet;
    let ctx = crate::utils::create_context().await?;
    let table_name = "temp_table";
    ctx.register_table(table_name, df.clone().into_view())?;

    // Find the actual column name (case-insensitive matching)
    let schema = df.schema();
    let actual_col_name = schema.fields().iter()
        .find(|f| f.name().to_lowercase() == stratify_col.to_lowercase())
        .map(|f| f.name().clone())
        .ok_or_else(|| {
            let available_cols: Vec<String> = schema.fields().iter()
                .map(|f| f.name().clone())
                .collect();
            NailError::ColumnNotFound(format!(
                "Column '{}' not found. Available columns: {:?}", 
                stratify_col, available_cols
            ))
        })?;

    // First, let's try to get distinct values using SQL which is more robust
    let distinct_sql = format!(
        "SELECT DISTINCT {} FROM {} WHERE {} IS NOT NULL",
        actual_col_name, table_name, actual_col_name
    );
    
    let distinct_df = match ctx.sql(&distinct_sql).await {
        Ok(df) => df,
        Err(e) => {
            return Err(NailError::Statistics(format!("Failed to retrieve categories from column '{}': {}", actual_col_name, e)));
        }
    };
    
    let distinct_batches = distinct_df.clone().collect().await?;
    let mut categories = HashSet::new();
    
    for batch in &distinct_batches {
        if batch.num_columns() > 0 {
            let array_ref = batch.column(0);
        if let Some(arr) = array_ref.as_any().downcast_ref::<StringArray>() {
            for i in 0..arr.len() {
                if arr.is_valid(i) {
                        categories.insert(arr.value(i).to_string());
                }
            }
        } else if let Some(dict) = array_ref.as_any().downcast_ref::<DictionaryArray<UInt32Type>>() {
            let keys = dict.keys();
            if let Some(values) = dict.values().as_any().downcast_ref::<StringArray>() {
                for i in 0..keys.len() {
                    if keys.is_valid(i) {
                        let k = keys.value(i) as usize;
                            if k < values.len() {
                                categories.insert(values.value(k).to_string());
                            }
                    }
                }
            }
        } else {
                // Try to convert to string representation
                let schema = distinct_df.schema();
                if let Some(field) = schema.fields().get(0) {
                    match field.data_type() {
                        datafusion::arrow::datatypes::DataType::Utf8 => {
                            // Already handled above, but this is a fallback
                            for i in 0..array_ref.len() {
                                if !array_ref.is_null(i) {
                                    if let Some(scalar) = datafusion::arrow::compute::cast(array_ref, &datafusion::arrow::datatypes::DataType::Utf8).ok() {
                                        if let Some(str_arr) = scalar.as_any().downcast_ref::<StringArray>() {
                                            if i < str_arr.len() && str_arr.is_valid(i) {
                                                categories.insert(str_arr.value(i).to_string());
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        _ => {
                            return Err(NailError::Statistics(format!("Column '{}' must be of string type for stratified sampling", actual_col_name)));
                        }
                    }
                }
            }
        }
    }
    
    if categories.is_empty() {
        return Err(NailError::Statistics("No categories found for stratified sampling".to_string()));
    }
    
    let categories: Vec<String> = categories.into_iter().collect();
    let per_group = n / categories.len();
    let mut combined: Option<DataFrame> = None;
    
    for cat in &categories {
        // deterministic: take first per_group rows for each category
        let filtered = ctx.table(table_name).await?
            .filter(col(&actual_col_name).eq(lit(cat)))?;
        let limited = filtered.limit(0, Some(per_group))?;
        combined = Some(match combined {
            None => limited,
            Some(prev) => prev.union(limited)?,
        });
    }
    
    let mut result_df = combined.unwrap();
    // Handle remainder samples
    let remainder = n - per_group * categories.len();
    if remainder > 0 {
        // add random remainder from full dataset
        let rem = sample_random(df, remainder, None).await?;
        result_df = result_df.union(rem)?;
    }
    Ok(result_df)
}

use clap::Args;
use std::path::PathBuf;
use regex::Regex;
use crate::error::{NailError, NailResult};
use crate::utils::io::read_data;

#[derive(Args, Clone)]
pub struct HeadersArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(long, help = "Filter headers with regex pattern")]
	pub filter: Option<String>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: HeadersArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading schema from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let schema = df.schema();
	let field_names: Vec<String> = schema.fields().iter()
		.map(|f| f.name().clone())
		.collect();
	
	let filtered_names = if let Some(pattern) = &args.filter {
		let regex = Regex::new(pattern)
			.map_err(|e| NailError::InvalidArgument(format!("Invalid regex pattern: {}", e)))?;
		
		field_names.into_iter()
			.filter(|name| regex.is_match(name))
			.collect()
	} else {
		field_names
	};
	
	if args.verbose {
		eprintln!("Found {} headers", filtered_names.len());
	}
	
	match &args.output {
		Some(output_path) => {
			let content = match args.format {
				Some(crate::cli::OutputFormat::Json) => {
					serde_json::to_string_pretty(&filtered_names)?
				},
				_ => filtered_names.join("\n"),
			};
			std::fs::write(output_path, content)?;
		},
		None => {
			for name in filtered_names {
				println!("{}", name);
			}
		},
	}
	
	Ok(())
}

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct FilterArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Column filter conditions (e.g., 'age>25,salary<50000')")]
	pub columns: Option<String>,
	
	#[arg(short, long, help = "Row filter type", value_enum)]
	pub rows: Option<RowFilter>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

#[derive(clap::ValueEnum, Clone, Debug)]
pub enum RowFilter {
	NoNan,
	NumericOnly,
	CharOnly,
	NoZeros,
}

pub async fn execute(args: FilterArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let mut result_df = df;
	
	if let Some(col_conditions) = &args.columns {
		if args.verbose {
			eprintln!("Applying column filters: {}", col_conditions);
		}
		result_df = apply_column_filters(&result_df, col_conditions).await?;
	}
	
	if let Some(row_filter) = &args.rows {
		if args.verbose {
			eprintln!("Applying row filter: {:?}", row_filter);
		}
		result_df = apply_row_filter(&result_df, row_filter).await?;
	}
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&result_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&result_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

async fn apply_column_filters(df: &DataFrame, conditions: &str) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let schema = df.schema().clone().into();
	let mut filter_conditions = Vec::new();
	
	for condition in conditions.split(',') {
		let condition = condition.trim();
		let filter_expr = parse_condition_with_schema(condition, &schema).await?;
		filter_conditions.push(filter_expr);
	}
	
	let combined_filter = filter_conditions.into_iter()
		.reduce(|acc, expr| acc.and(expr))
		.unwrap();
	
	let result = ctx.table(table_name).await?.filter(combined_filter)?;
	Ok(result)
}

async fn parse_condition_with_schema(condition: &str, schema: &datafusion::common::DFSchemaRef) -> NailResult<Expr> {
	let operators = [">=", "<=", "!=", "=", ">", "<"];
	
	for op in &operators {
		if let Some(pos) = condition.find(op) {
			let column_name_input = condition[..pos].trim();
			let value_str = condition[pos + op.len()..].trim();
			
			// Find the actual column name (case-insensitive matching)
			let actual_column_name = schema.fields().iter()
				.find(|f| f.name().to_lowercase() == column_name_input.to_lowercase())
				.map(|f| f.name().clone())
				.ok_or_else(|| {
					let available_cols: Vec<String> = schema.fields().iter()
						.map(|f| f.name().clone())
						.collect();
					NailError::ColumnNotFound(format!(
						"Column '{}' not found. Available columns: {:?}", 
						column_name_input, available_cols
					))
				})?;
			
			let value_expr = if let Ok(int_val) = value_str.parse::<i64>() {
				lit(int_val)
			} else if let Ok(float_val) = value_str.parse::<f64>() {
				lit(float_val)
			} else {
				lit(value_str)
			};
			
			let column_expr = col(&actual_column_name);
			
			return Ok(match *op {
				"=" => column_expr.eq(value_expr),
				"!=" => column_expr.not_eq(value_expr),
				">" => column_expr.gt(value_expr),
				">=" => column_expr.gt_eq(value_expr),
				"<" => column_expr.lt(value_expr),
				"<=" => column_expr.lt_eq(value_expr),
				_ => unreachable!(),
			});
		}
	}
	
	Err(NailError::InvalidArgument(format!("Invalid condition: {}", condition)))
}

async fn apply_row_filter(df: &DataFrame, filter: &RowFilter) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let schema = df.schema();
	let filter_expr = match filter {
		RowFilter::NoNan => {
			let conditions: Vec<Expr> = schema.fields().iter()
				.map(|f| col(f.name()).is_not_null())
				.collect();
			conditions.into_iter().reduce(|acc, expr| acc.and(expr)).unwrap()
		},
		RowFilter::NumericOnly => {
			let numeric_columns: Vec<String> = schema.fields().iter()
				.filter(|f| matches!(f.data_type(), 
					datafusion::arrow::datatypes::DataType::Int64 | 
					datafusion::arrow::datatypes::DataType::Float64 | 
					datafusion::arrow::datatypes::DataType::Int32 | 
					datafusion::arrow::datatypes::DataType::Float32
				))
				.map(|f| f.name().clone())
				.collect();
			
			if numeric_columns.is_empty() {
				return Err(NailError::InvalidArgument("No numeric columns found".to_string()));
			}
			
			return Ok(df.clone().select(numeric_columns.iter().map(|name| col(name)).collect())?);
		},
		RowFilter::CharOnly => {
			let char_columns: Vec<String> = schema.fields().iter()
				.filter(|f| matches!(f.data_type(), datafusion::arrow::datatypes::DataType::Utf8))
				.map(|f| f.name().clone())
				.collect();
			
			if char_columns.is_empty() {
				return Err(NailError::InvalidArgument("No string columns found".to_string()));
			}
			
			return Ok(df.clone().select(char_columns.iter().map(|name| col(name)).collect())?);
		},
		RowFilter::NoZeros => {
			let conditions: Vec<Expr> = schema.fields().iter()
				.filter_map(|f| {
					match f.data_type() {
						datafusion::arrow::datatypes::DataType::Int64 | 
						datafusion::arrow::datatypes::DataType::Int32 => {
							Some(col(f.name()).not_eq(lit(0)))
						},
						datafusion::arrow::datatypes::DataType::Float64 | 
						datafusion::arrow::datatypes::DataType::Float32 => {
							Some(col(f.name()).not_eq(lit(0.0)))
						},
						_ => None,
					}
				})
				.collect();
			
			if conditions.is_empty() {
				return Ok(df.clone());
			}
			
			conditions.into_iter().reduce(|acc, expr| acc.and(expr)).unwrap()
		},
	};
	
	let result = ctx.table(table_name).await?.filter(filter_expr)?;
	Ok(result)
}

use clap::Args;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::read_data;
use crate::utils::format::display_dataframe;
use crate::utils::stats::{calculate_basic_stats, calculate_exhaustive_stats, calculate_hypothesis_tests, select_columns_by_pattern};

#[derive(Args, Clone)]
pub struct StatsArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Comma-separated column names or regex patterns")]
	pub columns: Option<String>,
	
	#[arg(short = 't', long, help = "Statistics type", value_enum, default_value = "basic")]
	pub stats_type: StatsType,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

#[derive(clap::ValueEnum, Clone, Debug)]
pub enum StatsType {
	Basic,
	Exhaustive,
	Hypothesis,
}

pub async fn execute(args: StatsArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let schema = df.schema();
	
	let target_columns = if let Some(col_spec) = &args.columns {
		select_columns_by_pattern(schema.clone().into(), col_spec)?
	} else {
		schema.fields().iter().map(|f| f.name().clone()).collect()
	};
	
	if args.verbose {
		eprintln!("Computing {:?} statistics for {} columns", args.stats_type, target_columns.len());
	}
	
	let stats_df = match args.stats_type {
		StatsType::Basic => calculate_basic_stats(&df, &target_columns).await?,
		StatsType::Exhaustive => calculate_exhaustive_stats(&df, &target_columns).await?,
		StatsType::Hypothesis => calculate_hypothesis_tests(&df, &target_columns).await?,
	};
	
	display_dataframe(&stats_df, args.output.as_deref(), args.format.as_ref()).await?;
	
	// Print overall row count for basic stats when outputting to console
	if args.output.is_none() && args.format.is_none() {
		let total_rows = read_data(&args.input).await?.clone().count().await?;
		println!("count | {}", total_rows);
	}
	
	Ok(())
}

pub mod cli;
pub mod commands;
pub mod error;
pub mod utils;

pub use error::{NailError, NailResult};

use datafusion::prelude::*;
use datafusion::common::DFSchemaRef;
use regex::Regex;
use crate::error::{NailError, NailResult};

#[derive(clap::ValueEnum, Clone, Debug, PartialEq)]
pub enum CorrelationType {
	Pearson,
	Kendall,
	Spearman,
}

pub fn select_columns_by_pattern(schema: DFSchemaRef, pattern: &str) -> NailResult<Vec<String>> {
	let patterns: Vec<&str> = pattern.split(',').map(|s| s.trim()).collect();
	let mut selected = Vec::new();
	let mut not_found = Vec::new();
	
	for pattern in &patterns {
		let mut found = false;
		
		// First try exact match (case-sensitive)
		for field in schema.fields() {
			let field_name = field.name();
			
			if pattern.contains('*') || pattern.contains('^') || pattern.contains('$') {
				let regex = Regex::new(pattern)?;
				if regex.is_match(field_name) {
					selected.push(field_name.clone());
					found = true;
				}
			} else if field_name == *pattern {
				selected.push(field_name.clone());
				found = true;
				break;
			}
		}
		
		// If not found, try case-insensitive match
		if !found {
			for field in schema.fields() {
				let field_name = field.name();
				
				if pattern.contains('*') || pattern.contains('^') || pattern.contains('$') {
					// For regex patterns, create case-insensitive version
					let case_insensitive_pattern = format!("(?i){}", pattern);
					if let Ok(regex) = Regex::new(&case_insensitive_pattern) {
						if regex.is_match(field_name) {
							selected.push(field_name.clone());
							found = true;
						}
					}
				} else if field_name.to_lowercase() == pattern.to_lowercase() {
					selected.push(field_name.clone());
					found = true;
					break;
				}
			}
		}
		
		if !found {
			not_found.push(*pattern);
		}
	}
	
	if !not_found.is_empty() {
		let available_columns: Vec<String> = schema.fields().iter()
			.map(|f| f.name().clone())
			.collect();
		return Err(NailError::ColumnNotFound(format!(
			"Columns not found: {:?}. Available columns: {:?}", 
			not_found, available_columns
		)));
	}
	
	// Remove duplicates while preserving order
	let mut unique_selected = Vec::new();
	for col in selected {
		if !unique_selected.contains(&col) {
			unique_selected.push(col);
		}
	}
	
	if unique_selected.is_empty() {
		return Err(NailError::ColumnNotFound(format!("No columns matched pattern: {}", pattern)));
	}
	
	Ok(unique_selected)
}

pub async fn calculate_basic_stats(df: &DataFrame, columns: &[String]) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let mut stats_rows = Vec::new();
	
	for column in columns {
		let field = df.schema().field_with_name(None, column)
			.map_err(|_| NailError::ColumnNotFound(column.clone()))?;
		
		match field.data_type() {
			datafusion::arrow::datatypes::DataType::Int64 | 
			datafusion::arrow::datatypes::DataType::Float64 | 
			datafusion::arrow::datatypes::DataType::Int32 | 
			datafusion::arrow::datatypes::DataType::Float32 => {
				let stats_sql = format!(
					"SELECT 
						'{}' as column_name,
						COUNT({}) as count,
						AVG({}) as mean,
						APPROX_PERCENTILE_CONT({}, 0.25) as q25,
						APPROX_PERCENTILE_CONT({}, 0.5) as q50,
						APPROX_PERCENTILE_CONT({}, 0.75) as q75,
						COUNT(DISTINCT {}) as num_classes
					FROM {}",
					column, column, column, column, column, column, column, table_name
				);
				
				let stats_df = ctx.sql(&stats_sql).await?;
				stats_rows.push(stats_df);
			},
			datafusion::arrow::datatypes::DataType::Utf8 => {
				let stats_sql = format!(
					"SELECT 
						'{}' as column_name,
						COUNT({}) as count,
						NULL as mean,
						NULL as q25,
						NULL as q50,
						NULL as q75,
						COUNT(DISTINCT {}) as num_classes
					FROM {}",
					column, column, column, table_name
				);
				
				let stats_df = ctx.sql(&stats_sql).await?;
				stats_rows.push(stats_df);
			},
			_ => continue,
		}
	}
	
	if stats_rows.is_empty() {
		return Err(NailError::Statistics("No suitable columns for statistics".to_string()));
	}
	
	let mut iter = stats_rows.into_iter();
	let mut combined = iter.next().unwrap();
	for df in iter {
		combined = combined.union(df)?;
	}
	
	Ok(combined)
}

pub async fn calculate_exhaustive_stats(df: &DataFrame, columns: &[String]) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let mut stats_rows = Vec::new();
	
	for column in columns {
		let field = df.schema().field_with_name(None, column)
			.map_err(|_| NailError::ColumnNotFound(column.clone()))?;
		
		match field.data_type() {
			datafusion::arrow::datatypes::DataType::Int64 | 
			datafusion::arrow::datatypes::DataType::Float64 | 
			datafusion::arrow::datatypes::DataType::Int32 | 
			datafusion::arrow::datatypes::DataType::Float32 => {
				let stats_sql = format!(
					"SELECT 
						'{}' as column_name,
						COUNT({}) as count,
						AVG({}) as mean,
						STDDEV({}) as std_dev,
						MIN({}) as min_val,
						APPROX_PERCENTILE_CONT({}, 0.25) as q25,
						APPROX_PERCENTILE_CONT({}, 0.5) as median,
						APPROX_PERCENTILE_CONT({}, 0.75) as q75,
						MAX({}) as max_val,
						VAR_POP({}) as variance,
						COUNT(DISTINCT {}) as num_classes,
						(COUNT({}) - COUNT(DISTINCT {})) as duplicates
					FROM {}",
					column, column, column, column, column, column, column, column, 
					column, column, column, column, column, table_name
				);
				
				let stats_df = ctx.sql(&stats_sql).await?;
				stats_rows.push(stats_df);
			},
			datafusion::arrow::datatypes::DataType::Utf8 => {
				let stats_sql = format!(
					"SELECT 
						'{}' as column_name,
						COUNT({}) as count,
						NULL as mean,
						NULL as std_dev,
						NULL as min_val,
						NULL as q25,
						NULL as median,
						NULL as q75,
						NULL as max_val,
						NULL as variance,
						COUNT(DISTINCT {}) as num_classes,
						(COUNT({}) - COUNT(DISTINCT {})) as duplicates
					FROM {}",
					column, column, column, column, column, table_name
				);
				
				let stats_df = ctx.sql(&stats_sql).await?;
				stats_rows.push(stats_df);
			},
			_ => continue,
		}
	}
	
	if stats_rows.is_empty() {
		return Err(NailError::Statistics("No suitable columns for statistics".to_string()));
	}
	
	let mut iter = stats_rows.into_iter();
	let mut combined = iter.next().unwrap();
	for df in iter {
		combined = combined.union(df)?;
	}
	
	Ok(combined)
}

pub async fn calculate_hypothesis_tests(_df: &DataFrame, _columns: &[String]) -> NailResult<DataFrame> {
	Err(NailError::Statistics("Hypothesis tests not yet implemented".to_string()))
}

pub async fn calculate_correlations(
	df: &DataFrame,
	columns: &[String],
	correlation_type: &CorrelationType,
	matrix_format: bool,
	_include_tests: bool,
) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	if matrix_format {
		calculate_correlation_matrix(ctx, table_name, columns, correlation_type).await
	} else {
		calculate_correlation_pairs(ctx, table_name, columns, correlation_type).await
	}
}

async fn calculate_correlation_matrix(
	ctx: SessionContext,
	table_name: &str,
	columns: &[String],
	_correlation_type: &CorrelationType,
) -> NailResult<DataFrame> {
	let mut correlation_queries = Vec::new();
	
	for col1 in columns {
		let mut row_values = Vec::new();
		row_values.push(format!("'{}' as variable", col1));
		
		for col2 in columns {
			if col1 == col2 {
				row_values.push(format!("1.0 as corr_with_{}", col2.replace(".", "_")));
			} else {
				row_values.push(format!(
					"(SELECT CORR({}, {}) FROM {}) as corr_with_{}",
					col1, col2, table_name, col2.replace(".", "_")
				));
			}
		}
		
		let row_sql = format!("SELECT {}", row_values.join(", "));
		correlation_queries.push(row_sql);
	}
	
	if correlation_queries.is_empty() {
		return Err(NailError::Statistics("No columns for correlation".to_string()));
	}
	
	let mut combined = ctx.sql(&correlation_queries[0]).await?;
	for query in correlation_queries.into_iter().skip(1) {
		let df = ctx.sql(&query).await?;
		combined = combined.union(df)?;
	}
	
	Ok(combined)
}

async fn calculate_correlation_pairs(
	ctx: SessionContext,
	table_name: &str,
	columns: &[String],
	_correlation_type: &CorrelationType,
) -> NailResult<DataFrame> {
	let mut pair_queries = Vec::new();
	
	for (i, col1) in columns.iter().enumerate() {
		for col2 in columns.iter().skip(i + 1) {
			let pair_sql = format!(
				"SELECT '{}' as column1, '{}' as column2, CORR({}, {}) as correlation FROM {}",
				col1, col2, col1, col2, table_name
			);
			pair_queries.push(pair_sql);
		}
	}
	
	if pair_queries.is_empty() {
		return Err(NailError::Statistics("Need at least 2 columns for correlation".to_string()));
	}
	
	let mut combined = ctx.sql(&pair_queries[0]).await?;
	for query in pair_queries.into_iter().skip(1) {
		let df = ctx.sql(&query).await?;
		combined = combined.union(df)?;
	}
	
	Ok(combined)
}

use datafusion::prelude::*;
use datafusion::arrow::array::*;
use datafusion::arrow::datatypes::DataType;
use std::path::Path;
use std::io::{self, Write};
use crate::error::NailResult;
use crate::cli::OutputFormat;
use crate::utils::io::write_data;
use crate::utils::FileFormat;

// ANSI color codes
const RESET: &str = "\x1b[0m";
const BOLD: &str = "\x1b[1m";
const DIM: &str = "\x1b[2m";
const HEADER_COLOR: &str = "\x1b[1;36m"; // Bold cyan
const NUMERIC_COLOR: &str = "\x1b[33m";  // Yellow
const STRING_COLOR: &str = "\x1b[32m";   // Green
const NULL_COLOR: &str = "\x1b[2;37m";   // Dim white
const BORDER_COLOR: &str = "\x1b[2;90m"; // Dim gray

pub async fn display_dataframe(
	df: &DataFrame,
	output_path: Option<&Path>,
	format: Option<&OutputFormat>,
) -> NailResult<()> {
	match output_path {
		Some(path) => {
			let file_format = match format {
				Some(OutputFormat::Json) => Some(FileFormat::Json),
				Some(OutputFormat::Csv) => Some(FileFormat::Csv),
				Some(OutputFormat::Parquet) => Some(FileFormat::Parquet),
				Some(OutputFormat::Text) | None => {
					match path.extension().and_then(|s| s.to_str()) {
						Some("json") => Some(FileFormat::Json),
						Some("csv") => Some(FileFormat::Csv),
						Some("parquet") => Some(FileFormat::Parquet),
						_ => Some(FileFormat::Parquet),
					}
				},
			};
			
			write_data(df, path, file_format.as_ref()).await
		},
		None => {
			match format {
				Some(OutputFormat::Json) => {
					display_as_json(df).await?;
				},
				Some(OutputFormat::Text) | None => {
					display_as_table(df).await?;
				},
				_ => {
					return Err(crate::error::NailError::InvalidArgument(
						"CSV and Parquet formats require an output file".to_string()
					));
				},
			}
			
			Ok(())
		},
	}
}

async fn display_as_json(df: &DataFrame) -> NailResult<()> {
	let batches = df.clone().collect().await?;
	let schema = df.schema();
	
	println!("[");
	let mut first_record = true;
	
	for batch in &batches {
		for row_idx in 0..batch.num_rows() {
			if !first_record {
				println!(",");
			}
			first_record = false;
			
			print!("  {{");
			let mut first_field = true;
			
			for (col_idx, field) in schema.fields().iter().enumerate() {
				if !first_field {
					print!(", ");
				}
				first_field = false;
				
				let column = batch.column(col_idx);
				let value = format_json_value(column, row_idx, field.data_type());
				print!("\"{}\": {}", field.name(), value);
			}
			print!("}}");
		}
	}
	
	println!("\n]");
	Ok(())
}

async fn display_as_table(df: &DataFrame) -> NailResult<()> {
	let batches = df.clone().collect().await?;
	let schema = df.schema();
	
	if batches.is_empty() {
		println!("{}No data to display{}", DIM, RESET);
		return Ok(());
	}
	
	// Calculate column widths
	let mut col_widths = Vec::new();
	let max_width = 50; // Maximum width per column
	let min_width = 8;  // Minimum width per column
	
	for field in schema.fields() {
		let header_width = field.name().len();
		let mut max_content_width = header_width;
		
		// Sample some rows to determine content width
		for batch in &batches {
			let col_idx = schema.fields().iter().position(|f| f.name() == field.name()).unwrap();
			let column = batch.column(col_idx);
			
			for row_idx in 0..std::cmp::min(batch.num_rows(), 100) { // Sample first 100 rows
				let value = format_cell_value(column, row_idx, field.data_type(), false);
				max_content_width = std::cmp::max(max_content_width, value.len());
			}
		}
		
		let width = std::cmp::min(max_width, std::cmp::max(min_width, max_content_width));
		col_widths.push(width);
	}
	
	// Print header
	print_table_separator(&col_widths, true);
	print_table_header(schema.fields(), &col_widths);
	print_table_separator(&col_widths, false);
	
	// Print data rows
	let mut row_count = 0;
	for batch in &batches {
		for row_idx in 0..batch.num_rows() {
			print_table_row(batch, row_idx, schema.fields(), &col_widths);
			row_count += 1;
			
			// Add separator every 20 rows for readability
			if row_count % 20 == 0 && row_count < batches.iter().map(|b| b.num_rows()).sum::<usize>() {
				print_table_separator(&col_widths, false);
			}
		}
	}
	
	print_table_separator(&col_widths, true);
	
	// Print summary
	println!("{}Rows: {}{}{}", DIM, BOLD, row_count, RESET);
	
	Ok(())
}

fn print_table_separator(col_widths: &[usize], is_border: bool) {
	print!("{}", BORDER_COLOR);
	if is_border {
		print!("┌");
		for (i, &width) in col_widths.iter().enumerate() {
			print!("{}", "─".repeat(width + 2));
			if i < col_widths.len() - 1 {
				print!("┬");
			}
		}
		println!("┐{}", RESET);
	} else {
		print!("├");
		for (i, &width) in col_widths.iter().enumerate() {
			print!("{}", "─".repeat(width + 2));
			if i < col_widths.len() - 1 {
				print!("┼");
			}
		}
		println!("┤{}", RESET);
	}
}

fn print_table_header(fields: &[datafusion::arrow::datatypes::FieldRef], col_widths: &[usize]) {
	print!("{}", BORDER_COLOR);
	print!("│{}", RESET);
	
	for (i, field) in fields.iter().enumerate() {
		let width = col_widths[i];
		let name = truncate_string(field.name(), width);
		print!(" {}{:<width$}{} {}", HEADER_COLOR, name, RESET, BORDER_COLOR, width = width);
		print!("│{}", RESET);
	}
	println!();
}

fn print_table_row(
	batch: &datafusion::arrow::record_batch::RecordBatch,
	row_idx: usize,
	fields: &[datafusion::arrow::datatypes::FieldRef],
	col_widths: &[usize],
) {
	print!("{}", BORDER_COLOR);
	print!("│{}", RESET);
	
	for (col_idx, field) in fields.iter().enumerate() {
		let width = col_widths[col_idx];
		let column = batch.column(col_idx);
		let value = format_cell_value(column, row_idx, field.data_type(), true);
		let truncated = truncate_string(&value, width);
		
		print!(" {:<width$} {}", truncated, BORDER_COLOR, width = width);
		print!("│{}", RESET);
	}
	println!();
}

fn format_cell_value(column: &dyn Array, row_idx: usize, data_type: &DataType, with_color: bool) -> String {
	if column.is_null(row_idx) {
		let value = "NULL";
		if with_color {
			format!("{}{}{}", NULL_COLOR, value, RESET)
		} else {
			value.to_string()
		}
	} else {
		let value = match data_type {
			DataType::Utf8 => {
				let array = column.as_any().downcast_ref::<StringArray>().unwrap();
				let val = array.value(row_idx);
				if with_color {
					format!("{}{}{}", STRING_COLOR, val, RESET)
				} else {
					val.to_string()
				}
			},
			DataType::Int64 => {
				let array = column.as_any().downcast_ref::<Int64Array>().unwrap();
				let val = array.value(row_idx).to_string();
				if with_color {
					format!("{}{}{}", NUMERIC_COLOR, val, RESET)
				} else {
					val
				}
			},
			DataType::Float64 => {
				let array = column.as_any().downcast_ref::<Float64Array>().unwrap();
				let val = format!("{:.2}", array.value(row_idx));
				if with_color {
					format!("{}{}{}", NUMERIC_COLOR, val, RESET)
				} else {
					val
				}
			},
			DataType::Int32 => {
				let array = column.as_any().downcast_ref::<Int32Array>().unwrap();
				let val = array.value(row_idx).to_string();
				if with_color {
					format!("{}{}{}", NUMERIC_COLOR, val, RESET)
				} else {
					val
				}
			},
			DataType::Float32 => {
				let array = column.as_any().downcast_ref::<Float32Array>().unwrap();
				let val = format!("{:.2}", array.value(row_idx));
				if with_color {
					format!("{}{}{}", NUMERIC_COLOR, val, RESET)
				} else {
					val
				}
			},
			DataType::Boolean => {
				let array = column.as_any().downcast_ref::<BooleanArray>().unwrap();
				let val = array.value(row_idx).to_string();
				if with_color {
					format!("{}{}{}", NUMERIC_COLOR, val, RESET)
				} else {
					val
				}
			},
			DataType::Date32 => {
				let array = column.as_any().downcast_ref::<Date32Array>().unwrap();
				let days_since_epoch = array.value(row_idx);
				// Convert days since epoch to a readable date
				let date = chrono::NaiveDate::from_num_days_from_ce_opt(days_since_epoch + 719163)
					.unwrap_or_else(|| chrono::NaiveDate::from_ymd_opt(1970, 1, 1).unwrap());
				let val = date.format("%Y-%m-%d").to_string();
				if with_color {
					format!("{}{}{}", STRING_COLOR, val, RESET)
				} else {
					val
				}
			},
			DataType::Date64 => {
				let array = column.as_any().downcast_ref::<Date64Array>().unwrap();
				let millis_since_epoch = array.value(row_idx);
				let datetime = chrono::DateTime::from_timestamp_millis(millis_since_epoch)
					.unwrap_or_else(|| chrono::DateTime::from_timestamp(0, 0).unwrap());
				let val = datetime.format("%Y-%m-%d").to_string();
				if with_color {
					format!("{}{}{}", STRING_COLOR, val, RESET)
				} else {
					val
				}
			},
			DataType::Timestamp(_, _) => {
				// Handle timestamp types
				let val = "timestamp"; // Simplified for now
				if with_color {
					format!("{}{}{}", STRING_COLOR, val, RESET)
				} else {
					val.to_string()
				}
			},
			_ => {
				// Fallback for other types - try to get a string representation
				let val = format!("{:?}", column.slice(row_idx, 1))
					.lines()
					.next()
					.unwrap_or("unknown")
					.trim_start_matches('[')
					.trim_end_matches(']')
					.trim_start_matches("\"")
					.trim_end_matches("\"")
					.trim()
					.to_string();
				if with_color {
					format!("{}{}{}", STRING_COLOR, val, RESET)
				} else {
					val
				}
			},
		};
		value
	}
}

fn format_json_value(column: &dyn Array, row_idx: usize, data_type: &DataType) -> String {
	if column.is_null(row_idx) {
		"null".to_string()
	} else {
		match data_type {
			DataType::Utf8 => {
				let array = column.as_any().downcast_ref::<StringArray>().unwrap();
				format!("\"{}\"", array.value(row_idx).replace("\"", "\\\""))
			},
			DataType::Int64 => {
				let array = column.as_any().downcast_ref::<Int64Array>().unwrap();
				array.value(row_idx).to_string()
			},
			DataType::Float64 => {
				let array = column.as_any().downcast_ref::<Float64Array>().unwrap();
				array.value(row_idx).to_string()
			},
			DataType::Int32 => {
				let array = column.as_any().downcast_ref::<Int32Array>().unwrap();
				array.value(row_idx).to_string()
			},
			DataType::Float32 => {
				let array = column.as_any().downcast_ref::<Float32Array>().unwrap();
				array.value(row_idx).to_string()
			},
			DataType::Boolean => {
				let array = column.as_any().downcast_ref::<BooleanArray>().unwrap();
				array.value(row_idx).to_string()
			},
			_ => {
				format!("\"{}\"", format!("{:?}", column.slice(row_idx, 1)).replace("\"", "\\\""))
			},
		}
	}
}

fn truncate_string(s: &str, max_len: usize) -> String {
	if s.len() <= max_len {
		s.to_string()
	} else {
		format!("{}…", &s[..max_len.saturating_sub(1)])
	}
}

fn main() {
	println!("cargo:rerun-if-changed=build.rs");
}

use clap::Args;
use std::path::PathBuf;
use crate::error::{NailError, NailResult};
use crate::utils::io::read_data;
use crate::utils::format::display_dataframe;
use crate::utils::stats::{calculate_correlations, CorrelationType, select_columns_by_pattern};

#[derive(Args, Clone)]
pub struct CorrelationsArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Comma-separated column names or regex patterns")]
	pub columns: Option<String>,
	
	#[arg(short = 't', long, help = "Correlation type", value_enum, default_value = "pearson")]
	pub correlation_type: CorrelationType,
	
	#[arg(long, help = "Output correlation matrix format")]
	pub correlation_matrix: bool,
	
	#[arg(long, help = "Include statistical significance tests")]
	pub stats_tests: bool,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: CorrelationsArgs) -> NailResult<()> {
    // Only Pearson correlation implemented
    if args.correlation_type != CorrelationType::Pearson {
        return Err(NailError::Statistics(
            format!("{:?} correlations not yet implemented", args.correlation_type)
        ));
    }
    if args.verbose {
        eprintln!("Reading data from: {}", args.input.display());
    }
    
    let df = read_data(&args.input).await?;
    let schema = df.schema();
    
    let target_columns = if let Some(col_spec) = &args.columns {
        let selected = select_columns_by_pattern(schema.clone().into(), col_spec)?;
        
        // Validate that all selected columns are numeric
        let mut numeric_columns = Vec::new();
        let mut non_numeric_columns = Vec::new();
        
        for col_name in &selected {
            if let Ok(field) = schema.field_with_name(None, col_name) {
                match field.data_type() {
                    datafusion::arrow::datatypes::DataType::Int64 | 
                    datafusion::arrow::datatypes::DataType::Float64 | 
                    datafusion::arrow::datatypes::DataType::Int32 | 
                    datafusion::arrow::datatypes::DataType::Float32 |
                    datafusion::arrow::datatypes::DataType::Int16 |
                    datafusion::arrow::datatypes::DataType::Int8 |
                    datafusion::arrow::datatypes::DataType::UInt64 |
                    datafusion::arrow::datatypes::DataType::UInt32 |
                    datafusion::arrow::datatypes::DataType::UInt16 |
                    datafusion::arrow::datatypes::DataType::UInt8 => {
                        numeric_columns.push(col_name.clone());
                    },
                    _ => {
                        non_numeric_columns.push((col_name.clone(), field.data_type().clone()));
                    }
                }
            }
        }
        
        if !non_numeric_columns.is_empty() {
            let non_numeric_info: Vec<String> = non_numeric_columns.iter()
                .map(|(name, dtype)| format!("'{}' ({:?})", name, dtype))
                .collect();
            return Err(NailError::Statistics(
                format!("Correlation requires numeric columns only. Non-numeric columns found: {}. Available numeric columns: {:?}", 
                    non_numeric_info.join(", "), numeric_columns)
            ));
        }
        
        if numeric_columns.len() < 2 {
            return Err(NailError::Statistics(
                format!("Need at least 2 numeric columns for correlation. Found {} numeric columns: {:?}", 
                    numeric_columns.len(), numeric_columns)
            ));
        }
        
        numeric_columns
    } else {
        let numeric_columns: Vec<String> = schema.fields().iter()
            .filter(|f| matches!(f.data_type(), 
                datafusion::arrow::datatypes::DataType::Int64 | 
                datafusion::arrow::datatypes::DataType::Float64 | 
                datafusion::arrow::datatypes::DataType::Int32 | 
                datafusion::arrow::datatypes::DataType::Float32 |
                datafusion::arrow::datatypes::DataType::Int16 |
                datafusion::arrow::datatypes::DataType::Int8 |
                datafusion::arrow::datatypes::DataType::UInt64 |
                datafusion::arrow::datatypes::DataType::UInt32 |
                datafusion::arrow::datatypes::DataType::UInt16 |
                datafusion::arrow::datatypes::DataType::UInt8
            ))
            .map(|f| f.name().clone())
            .collect();
            
        if numeric_columns.len() < 2 {
            return Err(NailError::Statistics(
                format!("Need at least 2 numeric columns for correlation. Found {} numeric columns: {:?}", 
                    numeric_columns.len(), numeric_columns)
            ));
        }
        
        numeric_columns
    };
    
    if args.verbose {
        eprintln!("Computing {:?} correlations for {} numeric columns: {:?}", 
            args.correlation_type, target_columns.len(), target_columns);
    }
    
    let corr_df = calculate_correlations(
        &df, 
        &target_columns, 
        &args.correlation_type,
        args.correlation_matrix,
        args.stats_tests
    ).await?;
    
    display_dataframe(&corr_df, args.output.as_deref(), args.format.as_ref()).await?;
    
    Ok(())
}

use clap::Parser;
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "nail")]
#[command(about = "A fast parquet utility written in Rust")]
#[command(version = "1.1.0")]
pub struct Cli {
	#[command(subcommand)]
	pub command: crate::commands::Commands,
	
	#[arg(short, long, global = true, help = "Enable verbose output")]
	pub verbose: bool,
	
	#[arg(short, long, global = true, help = "Number of parallel jobs", default_value = "4")]
	pub jobs: usize,
}

#[derive(clap::Args, Clone)]
pub struct GlobalArgs {
	#[arg(short, long, help = "Input file")]
	pub input: Option<PathBuf>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format (auto-detect by default)", value_enum)]
	pub format: Option<OutputFormat>,
	
	#[arg(long, help = "Random seed for reproducible results")]
	pub random: Option<u64>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
	
	#[arg(short, long, help = "Number of parallel jobs", default_value = "4")]
	pub jobs: usize,
}

#[derive(clap::ValueEnum, Clone, Debug)]
pub enum OutputFormat {
	Json,
	Text,
	Csv,
	Parquet,
}

use clap::Args;
use datafusion::prelude::*;
use datafusion::common::DFSchemaRef;
use std::path::PathBuf;
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct AppendArgs {
	#[arg(short, long, help = "Input file (base table)")]
	pub input: PathBuf,
	
	#[arg(long, help = "Files to append (comma-separated)")]
	pub files: String,
	
	#[arg(long, help = "Ignore schema mismatches")]
	pub ignore_schema: bool,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: AppendArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading base table from: {}", args.input.display());
	}
	
	let mut base_df = read_data(&args.input).await?;
	let base_schema: DFSchemaRef = base_df.schema().clone().into();
	
	let append_files: Vec<&str> = args.files.split(',').map(|s| s.trim()).collect();
	
	if args.verbose {
		eprintln!("Appending {} files", append_files.len());
	}
	
	for file_path in append_files {
		let path = PathBuf::from(file_path);
		
		if args.verbose {
			eprintln!("Appending: {}", path.display());
		}
		
		let append_df = read_data(&path).await?;
		let append_schema: DFSchemaRef = append_df.schema().clone().into();
		
		if !args.ignore_schema && !schemas_compatible(&base_schema, &append_schema) {
			return Err(NailError::InvalidArgument(format!(
				"Schema mismatch in file: {}. Use --ignore-schema to force append.",
				path.display()
			)));
		}
		
		let aligned_df = if args.ignore_schema {
			align_schemas(&append_df, &base_schema).await?
		} else {
			append_df
		};
		
		base_df = base_df.union(aligned_df)?;
	}
	
	if args.verbose {
		let total_rows = base_df.clone().count().await?;
		eprintln!("Final dataset contains {} rows", total_rows);
	}
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&base_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&base_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

fn schemas_compatible(schema1: &datafusion::common::DFSchemaRef, schema2: &datafusion::common::DFSchemaRef) -> bool {
	if schema1.fields().len() != schema2.fields().len() {
		return false;
	}
	
	for (field1, field2) in schema1.fields().iter().zip(schema2.fields().iter()) {
		if field1.name() != field2.name() || field1.data_type() != field2.data_type() {
			return false;
		}
	}
	
	true
}

async fn align_schemas(df: &DataFrame, target_schema: &datafusion::common::DFSchemaRef) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let current_schema = df.schema();
	let mut select_exprs = Vec::new();
	
	for target_field in target_schema.fields() {
		let target_name = target_field.name();
		
		if let Ok(_current_field) = current_schema.field_with_name(None, target_name) {
			select_exprs.push(col(target_name));
		} else {
			let null_expr = match target_field.data_type() {
				datafusion::arrow::datatypes::DataType::Int64 => lit(0i64).alias(target_name),
				datafusion::arrow::datatypes::DataType::Float64 => lit(0.0f64).alias(target_name),
				datafusion::arrow::datatypes::DataType::Utf8 => lit("").alias(target_name),
				datafusion::arrow::datatypes::DataType::Boolean => lit(false).alias(target_name),
				_ => lit("").alias(target_name),
			};
			select_exprs.push(null_expr);
		}
	}
	
	let result = ctx.table(table_name).await?.select(select_exprs)?;
	Ok(result)
}

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;
use crate::commands::select::{select_columns_by_pattern, parse_row_specification};

#[derive(Args, Clone)]
pub struct DropArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Column names or regex patterns to drop (comma-separated)")]
	pub columns: Option<String>,
	
	#[arg(short, long, help = "Row numbers or ranges to drop (e.g., 1,3,5-10)")]
	pub rows: Option<String>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: DropArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let mut result_df = df;
	
	if let Some(col_spec) = &args.columns {
		let schema = result_df.schema();
		let columns_to_drop = select_columns_by_pattern(schema.clone().into(), col_spec)?;
		
		if args.verbose {
			eprintln!("Dropping {} columns: {:?}", columns_to_drop.len(), columns_to_drop);
		}
		
		let remaining_columns: Vec<Expr> = result_df.schema().fields().iter()
			.filter(|f| !columns_to_drop.contains(f.name()))
			.map(|f| col(f.name()))
			.collect();
		
		result_df = result_df.select(remaining_columns)?;
	}
	
	if let Some(row_spec) = &args.rows {
		let row_indices = parse_row_specification(row_spec)?;
		
		if args.verbose {
			eprintln!("Dropping {} rows", row_indices.len());
		}
		
		result_df = drop_rows_by_indices(&result_df, &row_indices).await?;
	}
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&result_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&result_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

async fn drop_rows_by_indices(df: &DataFrame, indices: &[usize]) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let indices_str = indices.iter()
		.map(|&i| (i + 1).to_string())
		.collect::<Vec<_>>()
		.join(",");
	
	// Get the original column names and quote them to preserve case
	let original_columns: Vec<String> = df.schema().fields().iter()
		.map(|f| format!("\"{}\"", f.name()))
		.collect();
	
	let sql = format!(
		"SELECT {} FROM (SELECT {}, ROW_NUMBER() OVER() as rn FROM {}) WHERE rn NOT IN ({})",
		original_columns.join(", "),
		original_columns.join(", "),
		table_name, 
		indices_str
	);
	
	let result = ctx.sql(&sql).await?;
	Ok(result)
}

use clap::Subcommand;

pub mod head;
pub mod tail;
pub mod preview;
pub mod headers;
pub mod stats;
pub mod correlations;
pub mod select;
pub mod drop;
pub mod fill;
pub mod filter;
pub mod id;
pub mod merge;
pub mod append;
pub mod schema;
pub mod sample;
pub mod convert;
pub mod shuffle;

#[derive(Subcommand)]
pub enum Commands {
	#[command(about = "Display first N rows")]
	Head(head::HeadArgs),
	
	#[command(about = "Display last N rows")]
	Tail(tail::TailArgs),
	
	#[command(about = "Preview random N rows")]
	Preview(preview::PreviewArgs),
	
	#[command(about = "Display column headers")]
	Headers(headers::HeadersArgs),
	
	#[command(about = "Display statistics")]
	Stats(stats::StatsArgs),
	
	#[command(about = "Display correlations")]
	Correlations(correlations::CorrelationsArgs),
	
	#[command(about = "Select columns or rows")]
	Select(select::SelectArgs),
	
	#[command(about = "Drop columns or rows")]
	Drop(drop::DropArgs),
	
	#[command(about = "Fill missing values")]
	Fill(fill::FillArgs),
	
	#[command(about = "Filter data")]
	Filter(filter::FilterArgs),
	
	#[command(about = "Add ID column")]
	Id(id::IdArgs),
	
	#[command(about = "Merge datasets")]
	Merge(merge::MergeArgs),
	
	#[command(about = "Append datasets")]
	Append(append::AppendArgs),
	
	#[command(about = "Display schema")]
	Schema(schema::SchemaArgs),
	
	#[command(about = "Sample data")]
	Sample(sample::SampleArgs),
	
	#[command(about = "Convert between formats")]
	Convert(convert::ConvertArgs),
	
	#[command(about = "Shuffle data")]
	Shuffle(shuffle::ShuffleArgs),
}

pub mod io;
pub mod format;
pub mod stats;

use datafusion::prelude::*;
use std::path::Path;
use crate::error::{NailError, NailResult};

pub async fn create_context() -> NailResult<SessionContext> {
	let config = SessionConfig::new()
		.with_batch_size(8192)
		.with_target_partitions(num_cpus::get());
	
	Ok(SessionContext::new_with_config(config))
}

pub fn detect_file_format(path: &Path) -> NailResult<FileFormat> {
	match path.extension().and_then(|s| s.to_str()) {
		Some("parquet") => Ok(FileFormat::Parquet),
		Some("csv") => Ok(FileFormat::Csv),
		Some("json") => Ok(FileFormat::Json),
		Some("xlsx") => Ok(FileFormat::Excel),
		_ => Err(NailError::UnsupportedFormat(
			format!("Unable to detect format for file: {}", path.display())
		)),
	}
}

#[derive(Debug, Clone)]
pub enum FileFormat {
	Parquet,
	Csv,
	Json,
	Excel,
}

use assert_cmd::Command;
use predicates::prelude::*;
use std::fs;
use std::path::PathBuf;
use tempfile::{tempdir, TempDir};

#[path = "common/mod.rs"]
mod common;

// Helper to create a temporary directory and all sample files for each test.
fn setup() -> (TempDir, PathBuf, PathBuf, PathBuf, PathBuf) {
    let temp_dir = tempdir().unwrap();
    let input_path = temp_dir.path().join("sample.parquet");
    let input2_path = temp_dir.path().join("sample2.parquet");
    let input3_path = temp_dir.path().join("sample3.parquet");
    let input_csv_path = temp_dir.path().join("sample.csv");

    common::create_sample_parquet(&input_path).unwrap();
    common::create_sample2_parquet(&input2_path).unwrap();
    common::create_sample3_parquet(&input3_path).unwrap();
    common::create_sample_csv(&input_csv_path).unwrap();

    // Create Excel test file in fixtures directory
    let fixtures_dir = std::path::Path::new("tests/fixtures");
    if !fixtures_dir.exists() {
        std::fs::create_dir_all(fixtures_dir).unwrap();
    }
    let excel_path = fixtures_dir.join("sample.xlsx");
    if !excel_path.exists() {
        common::create_sample_excel(&excel_path).unwrap();
    }

    (temp_dir, input_path, input2_path, input3_path, input_csv_path)
}

#[test]
fn test_cli_help() {
    Command::cargo_bin("nail").unwrap().arg("--help").assert().success();
}

// --- Data Inspection ---

mod head_tests {
    use super::*;
    #[test]
    fn test_head_to_json_file() {
        let (td, input, _, _, _) = setup();
        let out = td.path().join("out.json");
        Command::cargo_bin("nail").unwrap()
            .args(["head", "-i", input.to_str().unwrap(), "-n", "1", "-o", out.to_str().unwrap()])
            .assert().success();
        let content = fs::read_to_string(out).unwrap();
        assert!(content.contains(r#""id":1,"name":"Alice","value":100.0,"category":"A"}"#));
    }
}

mod tail_tests {
    use super::*;
    #[test]
    fn test_tail_verbose() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["tail", "-i", input.to_str().unwrap(), "-n", "1", "--verbose"])
            .assert().success()
            .stderr(predicate::str::contains("Displaying last 1 rows"));
    }
}

mod preview_tests {
    use super::*;
    #[test]
    fn test_preview_reproducible() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["preview", "-i", input.to_str().unwrap(), "-n", "2", "--random", "42"])
            .assert().success()
            .stdout(predicate::str::contains("Alice"))
            .stdout(predicate::str::contains("Eve"));
    }
}

mod headers_tests {
    use super::*;
    #[test]
    fn test_headers_filter_regex() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["headers", "-i", input.to_str().unwrap(), "--filter", "(id|name)"])
            .assert().success()
            .stdout(predicate::str::is_match("^id\nname\n$").unwrap());
    }

    #[test]
    fn test_headers_all_columns() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["headers", "-i", input.to_str().unwrap()])
            .assert().success()
            .stdout(predicate::str::contains("id"))
            .stdout(predicate::str::contains("name"))
            .stdout(predicate::str::contains("value"))
            .stdout(predicate::str::contains("category"));
    }

    #[test]
    fn test_headers_output_to_file() {
        let (td, input, _, _, _) = setup();
        let out = td.path().join("headers.txt");
        Command::cargo_bin("nail").unwrap()
            .args(["headers", "-i", input.to_str().unwrap(), "-o", out.to_str().unwrap()])
            .assert().success();
        let content = fs::read_to_string(out).unwrap();
        assert!(content.contains("id"));
        assert!(content.contains("name"));
    }
}

mod schema_tests {
    use super::*;
    #[test]
    fn test_schema_display() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["schema", "-i", input.to_str().unwrap()])
            .assert().success()
            .stdout(predicate::str::contains("Column"))
            .stdout(predicate::str::contains("Int64"))
            .stdout(predicate::str::contains("Utf8"));
    }

    #[test]
    fn test_schema_verbose() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["schema", "-i", input.to_str().unwrap(), "--verbose"])
            .assert().success()
            .stderr(predicate::str::contains("Reading schema"));
    }
}

// --- Statistics & Analysis ---

mod stats_tests {
    use super::*;
    #[test]
    fn test_stats_exhaustive_on_columns_regex() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["stats", "-i", input.to_str().unwrap(), "-t", "exhaustive", "-c", "^v"])
            .assert().success()
            .stdout(predicate::str::contains("value"))
            .stdout(predicate::str::contains("id").not());
    }

    #[test]
    fn test_stats_unimplemented_fails_gracefully() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["stats", "-i", input.to_str().unwrap(), "-t", "hypothesis"])
            .assert().failure()
            .stderr(predicate::str::contains("Hypothesis tests not yet implemented"));
    }
}

mod correlations_tests {
    use super::*;
    #[test]
    fn test_correlations_unimplemented_fails_gracefully() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["correlations", "-i", input.to_str().unwrap(), "-t", "spearman"])
            .assert().failure()
            .stderr(predicate::str::contains("not yet implemented"));
    }
}

// --- Data Manipulation ---

mod filter_tests {
    use super::*;
    #[test]
    fn test_filter_multiple_conditions() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["filter", "-i", input.to_str().unwrap(), "-c", "id>1,category=A"])
            .assert().success()
            .stdout(predicate::str::contains("Charlie")) // id=3, cat=A
            .stdout(predicate::str::contains("Eve"))     // id=5, cat=A
            .stdout(predicate::str::contains("Alice").not()); // id=1
    }

    #[test]
    fn test_filter_row_numeric_only() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["filter", "-i", input.to_str().unwrap(), "--rows", "numeric-only"])
            .assert().success()
            .stdout(predicate::str::contains("id"))
            .stdout(predicate::str::contains("value"))
            .stdout(predicate::str::contains("name").not());
    }
}

mod fill_tests {
    use super::*;
    #[test]
    fn test_fill_with_mean() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["fill", "-i", input.to_str().unwrap(), "--method", "mean", "-c", "value"])
            .assert().success()
            .stdout(predicate::str::contains("325.0")); // Mean of (100,300,400,500) is 325
    }

    #[test]
    fn test_fill_with_value() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["fill", "-i", input.to_str().unwrap(), "--method", "value", "--value", "999", "-c", "value"])
            .assert().success()
            .stdout(predicate::str::contains("999"));
    }

    #[test]
    fn test_fill_verbose() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["fill", "-i", input.to_str().unwrap(), "--method", "value", "--value", "0", "--verbose"])
            .assert().success()
            .stderr(predicate::str::contains("Filling missing values"));
    }
}

mod select_tests {
    use super::*;
    #[test]
    fn test_select_columns() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["select", "-i", input.to_str().unwrap(), "-c", "id,name"])
            .assert().success()
            .stdout(predicate::str::contains("id"))
            .stdout(predicate::str::contains("name"))
            .stdout(predicate::str::contains("value").not());
    }

    #[test]
    fn test_select_columns_regex() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["select", "-i", input.to_str().unwrap(), "-c", "^(id|name)$"])
            .assert().success()
            .stdout(predicate::str::contains("Alice"))
            .stdout(predicate::str::contains("Bob"));
    }

    #[test]
    fn test_select_with_output() {
        let (td, input, _, _, _) = setup();
        let out = td.path().join("selected.parquet");
        Command::cargo_bin("nail").unwrap()
            .args(["select", "-i", input.to_str().unwrap(), "-c", "id,name", "-o", out.to_str().unwrap()])
            .assert().success();
        
        // Verify the output file was created and has correct columns
        Command::cargo_bin("nail").unwrap()
            .args(["headers", "-i", out.to_str().unwrap()])
            .assert().success()
            .stdout(predicate::str::contains("id"))
            .stdout(predicate::str::contains("name"))
            .stdout(predicate::str::contains("value").not());
    }
}

mod drop_tests {
    use super::*;
    #[test]
    fn test_drop_columns() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["drop", "-i", input.to_str().unwrap(), "-c", "value,category"])
            .assert().success()
            .stdout(predicate::str::contains("id"))
            .stdout(predicate::str::contains("name"))
            .stdout(predicate::str::contains("value").not())
            .stdout(predicate::str::contains("category").not());
    }

    #[test]
    fn test_drop_columns_regex() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["drop", "-i", input.to_str().unwrap(), "-c", "^(value|category)$"])
            .assert().success()
            .stdout(predicate::str::contains("id"))
            .stdout(predicate::str::contains("name"))
            .stdout(predicate::str::contains("value").not());
    }

    #[test]
    fn test_drop_verbose() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["drop", "-i", input.to_str().unwrap(), "-c", "value", "--verbose"])
            .assert().success()
            .stderr(predicate::str::contains("Dropping"));
    }
}

// --- Data Sampling & Transformation ---

mod sample_tests {
    use super::*;
    #[test]
    fn test_sample_stratified() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["sample", "-i", input.to_str().unwrap(), "-n", "2", "--method", "stratified", "--stratify-by", "category", "--random", "1"])
            .assert().success()
            .stdout(predicate::str::contains("Alice")) // From category A
            .stdout(predicate::str::contains("Bob"));  // From category B
    }

    #[test]
    fn test_sample_last() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["sample", "-i", input.to_str().unwrap(), "-n", "2", "--method", "last"])
            .assert().success()
            .stdout(predicate::str::contains("Eve"))
            .stdout(predicate::str::contains("Alice").not());
    }

    #[test]
    fn test_sample_first() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["sample", "-i", input.to_str().unwrap(), "-n", "2", "--method", "first"])
            .assert().success()
            .stdout(predicate::str::contains("Alice"))
            .stdout(predicate::str::contains("Bob"))
            .stdout(predicate::str::contains("Eve").not());
    }

    #[test]
    fn test_sample_random_reproducible() {
        let (_td, input, _, _, _) = setup();
        let result1 = Command::cargo_bin("nail").unwrap()
            .args(["sample", "-i", input.to_str().unwrap(), "-n", "3", "--method", "random", "--random", "42"])
            .assert().success()
            .get_output().stdout.clone();

        let result2 = Command::cargo_bin("nail").unwrap()
            .args(["sample", "-i", input.to_str().unwrap(), "-n", "3", "--method", "random", "--random", "42"])
            .assert().success()
            .get_output().stdout.clone();

        assert_eq!(result1, result2); // Should be identical with same seed
    }

    #[test]
    fn test_sample_verbose() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["sample", "-i", input.to_str().unwrap(), "-n", "2", "--verbose"])
            .assert().success()
            .stderr(predicate::str::contains("Sampling 2 rows"));
    }
}

mod shuffle_tests {
    use super::*;
    #[test]
    fn test_shuffle_with_seed() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["shuffle", "-i", input.to_str().unwrap(), "--random", "42"])
            .assert().success()
            .stdout(predicate::str::contains("Alice"))
            .stdout(predicate::str::contains("Bob"));
    }

    #[test]
    fn test_shuffle_verbose() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["shuffle", "-i", input.to_str().unwrap(), "--verbose"])
            .assert().success()
            .stderr(predicate::str::contains("Shuffling"));
    }

    #[test]
    fn test_shuffle_to_file() {
        let (td, input, _, _, _) = setup();
        let out = td.path().join("shuffled.parquet");
        Command::cargo_bin("nail").unwrap()
            .args(["shuffle", "-i", input.to_str().unwrap(), "-o", out.to_str().unwrap()])
            .assert().success();
        
        // Verify output file was created
        assert!(out.exists());
    }
}

mod id_tests {
    use super::*;
    #[test]
    fn test_id_create_error() {
        let (_td, input, _, _, _) = setup();
        // This should fail because 'id' column already exists in test data
        Command::cargo_bin("nail").unwrap()
            .args(["id", "-i", input.to_str().unwrap(), "--create"])
            .assert().failure()
            .stderr(predicate::str::contains("already exists"));
    }

    #[test]
    fn test_id_create_custom_name_prefix() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["id", "-i", input.to_str().unwrap(), "--create", "--id-col-name", "record_id", "--prefix", "REC"])
            .assert().success()
            .stdout(predicate::str::contains("record_id"))
            .stdout(predicate::str::contains("REC"));
    }

    #[test]
    fn test_id_verbose_error() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["id", "-i", input.to_str().unwrap(), "--create", "--verbose"])
            .assert().failure()
            .stderr(predicate::str::contains("Creating ID column"))
            .stderr(predicate::str::contains("already exists"));
    }
}

// --- Data Combination ---

mod merge_tests {
    use super::*;
    #[test]
    fn test_merge_with_key_mapping() {
        let (_td, input1, _, input3, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["merge", "-i", input1.to_str().unwrap(), "--right", input3.to_str().unwrap(), "--key-mapping", "id=person_id"])
            .assert().success()
            .stdout(predicate::str::contains("Alice"))
            .stdout(predicate::str::contains("active"))
            .stdout(predicate::str::contains("inactive"));
    }
}

mod append_tests {
    use super::*;
    #[test]
    fn test_append_multiple_files() {
        let (td, input1, input2, input3, _) = setup();
        let output_path = td.path().join("appended.parquet");
        let files_to_append = format!("{},{}", input2.to_str().unwrap(), input3.to_str().unwrap());
        Command::cargo_bin("nail").unwrap()
            .args(["append", "-i", input1.to_str().unwrap(), "--files", &files_to_append, "-o", output_path.to_str().unwrap(), "--ignore-schema"])
            .assert().success();

        // Check row count: 5 (input1) + 4 (input2) + 3 (input3) = 12
        Command::cargo_bin("nail").unwrap()
            .args(["stats", "-i", output_path.to_str().unwrap()])
            .assert().success()
            .stdout(predicate::str::is_match(r"count\s+\|\s+12").unwrap());
    }
}

// --- Format Conversion ---

mod convert_tests {
    use super::*;
    #[test]
    fn test_convert_csv_to_parquet() {
        let (td, _, _, _, input_csv) = setup();
        let output_path = td.path().join("output.parquet");
        Command::cargo_bin("nail").unwrap()
            .args(["convert", "-i", input_csv.to_str().unwrap(), "-o", output_path.to_str().unwrap()])
            .assert().success();

        // Verify the content of the created parquet file
        Command::cargo_bin("nail").unwrap()
            .args(["head", "-i", output_path.to_str().unwrap()])
            .assert().success()
            .stdout(predicate::str::contains("Frank"))
            .stdout(predicate::str::contains("1000"));
    }

    #[test]
    fn test_convert_parquet_to_csv() {
        let (td, input, _, _, _) = setup();
        let out = td.path().join("out.csv");
        Command::cargo_bin("nail").unwrap()
            .args(["convert", "-i", input.to_str().unwrap(), "-o", out.to_str().unwrap()])
            .assert().success();
        
        // Verify CSV content
        let content = fs::read_to_string(out).unwrap();
        assert!(content.contains("id,name,value,category"));
        assert!(content.contains("Alice"));
    }

    #[test]
    fn test_convert_parquet_to_json() {
        let (td, input, _, _, _) = setup();
        let out = td.path().join("out.json");
        Command::cargo_bin("nail").unwrap()
            .args(["convert", "-i", input.to_str().unwrap(), "-o", out.to_str().unwrap()])
            .assert().success();
        
        // Verify JSON content
        let content = fs::read_to_string(out).unwrap();
        assert!(content.contains(r#""name":"Alice""#));
    }

    #[test]
    fn test_convert_verbose() {
        let (td, input, _, _, _) = setup();
        let out = td.path().join("out.csv");
        Command::cargo_bin("nail").unwrap()
            .args(["convert", "-i", input.to_str().unwrap(), "-o", out.to_str().unwrap(), "--verbose"])
            .assert().success()
            .stderr(predicate::str::contains("Converting"))
            .stderr(predicate::str::contains("Processing"));
    }

    #[test]
    fn test_convert_format_detection() {
        let (td, input, _, _, _) = setup();
        let out = td.path().join("out.json");
        Command::cargo_bin("nail").unwrap()
            .args(["convert", "-i", input.to_str().unwrap(), "-o", out.to_str().unwrap()])
            .assert().success();
        
        // Verify JSON format was auto-detected from extension
        assert!(out.exists());
        let content = fs::read_to_string(out).unwrap();
        assert!(content.contains(r#""name":"Alice""#));
    }
}

// --- Extended Tests ---

mod merge_extended_tests {
    use super::*;
    #[test]
    fn test_merge_left_join() {
        let (_td, input1, input2, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["merge", "-i", input1.to_str().unwrap(), "--right", input2.to_str().unwrap(), "--left-join", "--key", "id"])
            .assert().success()
            .stdout(predicate::str::contains("Alice")) // From left table
            .stdout(predicate::str::contains("Bob"))   // From left table
            .stdout(predicate::str::contains("88.0"));  // From right table for id=4
    }

    #[test]
    fn test_merge_right_join() {
        let (_td, input1, input2, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["merge", "-i", input1.to_str().unwrap(), "--right", input2.to_str().unwrap(), "--right-join", "--key", "id"])
            .assert().success()
            .stdout(predicate::str::contains("88.0"))   // From right table
            .stdout(predicate::str::contains("92.5"));  // From right table
    }

    #[test]
    fn test_merge_inner_join() {
        let (_td, input1, input2, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["merge", "-i", input1.to_str().unwrap(), "--right", input2.to_str().unwrap(), "--key", "id"])
            .assert().success()
            .stdout(predicate::str::contains("Eve"))    // id=5 exists in both
            .stdout(predicate::str::contains("92.5"))   // score for id=5
            .stdout(predicate::str::contains("Alice").not()); // id=1 only in left
    }

    #[test]
    fn test_merge_verbose() {
        let (_td, input1, input2, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["merge", "-i", input1.to_str().unwrap(), "--right", input2.to_str().unwrap(), "--key", "id", "--verbose"])
            .assert().success()
            .stderr(predicate::str::contains("Reading"))
            .stderr(predicate::str::contains("Inner join"));
    }
}

mod filter_extended_tests {
    use super::*;
    #[test]
    fn test_filter_no_nan() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["filter", "-i", input.to_str().unwrap(), "--rows", "no-nan"])
            .assert().success()
            .stdout(predicate::str::contains("Alice"))
            .stdout(predicate::str::contains("Charlie"));
        // Bob should be filtered out due to null value
    }

    #[test]
    fn test_filter_char_only() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["filter", "-i", input.to_str().unwrap(), "--rows", "char-only"])
            .assert().success()
            .stdout(predicate::str::contains("name"))
            .stdout(predicate::str::contains("category"))
            .stdout(predicate::str::contains("id").not())
            .stdout(predicate::str::contains("value").not());
    }

    #[test]
    fn test_filter_complex_conditions() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["filter", "-i", input.to_str().unwrap(), "-c", "id>=3,category=A"])
            .assert().success()
            .stdout(predicate::str::contains("Charlie")) // id=3, category=A
            .stdout(predicate::str::contains("Eve"))     // id=5, category=A
            .stdout(predicate::str::contains("Alice").not()) // id=1, category=A but id<3
            .stdout(predicate::str::contains("Bob").not());   // id=2, category=B
    }
}

mod stats_extended_tests {
    use super::*;
    #[test]
    fn test_stats_basic() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["stats", "-i", input.to_str().unwrap(), "-t", "basic"])
            .assert().success()
            .stdout(predicate::str::contains("count"))
            .stdout(predicate::str::contains("mean"));
    }

    #[test]
    fn test_stats_specific_columns() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["stats", "-i", input.to_str().unwrap(), "-c", "id,value"])
            .assert().success()
            .stdout(predicate::str::contains("id"))
            .stdout(predicate::str::contains("value"))
            .stdout(predicate::str::contains("category").not());
    }

    #[test]
    fn test_stats_output_to_file() {
        let (td, input, _, _, _) = setup();
        let out = td.path().join("stats.json");
        Command::cargo_bin("nail").unwrap()
            .args(["stats", "-i", input.to_str().unwrap(), "-o", out.to_str().unwrap(), "-f", "json"])
            .assert().success();
        
        assert!(out.exists());
        let content = fs::read_to_string(out).unwrap();
        assert!(content.contains("count") || content.contains("mean"));
    }
}

mod append_extended_tests {
    use super::*;
    #[test]
    fn test_append_verbose() {
        let (td, input1, input2, _, _) = setup();
        let output_path = td.path().join("appended.parquet");
        Command::cargo_bin("nail").unwrap()
            .args(["append", "-i", input1.to_str().unwrap(), "--files", input2.to_str().unwrap(), "-o", output_path.to_str().unwrap(), "--ignore-schema", "--verbose"])
            .assert().success()
            .stderr(predicate::str::contains("Appending"))
            .stderr(predicate::str::contains("Final dataset"));
    }

    #[test]
    fn test_append_schema_validation() {
        let (td, input1, input2, _, _) = setup();
        let output_path = td.path().join("appended.parquet");
        // These schemas are different, so use --ignore-schema
        Command::cargo_bin("nail").unwrap()
            .args(["append", "-i", input1.to_str().unwrap(), "--files", input2.to_str().unwrap(), "-o", output_path.to_str().unwrap(), "--ignore-schema"])
            .assert().success();
    }
}

mod error_handling_tests {
    use super::*;
    #[test]
    fn test_missing_input_file() {
        Command::cargo_bin("nail").unwrap()
            .args(["head", "-i", "nonexistent.parquet"])
            .assert().failure()
            .stderr(predicate::str::contains("No such file"));
    }

    #[test]
    fn test_invalid_column_name() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["select", "-i", input.to_str().unwrap(), "-c", "nonexistent_column"])
            .assert().failure()
            .stderr(predicate::str::contains("not found").or(predicate::str::contains("Column")));
    }

    #[test]
    fn test_invalid_regex_pattern() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["headers", "-i", input.to_str().unwrap(), "--filter", "[invalid"])
            .assert().failure()
            .stderr(predicate::str::contains("regex").or(predicate::str::contains("pattern")));
    }

    #[test]
    fn test_fill_without_value() {
        let (_td, input, _, _, _) = setup();
        Command::cargo_bin("nail").unwrap()
            .args(["fill", "-i", input.to_str().unwrap(), "--method", "value"])
            .assert().failure()
            .stderr(predicate::str::contains("value").or(predicate::str::contains("required")));
    }
}

use datafusion::prelude::*;
use datafusion::dataframe::DataFrameWriteOptions;
use std::path::Path;
use crate::error::{NailError, NailResult};
use crate::utils::{create_context, detect_file_format, FileFormat};
use datafusion::arrow::array::{ArrayRef, StringArray, Float64Array, Int64Array, BooleanArray, RecordBatch};
use datafusion::arrow::datatypes::{DataType, Field, Schema};
use calamine::{Reader, Xlsx, open_workbook, Data};
use std::sync::Arc;

pub async fn read_data(path: &Path) -> NailResult<DataFrame> {
	let ctx = create_context().await?;
	let format = detect_file_format(path)?;
	
	let result = match format {
		FileFormat::Parquet => {
			ctx.read_parquet(path.to_str().unwrap(), ParquetReadOptions::default()).await
		},
		FileFormat::Csv => {
			ctx.read_csv(path.to_str().unwrap(), CsvReadOptions::default()).await
		},
		FileFormat::Json => {
			ctx.read_json(path.to_str().unwrap(), NdJsonReadOptions::default()).await
		},
		FileFormat::Excel => {
			read_excel_file(path, &ctx).await
		},
	};
	
	result.map_err(NailError::DataFusion)
}

async fn read_excel_file(path: &Path, ctx: &SessionContext) -> Result<DataFrame, datafusion::error::DataFusionError> {
	let mut workbook: Xlsx<_> = open_workbook(path)
		.map_err(|e| datafusion::error::DataFusionError::External(Box::new(e)))?;
	
	// Get the first worksheet
	let sheet_names = workbook.sheet_names();
	if sheet_names.is_empty() {
		return Err(datafusion::error::DataFusionError::External(
			"No worksheets found in Excel file".into()
		));
	}
	
	let sheet_name = &sheet_names[0];
	let range = workbook.worksheet_range(sheet_name)
		.map_err(|e| datafusion::error::DataFusionError::External(Box::new(e)))?;
	
	if range.is_empty() {
		return Err(datafusion::error::DataFusionError::External(
			"Empty worksheet".into()
		));
	}
	
	// Extract headers from first row
	let mut headers = Vec::new();
	let (rows, cols) = range.get_size();
	
	for col in 0..cols {
		let cell_value = range.get_value((0, col as u32)).unwrap_or(&Data::Empty);
		let header = match cell_value {
			Data::String(s) => s.clone(),
			Data::Int(i) => i.to_string(),
			Data::Float(f) => f.to_string(),
			_ => format!("Column_{}", col + 1),
		};
		headers.push(header);
	}
	
	// Determine column types by sampling data
	let mut column_types = vec![DataType::Utf8; cols];
	for col in 0..cols {
		let mut has_int = false;
		let mut has_float = false;
		let mut has_bool = false;
		
		// Sample up to 10 rows to determine type
		let sample_rows = std::cmp::min(rows, 11);
		for row in 1..sample_rows {
			let cell_value = range.get_value((row as u32, col as u32)).unwrap_or(&Data::Empty);
			match cell_value {
				Data::Int(_) => has_int = true,
				Data::Float(_) => has_float = true,
				Data::Bool(_) => has_bool = true,
				_ => {}
			}
		}
		
		// Determine type priority: Bool > Float > Int > String
		if has_bool && !has_int && !has_float {
			column_types[col] = DataType::Boolean;
		} else if has_float {
			column_types[col] = DataType::Float64;
		} else if has_int && !has_float {
			column_types[col] = DataType::Int64;
		}
	}
	
	// Create schema
	let fields: Vec<Field> = headers.iter().zip(column_types.iter())
		.map(|(name, dtype)| Field::new(name, dtype.clone(), true))
		.collect();
	let schema = Arc::new(Schema::new(fields));
	
	// Extract data
	let mut columns: Vec<ArrayRef> = Vec::new();
	
	for col in 0..cols {
		let dtype = &column_types[col];
		let mut values = Vec::new();
		
		for row in 1..rows {
			let cell_value = range.get_value((row as u32, col as u32)).unwrap_or(&Data::Empty);
			values.push(cell_value.clone());
		}
		
		let array: ArrayRef = match dtype {
			DataType::Boolean => {
				let bool_values: Vec<Option<bool>> = values.iter().map(|v| match v {
					Data::Bool(b) => Some(*b),
					Data::String(s) => {
						match s.to_lowercase().as_str() {
							"true" | "1" | "yes" => Some(true),
							"false" | "0" | "no" => Some(false),
							_ => None,
						}
					},
					_ => None,
				}).collect();
				Arc::new(BooleanArray::from(bool_values))
			},
			DataType::Int64 => {
				let int_values: Vec<Option<i64>> = values.iter().map(|v| match v {
					Data::Int(i) => Some(*i),
					Data::Float(f) => Some(*f as i64),
					Data::String(s) => s.parse().ok(),
					_ => None,
				}).collect();
				Arc::new(Int64Array::from(int_values))
			},
			DataType::Float64 => {
				let float_values: Vec<Option<f64>> = values.iter().map(|v| match v {
					Data::Float(f) => Some(*f),
					Data::Int(i) => Some(*i as f64),
					Data::String(s) => s.parse().ok(),
					_ => None,
				}).collect();
				Arc::new(Float64Array::from(float_values))
			},
			_ => { // String
				let string_values: Vec<Option<String>> = values.iter().map(|v| match v {
					Data::String(s) => Some(s.clone()),
					Data::Int(i) => Some(i.to_string()),
					Data::Float(f) => Some(f.to_string()),
					Data::Bool(b) => Some(b.to_string()),
					Data::DateTime(dt) => Some(format!("{}", dt)),
					Data::DateTimeIso(dt) => Some(dt.clone()),
					Data::DurationIso(d) => Some(d.clone()),
					Data::Error(e) => Some(format!("Error: {:?}", e)),
					Data::Empty => None,
				}).collect();
				Arc::new(StringArray::from(string_values))
			}
		};
		
		columns.push(array);
	}
	
	// Create record batch
	let batch = RecordBatch::try_new(schema.clone(), columns)
		.map_err(|e| datafusion::error::DataFusionError::External(Box::new(e)))?;
	
	// Convert to DataFrame
	ctx.read_batch(batch)
}

pub async fn write_data(df: &DataFrame, path: &Path, format: Option<&FileFormat>) -> NailResult<()> {
	let output_format = format.map(|f| f.clone()).unwrap_or_else(|| detect_file_format(path).unwrap_or(FileFormat::Parquet));
	
	match output_format {
		FileFormat::Parquet => {
			df.clone().write_parquet(
				path.to_str().unwrap(),
				DataFrameWriteOptions::new(),
				None,
			).await.map_err(NailError::DataFusion)?;
		},
		FileFormat::Csv => {
			df.clone().write_csv(
				path.to_str().unwrap(),
				DataFrameWriteOptions::new(),
				None,
			).await.map_err(NailError::DataFusion)?;
		},
		FileFormat::Json => {
			df.clone().write_json(
				path.to_str().unwrap(),
				DataFrameWriteOptions::new(),
				None,
			).await.map_err(NailError::DataFusion)?;
		},
		FileFormat::Excel => {
			return Err(NailError::UnsupportedFormat("Writing to Excel format is not yet supported".to_string()));
		},
	};
	
	Ok(())
}

use arrow::array::{Float64Array, Int64Array, StringArray};
use arrow::record_batch::RecordBatch;
use arrow_schema::{DataType, Field, Schema};
use parquet::arrow::ArrowWriter;
use std::fs::{self, File};
use std::path::Path;
use std::sync::Arc;

// Creates sample.parquet
pub fn create_sample_parquet(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
    let schema = Arc::new(Schema::new(vec![
        Field::new("id", DataType::Int64, false),
        Field::new("name", DataType::Utf8, true),
        Field::new("value", DataType::Float64, true),
        Field::new("category", DataType::Utf8, true),
    ]));

    let id_array = Int64Array::from(vec![1, 2, 3, 4, 5]);
    let name_array = StringArray::from(vec![Some("Alice"), Some("Bob"), Some("Charlie"), None, Some("Eve")]);
    let value_array = Float64Array::from(vec![Some(100.0), None, Some(300.0), Some(400.0), Some(500.0)]);
    let category_array = StringArray::from(vec![Some("A"), Some("B"), Some("A"), Some("B"), Some("A")]);

    let batch = RecordBatch::try_new(schema.clone(), vec![
        Arc::new(id_array),
        Arc::new(name_array),
        Arc::new(value_array),
        Arc::new(category_array),
    ])?;

    let file = File::create(path)?;
    let mut writer = ArrowWriter::try_new(file, schema, None)?;
    writer.write(&batch)?;
    writer.close()?;
    Ok(())
}

// Creates sample2.parquet for joins
pub fn create_sample2_parquet(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
    let schema = Arc::new(Schema::new(vec![
        Field::new("id", DataType::Int64, false),
        Field::new("score", DataType::Float64, true),
    ]));

    let id_array = Int64Array::from(vec![4, 5, 6, 7]);
    let score_array = Float64Array::from(vec![Some(88.0), Some(92.5), None, Some(88.0)]);

    let batch = RecordBatch::try_new(schema.clone(), vec![Arc::new(id_array), Arc::new(score_array)])?;

    let file = File::create(path)?;
    let mut writer = ArrowWriter::try_new(file, schema, None)?;
    writer.write(&batch)?;
    writer.close()?;
    Ok(())
}

// Creates sample3.parquet for key-mapping joins
pub fn create_sample3_parquet(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
    let schema = Arc::new(Schema::new(vec![
        Field::new("person_id", DataType::Int64, false),
        Field::new("status", DataType::Utf8, false),
    ]));

    let id_array = Int64Array::from(vec![1, 3, 5]);
    let status_array = StringArray::from(vec!["active", "inactive", "active"]);

    let batch = RecordBatch::try_new(schema.clone(), vec![Arc::new(id_array), Arc::new(status_array)])?;

    let file = File::create(path)?;
    let mut writer = ArrowWriter::try_new(file, schema, None)?;
    writer.write(&batch)?;
    writer.close()?;
    Ok(())
}

// Creates sample.csv for conversion tests
pub fn create_sample_csv(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
    let content = "id,name,value\n10,Frank,1000.0\n11,Grace,1100.0\n";
    fs::write(path, content)?;
    Ok(())
}

// Creates sample.xlsx for Excel conversion tests  
pub fn create_sample_excel(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
    // Create a simple CSV file as Excel placeholder since we can't easily create Excel files
    let content = "sample_item,sample_value\nitem1,100\nitem2,200\n";
    let csv_path = path.with_extension("csv");
    fs::write(&csv_path, content)?;
    
    // Copy it to .xlsx extension for the test (it will still be read as CSV)
    fs::copy(&csv_path, path)?;
    fs::remove_file(&csv_path)?;
    Ok(())
}

use clap::Args;
use std::path::PathBuf;
use rand::seq::SliceRandom;
use rand::SeedableRng;
use rand::rngs::StdRng;
use crate::error::NailResult;
use crate::utils::io::read_data;
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct PreviewArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Number of rows to display", default_value = "5")]
	pub number: usize,
	
	#[arg(short, long, help = "Random seed for reproducible results")]
	pub random: Option<u64>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: PreviewArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let total_rows = df.clone().count().await?;
	
	if total_rows <= args.number {
		display_dataframe(&df, args.output.as_deref(), args.format.as_ref()).await?;
		return Ok(());
	}
	
	let mut rng = match args.random {
		Some(seed) => StdRng::seed_from_u64(seed),
		None => StdRng::from_entropy(),
	};
	
	let mut indices: Vec<usize> = (0..total_rows).collect();
	indices.shuffle(&mut rng);
	indices.truncate(args.number);
	indices.sort();
	
	if args.verbose {
		eprintln!("Randomly sampling {} rows from {} total rows", args.number, total_rows);
	}
	
	let ctx = crate::utils::create_context().await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let indices_str = indices.iter()
		.map(|&i| (i + 1).to_string())
		.collect::<Vec<_>>()
		.join(",");
	
	// Get the original column names and quote them to preserve case
	let original_columns: Vec<String> = df.schema().fields().iter()
		.map(|f| format!("\"{}\"", f.name()))
		.collect();
	
	let sql = format!(
		"SELECT {} FROM (SELECT {}, ROW_NUMBER() OVER() as rn FROM {}) WHERE rn IN ({})",
		original_columns.join(", "),
		original_columns.join(", "),
		table_name, 
		indices_str
	);
	
	if args.verbose {
		eprintln!("Executing SQL: {}", sql);
	}
	
	let result = ctx.sql(&sql).await?;
	
	display_dataframe(&result, args.output.as_deref(), args.format.as_ref()).await?;
	
	Ok(())
}

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct MergeArgs {
	#[arg(short, long, help = "Input file (left table)")]
	pub input: PathBuf,
	
	#[arg(long, help = "Right table file to merge with")]
	pub right: PathBuf,
	
	#[arg(long, help = "Perform left join")]
	pub left_join: bool,
	
	#[arg(long, help = "Perform right join")]
	pub right_join: bool,
	
	#[arg(long, help = "Join key column name")]
	pub key: Option<String>,
	
	#[arg(long, help = "Key mapping for different column names (format: left_col=right_col)")]
	pub key_mapping: Option<String>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: MergeArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading left table from: {}", args.input.display());
		eprintln!("Reading right table from: {}", args.right.display());
	}
	
	let left_df = read_data(&args.input).await?;
	let right_df = read_data(&args.right).await?;
	
	let join_type = if args.left_join {
		JoinType::Left
	} else if args.right_join {
		JoinType::Right
	} else {
		JoinType::Inner
	};
	
	let (left_key, right_key) = if let Some(key_mapping) = &args.key_mapping {
		parse_key_mapping(key_mapping)?
	} else if let Some(key) = &args.key {
		// Handle case-insensitive key matching
		let left_schema = left_df.schema();
		let right_schema = right_df.schema();
		
		let actual_left_key = left_schema.fields().iter()
			.find(|f| f.name().to_lowercase() == key.to_lowercase())
			.map(|f| f.name().clone())
			.ok_or_else(|| {
				let available_cols: Vec<String> = left_schema.fields().iter()
					.map(|f| f.name().clone())
					.collect();
				NailError::ColumnNotFound(format!(
					"Join key '{}' not found in left table. Available columns: {:?}", 
					key, available_cols
				))
			})?;
			
		let actual_right_key = right_schema.fields().iter()
			.find(|f| f.name().to_lowercase() == key.to_lowercase())
			.map(|f| f.name().clone())
			.ok_or_else(|| {
				let available_cols: Vec<String> = right_schema.fields().iter()
					.map(|f| f.name().clone())
					.collect();
				NailError::ColumnNotFound(format!(
					"Join key '{}' not found in right table. Available columns: {:?}", 
					key, available_cols
				))
			})?;
			
		(actual_left_key, actual_right_key)
	} else {
		return Err(NailError::InvalidArgument("Either --key or --key-mapping must be specified".to_string()));
	};
	
	if args.verbose {
		eprintln!("Performing {:?} join on left.{} = right.{}", join_type, left_key, right_key);
	}
	
	let result_df = perform_join(&left_df, &right_df, &left_key, &right_key, join_type).await?;
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&result_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&result_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

fn parse_key_mapping(mapping: &str) -> NailResult<(String, String)> {
	let parts: Vec<&str> = mapping.split('=').collect();
	if parts.len() != 2 {
		return Err(NailError::InvalidArgument("Key mapping must be in format 'left_col=right_col'".to_string()));
	}
	Ok((parts[0].trim().to_string(), parts[1].trim().to_string()))
}

async fn perform_join(
	left_df: &DataFrame,
	right_df: &DataFrame,
	left_key: &str,
	right_key: &str,
	join_type: JoinType,
) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	
	ctx.register_table("left_table", left_df.clone().into_view())?;
	ctx.register_table("right_table", right_df.clone().into_view())?;
	
	let left_schema = left_df.schema();
	let right_schema = right_df.schema();
	
	let mut left_cols = Vec::new();
	let mut right_cols = Vec::new();
	
	for field in left_schema.fields() {
		left_cols.push(format!("l.{}", field.name()));
	}
	
	for field in right_schema.fields() {
		if field.name() != right_key {
			right_cols.push(format!("r.{} as r_{}", field.name(), field.name()));
		}
	}
	
	let join_clause = match join_type {
		JoinType::Inner => "INNER JOIN",
		JoinType::Left => "LEFT JOIN",
		JoinType::Right => "RIGHT JOIN",
		_ => "INNER JOIN",
	};
	
	let sql = format!(
		"SELECT {} FROM left_table l {} right_table r ON l.{} = r.{}",
		[left_cols, right_cols].concat().join(", "),
		join_clause,
		left_key,
		right_key
	);
	
	let result = ctx.sql(&sql).await?;
	Ok(result)
}

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;
use crate::utils::stats::select_columns_by_pattern;
use datafusion::arrow::array::{Float64Array, Int64Array, Array};

#[derive(Args, Clone)]
pub struct FillArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(long, help = "Fill method", value_enum, default_value = "value")]
	pub method: FillMethod,
	
	#[arg(long, help = "Fill value (required for 'value' method)")]
	pub value: Option<String>,
	
	#[arg(short, long, help = "Comma-separated column names to fill")]
	pub columns: Option<String>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

#[derive(clap::ValueEnum, Clone, Debug)]
pub enum FillMethod {
	Value,
	Mean,
	Median,
	Mode,
	Forward,
	Backward,
}

pub async fn execute(args: FillArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	if matches!(args.method, FillMethod::Value) && args.value.is_none() {
		return Err(NailError::InvalidArgument("--value is required when using 'value' method".to_string()));
	}
	
	let df = read_data(&args.input).await?;
	let schema = df.schema();
	
	let target_columns = if let Some(col_spec) = &args.columns {
		select_columns_by_pattern(schema.clone().into(), col_spec)?
	} else {
		schema.fields().iter().map(|f| f.name().clone()).collect()
	};
	
	if args.verbose {
		eprintln!("Filling missing values in {} columns using {:?} method", target_columns.len(), args.method);
	}
	
	let filled_df = fill_missing_values(&df, &target_columns, &args.method, args.value.as_deref()).await?;
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&filled_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&filled_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

async fn fill_missing_values(
	df: &DataFrame,
	columns: &[String],
	method: &FillMethod,
	value: Option<&str>,
) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let mut select_exprs = Vec::new();
	let schema = df.schema();
	
	for field in schema.fields() {
		let field_name = field.name();
		
		if columns.contains(field_name) {
			let filled_expr = match method {
				FillMethod::Value => {
					let fill_val = value.unwrap();
					match field.data_type() {
						datafusion::arrow::datatypes::DataType::Int64 => {
							let val: i64 = fill_val.parse()
								.map_err(|_| NailError::InvalidArgument(format!("Invalid integer value: {}", fill_val)))?;
							coalesce(vec![col(field_name), lit(val)])
						},
						datafusion::arrow::datatypes::DataType::Float64 => {
							let val: f64 = fill_val.parse()
								.map_err(|_| NailError::InvalidArgument(format!("Invalid float value: {}", fill_val)))?;
							coalesce(vec![col(field_name), lit(val)])
						},
						datafusion::arrow::datatypes::DataType::Utf8 => {
							coalesce(vec![col(field_name), lit(fill_val)])
						},
						_ => col(field_name),
					}
				},
				FillMethod::Mean => {
					// Use DataFusion's built-in avg function instead of manual calculation
					match field.data_type() {
						datafusion::arrow::datatypes::DataType::Float64 | 
						datafusion::arrow::datatypes::DataType::Int64 => {
							// Create a subquery to calculate the mean
							                            let _mean_sql = format!(
								"SELECT AVG({}) as mean_val FROM {}",
								field_name, table_name
							);
							
							// Use coalesce with a scalar subquery
							coalesce(vec![
								col(field_name),
								lit(0.0) // This will be replaced by actual mean calculation below
							])
						},
						_ => {
							return Err(NailError::Statistics(format!("Mean calculation not supported for column '{}' of type {:?}", field_name, field.data_type())));
						},
					}
				},
				_ => {
					return Err(NailError::Statistics("Only 'value' and 'mean' fill methods implemented".to_string()));
				},
			};
			
			select_exprs.push(filled_expr.alias(field_name));
		} else {
			select_exprs.push(col(field_name));
		}
	}
	
	// For mean method, we need to calculate means first
	if matches!(method, FillMethod::Mean) {
		let batches = df.clone().collect().await?;
		let mut means = std::collections::HashMap::new();
		
		// Calculate means for each target column
		for col_name in columns {
			            let _field = schema.fields().iter()
				.find(|f| f.name() == col_name)
				.ok_or_else(|| NailError::Statistics(format!("Column '{}' not found", col_name)))?;
			
					let idx = schema.fields().iter()
				.position(|f| f.name() == col_name)
				.unwrap();
			
					let mut sum = 0f64;
					let mut count = 0usize;
			
					for batch in &batches {
						let array = batch.column(idx);
						if let Some(farr) = array.as_any().downcast_ref::<Float64Array>() {
							for i in 0..farr.len() {
								if farr.is_valid(i) {
									sum += farr.value(i);
									count += 1;
								}
							}
						} else if let Some(iarr) = array.as_any().downcast_ref::<Int64Array>() {
							for i in 0..iarr.len() {
								if iarr.is_valid(i) {
									sum += iarr.value(i) as f64;
									count += 1;
								}
							}
						}
					}
			
					if count == 0 {
				return Err(NailError::Statistics(format!("No non-null values found in column '{}' to compute mean", col_name)));
			}
			
			means.insert(col_name.clone(), sum / count as f64);
		}
		
		// Now rebuild select expressions with actual means
		select_exprs.clear();
		for field in schema.fields() {
			let field_name = field.name();
			
			if columns.contains(field_name) {
				if let Some(&mean_val) = means.get(field_name) {
					let filled_expr = match field.data_type() {
						datafusion::arrow::datatypes::DataType::Float64 => {
					coalesce(vec![col(field_name), lit(mean_val)])
				},
						datafusion::arrow::datatypes::DataType::Int64 => {
							coalesce(vec![col(field_name), lit(mean_val)])
				},
						_ => col(field_name),
			};
			select_exprs.push(filled_expr.alias(field_name));
		} else {
			select_exprs.push(col(field_name));
				}
			} else {
				select_exprs.push(col(field_name));
			}
		}
	}
	
	let result = ctx.table(table_name).await?.select(select_exprs)?;
	Ok(result)
}

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct ShuffleArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Random seed for reproducible results")]
	pub random: Option<u64>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: ShuffleArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	
	if args.verbose {
		let total_rows = df.clone().count().await?;
		eprintln!("Shuffling {} rows", total_rows);
	}
	
	let shuffled_df = shuffle_dataframe(&df, args.random).await?;
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&shuffled_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&shuffled_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

async fn shuffle_dataframe(df: &DataFrame, _seed: Option<u64>) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	ctx.register_table("temp_table", df.clone().into_view())?;

	// Simple shuffling using ORDER BY RANDOM() 
	// For now, ignore the seed parameter since DataFusion's RANDOM() doesn't support seeding reliably
	let sql = "SELECT * FROM temp_table ORDER BY RANDOM()";
	
	let result = ctx.sql(sql).await?;
	Ok(result)
}

use clap::Args;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::read_data;
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct TailArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Number of rows to display", default_value = "5")]
	pub number: usize,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: TailArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let total_rows = df.clone().count().await?;
	
	if total_rows <= args.number {
		display_dataframe(&df, args.output.as_deref(), args.format.as_ref()).await?;
	} else {
		let skip_rows = total_rows - args.number;
		let tail_df = df.limit(skip_rows, Some(args.number))?;
		
		if args.verbose {
			eprintln!("Displaying last {} rows (total: {})", args.number, total_rows);
		}
		
		display_dataframe(&tail_df, args.output.as_deref(), args.format.as_ref()).await?;
	}
	
	Ok(())
}

use arrow::array::{Float64Array, Int64Array, StringArray};
use arrow::record_batch::RecordBatch;
use arrow_schema::{DataType, Field, Schema};
use parquet::arrow::ArrowWriter;
use std::fs::File;
use std::path::Path;
use std::sync::Arc;

pub fn create_sample_parquet(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
    let schema = Arc::new(Schema::new(vec![
        Field::new("id", DataType::Int64, false),
        Field::new("name", DataType::Utf8, true),
        Field::new("value", DataType::Float64, true),
    ]));

    let id_array = Int64Array::from(vec![1, 2, 3, 4, 5]);
    let name_array = StringArray::from(vec![
        Some("Alice"),
        Some("Bob"),
        Some("Charlie"),
        None,
        Some("Eve"),
    ]);
    let value_array =
        Float64Array::from(vec![Some(100.0), None, Some(300.0), Some(400.0), Some(500.0)]);

    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![
            Arc::new(id_array),
            Arc::new(name_array),
            Arc::new(value_array),
        ],
    )?;

    let file = File::create(path)?;
    let mut writer = ArrowWriter::try_new(file, schema, None)?;
    writer.write(&batch)?;
    writer.close()?;

    Ok(())
}

pub fn create_sample2_parquet(path: &Path) -> Result<(), Box<dyn std::error::Error>> {
    let schema = Arc::new(Schema::new(vec![
        Field::new("id", DataType::Int64, false),
        Field::new("category", DataType::Utf8, false),
        Field::new("score", DataType::Float64, true),
    ]));

    let id_array = Int64Array::from(vec![4, 5, 6, 7]);
    let category_array = StringArray::from(vec!["A", "B", "A", "C"]);
    let score_array = Float64Array::from(vec![Some(88.0), Some(92.5), None, Some(88.0)]);

    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![
            Arc::new(id_array),
            Arc::new(category_array),
            Arc::new(score_array),
        ],
    )?;

    let file = File::create(path)?;
    let mut writer = ArrowWriter::try_new(file, schema, None)?;
    writer.write(&batch)?;
    writer.close()?;

    Ok(())
}

use clap::Args;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::{read_data, write_data};
use crate::utils::{detect_file_format};

#[derive(Args, Clone)]
pub struct ConvertArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Output file")]
	pub output: PathBuf,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: ConvertArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Converting {} to {}", args.input.display(), args.output.display());
	}
	
	let input_format = detect_file_format(&args.input)?;
	let output_format = detect_file_format(&args.output)?;
	
	if args.verbose {
		eprintln!("Input format: {:?}, Output format: {:?}", input_format, output_format);
	}
	
	let df = read_data(&args.input).await?;
	
	if args.verbose {
		let rows = df.clone().count().await?;
		let cols = df.schema().fields().len();
		eprintln!("Processing {} rows, {} columns", rows, cols);
	}
	
	write_data(&df, &args.output, Some(&output_format)).await?;
	
	// Conversion completed successfully
	
	if args.verbose {
		eprintln!("Conversion completed successfully");
	}
	
	Ok(())
}

use clap::Args;

use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::read_data;

#[derive(Args, Clone)]
pub struct SchemaArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: SchemaArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading schema from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let schema = df.schema();
	
	let schema_info: Vec<SchemaField> = schema.fields().iter()
		.map(|field| SchemaField {
			name: field.name().clone(),
			data_type: format!("{:?}", field.data_type()),
			nullable: field.is_nullable(),
		})
		.collect();
	
	if args.verbose {
		eprintln!("Schema contains {} fields", schema_info.len());
	}
	
	match &args.output {
		Some(output_path) => {
			let content = match args.format {
				Some(crate::cli::OutputFormat::Json) => {
					serde_json::to_string_pretty(&schema_info)?
				},
				_ => {
					format!("{}
-----------------------------------
LEGEND:
  Column|Type|Nullable
-----------------------------------
{}
","GOP API Registry v1.0", schema_info.iter().map(|f| format!("{}|{}|{}", f.name, f.data_type, f.nullable)).collect::<Vec<_>>().join("\n"))
				}
			};
			std::fs::write(output_path, content)?;
		},
		None => {
			match args.format {
				Some(crate::cli::OutputFormat::Json) => {
					println!("{}", serde_json::to_string_pretty(&schema_info)?);
				},
				_ => {
					println!("{}
-----------------------------------
LEGEND:
  Column|Type|Nullable
-----------------------------------
{}
","GOP API Registry v1.0", schema_info.iter().map(|f| format!("{}|{}|{}", f.name, f.data_type, f.nullable)).collect::<Vec<_>>().join("\n"));
				}
			}
		},
	}
	
	Ok(())
}

#[derive(serde::Serialize)]
struct SchemaField {
	name: String,
	data_type: String,
	nullable: bool,
}

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use regex::Regex;
use crate::error::{NailError, NailResult};
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct SelectArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(short, long, help = "Column names or regex patterns (comma-separated)")]
	pub columns: Option<String>,
	
	#[arg(short, long, help = "Row numbers or ranges (e.g., 1,3,5-10)")]
	pub rows: Option<String>,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: SelectArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	let mut result_df = df;
	
	if let Some(col_spec) = &args.columns {
		let schema = result_df.schema();
		let selected_columns = select_columns_by_pattern(schema.clone().into(), col_spec)?;
		
		if args.verbose {
			eprintln!("Selecting {} columns: {:?}", selected_columns.len(), selected_columns);
		}
		
		let select_exprs: Vec<Expr> = selected_columns.into_iter()
			.map(|name| col(name))
			.collect();
		
		result_df = result_df.select(select_exprs)?;
	}
	
	if let Some(row_spec) = &args.rows {
		let row_indices = parse_row_specification(row_spec)?;
		
		if args.verbose {
			eprintln!("Selecting {} rows", row_indices.len());
		}
		
		result_df = select_rows_by_indices(&result_df, &row_indices).await?;
	}
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&result_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&result_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

pub fn select_columns_by_pattern(schema: datafusion::common::DFSchemaRef, pattern: &str) -> NailResult<Vec<String>> {
	let patterns: Vec<&str> = pattern.split(',').map(|s| s.trim()).collect();
	let mut selected = Vec::new();
	let mut not_found = Vec::new();
	
	for pattern in &patterns {
		let mut found = false;
		
		// First try exact match (case-sensitive)
		for field in schema.fields() {
			let field_name = field.name();
			
			if pattern.contains('*') || pattern.contains('^') || pattern.contains('$') {
				let regex = Regex::new(pattern)?;
				if regex.is_match(field_name) {
					selected.push(field_name.clone());
					found = true;
				}
			} else if field_name == *pattern {
				selected.push(field_name.clone());
				found = true;
				break;
			}
		}
		
		// If not found, try case-insensitive match
		if !found {
			for field in schema.fields() {
				let field_name = field.name();
				
				if pattern.contains('*') || pattern.contains('^') || pattern.contains('$') {
					// For regex patterns, create case-insensitive version
					let case_insensitive_pattern = format!("(?i){}", pattern);
					if let Ok(regex) = Regex::new(&case_insensitive_pattern) {
						if regex.is_match(field_name) {
							selected.push(field_name.clone());
							found = true;
						}
					}
				} else if field_name.to_lowercase() == pattern.to_lowercase() {
					selected.push(field_name.clone());
					found = true;
					break;
				}
			}
		}
		
		if !found {
			not_found.push(*pattern);
		}
	}
	
	if !not_found.is_empty() {
		let available_columns: Vec<String> = schema.fields().iter()
			.map(|f| f.name().clone())
			.collect();
		return Err(NailError::ColumnNotFound(format!(
			"Columns not found: {:?}. Available columns: {:?}", 
			not_found, available_columns
		)));
	}
	
	// Remove duplicates while preserving order
	let mut unique_selected = Vec::new();
	for col in selected {
		if !unique_selected.contains(&col) {
			unique_selected.push(col);
		}
	}
	
	if unique_selected.is_empty() {
		return Err(NailError::ColumnNotFound(format!("No columns matched pattern: {}", pattern)));
	}
	
	Ok(unique_selected)
}

pub fn parse_row_specification(spec: &str) -> NailResult<Vec<usize>> {
	let mut indices = Vec::new();
	
	for part in spec.split(',') {
		let part = part.trim();
		
		if part.contains('-') {
			let range_parts: Vec<&str> = part.split('-').collect();
			if range_parts.len() != 2 {
				return Err(NailError::InvalidArgument(format!("Invalid range: {}", part)));
			}
			
			let start: usize = range_parts[0].parse()
				.map_err(|_| NailError::InvalidArgument(format!("Invalid start index: {}", range_parts[0])))?;
			let end: usize = range_parts[1].parse()
				.map_err(|_| NailError::InvalidArgument(format!("Invalid end index: {}", range_parts[1])))?;
			
			if start > end {
				return Err(NailError::InvalidArgument(format!("Start index {} greater than end index {}", start, end)));
			}
			
			for i in start..=end {
				indices.push(i.saturating_sub(1));
			}
		} else {
			let index: usize = part.parse()
				.map_err(|_| NailError::InvalidArgument(format!("Invalid index: {}", part)))?;
			indices.push(index.saturating_sub(1));
		}
	}
	
	indices.sort();
	indices.dedup();
	Ok(indices)
}

async fn select_rows_by_indices(df: &DataFrame, indices: &[usize]) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let indices_str = indices.iter()
		.map(|&i| (i + 1).to_string())
		.collect::<Vec<_>>()
		.join(",");
	
	// Get the original column names and quote them to preserve case
	let original_columns: Vec<String> = df.schema().fields().iter()
		.map(|f| format!("\"{}\"", f.name()))
		.collect();
	
	let sql = format!(
		"SELECT {} FROM (SELECT {}, ROW_NUMBER() OVER() as rn FROM {}) WHERE rn IN ({})",
		original_columns.join(", "),
		original_columns.join(", "),
		table_name, 
		indices_str
	);
	
	let result = ctx.sql(&sql).await?;
	
	Ok(result)
}

use std::path::Path;

pub fn setup_test_fixtures() {
	let fixtures_dir = Path::new("tests/fixtures");
	std::fs::create_dir_all(fixtures_dir).unwrap();
	
	let sample_parquet = fixtures_dir.join("sample.parquet");
	if !sample_parquet.exists() {
		crate::common::create_sample_parquet(&sample_parquet).unwrap();
	}
}

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct IdArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(long, help = "Create new ID column")]
	pub create: bool,
	
	#[arg(long, help = "Prefix for ID values", default_value = "id")]
	pub prefix: String,
	
	#[arg(long, help = "ID column name", default_value = "id")]
	pub id_col_name: String,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: IdArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	
	let result_df = if args.create {
		if args.verbose {
			eprintln!("Creating ID column '{}' with prefix '{}'", args.id_col_name, args.prefix);
		}
		add_id_column(&df, &args.id_col_name, &args.prefix).await?
	} else {
		df
	};
	
	if let Some(output_path) = &args.output {
		let file_format = match args.format {
			Some(crate::cli::OutputFormat::Json) => Some(crate::utils::FileFormat::Json),
			Some(crate::cli::OutputFormat::Csv) => Some(crate::utils::FileFormat::Csv),
			Some(crate::cli::OutputFormat::Parquet) => Some(crate::utils::FileFormat::Parquet),
			_ => None,
		};
		write_data(&result_df, output_path, file_format.as_ref()).await?;
	} else {
		display_dataframe(&result_df, None, args.format.as_ref()).await?;
	}
	
	Ok(())
}

async fn add_id_column(df: &DataFrame, col_name: &str, prefix: &str) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	// Check if column already exists
	let schema = df.schema();
	if schema.field_with_name(None, col_name).is_ok() {
		return Err(crate::error::NailError::InvalidArgument(
			format!("Column '{}' already exists. Use --id-col-name to specify a different name.", col_name)
		));
	}
	
	let id_col = if prefix.is_empty() {
		"ROW_NUMBER() OVER()".to_string()
	} else {
		format!("CONCAT('{}', ROW_NUMBER() OVER())", prefix)
	};
	
	let columns: Vec<String> = df.schema().fields().iter()
		.map(|f| f.name().clone())
		.collect();
	
	let sql = format!(
		"SELECT {} as {}, {} FROM {}",
		id_col,
		col_name,
		columns.join(", "),
		table_name
	);
	
	let result = ctx.sql(&sql).await?;
	Ok(result)
}

			
			for i in start..=end {
				indices.push(i.saturating_sub(1));
			}
		} else {
			let index: usize = part.parse()
				.map_err(|_| NailError::InvalidArgument(format!("Invalid index: {}", part)))?;
			indices.push(index.saturating_sub(1));
		}
	}
	
	indices.sort();
	indices.dedup();
	Ok(indices)
}

async fn select_rows_by_indices(df: &DataFrame, indices: &[usize]) -> NailResult<DataFrame> {
	let ctx = crate::utils::create_context().await?;
	
	let table_name = "temp_table";
	ctx.register_table(table_name, df.clone().into_view())?;
	
	let indices_str = indices.iter()
		.map(|&i| (i + 1).to_string())
		.collect::<Vec<_>>()
		.join(",");
	
	// Get the original column names and quote them to preserve case
	let original_columns: Vec<String> = df.schema().fields().iter()
		.map(|f| format!("\"{}\"", f.name()))
		.collect();
	
	let sql = format!(
		"SELECT {} FROM (SELECT {}, ROW_NUMBER() OVER() as rn FROM {}) WHERE rn IN ({})",
		original_columns.join(", "),
		original_columns.join(", "),
		table_name, 
		indices_str
	);
	
	let result = ctx.sql(&sql).await?;
	
	Ok(result)
}

use std::path::Path;

pub fn setup_test_fixtures() {
	let fixtures_dir = Path::new("tests/fixtures");
	std::fs::create_dir_all(fixtures_dir).unwrap();
	
	let sample_parquet = fixtures_dir.join("sample.parquet");
	if !sample_parquet.exists() {
		crate::common::create_sample_parquet(&sample_parquet).unwrap();
	}
}

use clap::Args;
use datafusion::prelude::*;
use std::path::PathBuf;
use crate::error::NailResult;
use crate::utils::io::{read_data, write_data};
use crate::utils::format::display_dataframe;

#[derive(Args, Clone)]
pub struct IdArgs {
	#[arg(short, long, help = "Input file")]
	pub input: PathBuf,
	
	#[arg(long, help = "Create new ID column")]
	pub create: bool,
	
	#[arg(long, help = "Prefix for ID values", default_value = "id")]
	pub prefix: String,
	
	#[arg(long, help = "ID column name", default_value = "id")]
	pub id_col_name: String,
	
	#[arg(short, long, help = "Output file (if not specified, prints to console)")]
	pub output: Option<PathBuf>,
	
	#[arg(short, long, help = "Output format", value_enum)]
	pub format: Option<crate::cli::OutputFormat>,
	
	#[arg(short, long, help = "Enable verbose output")]
	pub verbose: bool,
}

pub async fn execute(args: IdArgs) -> NailResult<()> {
	if args.verbose {
		eprintln!("Reading data from: {}", args.input.display());
	}
	
	let df = read_data(&args.input).await?;
	
	let result_df = if args.create {